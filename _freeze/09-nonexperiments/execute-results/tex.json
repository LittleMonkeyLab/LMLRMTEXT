{
  "hash": "de80e257bc68e891a7e40f32f9cedcf0",
  "result": {
    "markdown": "# Nonexperimental Research\n\nWhat do the following classic studies have in common?\n\n-   Stanley Milgram found that about two thirds of his research\n    participants were willing to administer dangerous shocks to another\n    person just because they were told to by an authority figure\n    [@milgram1963behavioral].\n-   Elizabeth Loftus and Jacqueline Pickrell showed that it is\n    relatively easy to \"implant\" false memories in people by repeatedly\n    asking them about childhood events that did not actually happen to\n    them [@loftus1995formation].\n-   John Cacioppo and Richard Petty evaluated the validity of their Need\n    for Cognition Scale---a measure of the extent to which people like\n    and value thinking---by comparing the scores of college professors\n    with those of factory workers [@cacioppo1982need].\n-   David Rosenhan found that confederates who went to psychiatric\n    hospitals claiming to have heard voices saying things like \"empty\"\n    and \"thud\" were labeled as schizophrenic by the hospital staff and\n    kept there even though they behaved normally in all other ways\n    [@rosenhan1973being].\n\nThe answer for purposes of this chapter is that they are not\nexperiments. In this chapter we look more closely at nonexperimental\nresearch. We begin with a general definition of nonexperimental\nresearch, along with a discussion of when and why nonexperimental\nresearch is more appropriate than experimental research. We then look\nseparately at three important types of nonexperimental research:\ncorrelational research, quasi-experimental research, and qualitative\nresearch.\n\n## Overview of Nonexperimental Research\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Define nonexperimental research, distinguish it clearly from\n    experimental research, and give several examples.\n2.  Explain when a researcher might choose to conduct nonexperimental\n    research as opposed to experimental research.\n:::\n\n### What Is Nonexperimental Research? {.unnumbered}\n\n[Nonexperimental research] is research that lacks the manipulation of an\nindependent variable, random assignment of participants to conditions or\norders of conditions, or both.\n\nIn a sense, it is unfair to define this large and diverse set of\napproaches collectively by what they are *not*. But doing so reflects\nthe fact that most researchers in psychology consider the distinction\nbetween experimental and nonexperimental research to be an extremely\nimportant one. This is because while experimental research can provide\nstrong evidence that changes in an independent variable cause\ndifferences in a dependent variable, nonexperimental research generally\ncannot. As we will see, however, this does not mean that nonexperimental\nresearch is less important than experimental research or inferior to it\nin any general sense.\n\n### When to Use Nonexperimental Research {.unnumbered}\n\nAs we saw in the chapter on experimental research, experimental research\nis appropriate when the researcher has a specific research question or\nhypothesis about a causal relationship between two variables---and it is\npossible, feasible, and ethical to manipulate the independent variable\nand randomly assign participants to conditions or to orders of\nconditions. It stands to reason, therefore, that nonexperimental\nresearch is appropriate---even necessary---when these conditions are not\nmet. There are many ways in which this can be the case.\n\n-   The research question or hypothesis can be about a single variable\n    rather than a statistical relationship between two variables (e.g.,\n    How accurate are people's first impressions?).\n-   The research question can be about a noncausal statistical\n    relationship between variables (e.g., Is there a correlation between\n    verbal intelligence and mathematical intelligence?).\n-   The research question can be about a causal relationship, but the\n    independent variable cannot be manipulated or participants cannot be\n    randomly assigned to conditions or orders of conditions (e.g., Does\n    damage to a person's hippocampus impair the formation of long-term\n    memory traces?).\n-   The research question can be broad and exploratory, or it can be\n    about what it is like to have a particular experience (e.g., What is\n    it like to be a working mother diagnosed with depression?).\n\nAgain, the choice between the experimental and nonexperimental\napproaches is generally dictated by the nature of the research question.\nIf it is about a causal relationship and involves an independent\nvariable that can be manipulated, the experimental approach is typically\npreferred. Otherwise, the nonexperimental approach is preferred. But the\ntwo approaches can also be used to address the same research question in\ncomplementary ways. For example, nonexperimental studies establishing\nthat there is a relationship between watching violent television and\naggressive behavior have been complemented by experimental studies\nconfirming that the relationship is a causal one [@bushman2001effects].\nSimilarly, after his original study, Milgram conducted experiments to\nexplore the factors that affect obedience. He manipulated several\nindependent variables, such as the distance between the experimenter and\nthe participant, the participant and the confederate, and the location\nof the study [@milgram1964obedience].\n\n### Types of Nonexperimental Research {.unnumbered}\n\nNonexperimental research falls into three broad categories:\nsingle-variable research, correlational and quasi-experimental research,\nand qualitative research. First, research can be nonexperimental because\nit focuses on a single variable rather than a statistical relationship\nbetween two variables. Although there is no widely shared term for this\nkind of research, we will call it [single-variable research]. Milgram's\noriginal obedience study was nonexperimental in this way. He was\nprimarily interested in one variable---the extent to which participants\nobeyed the researcher when he told them to shock the confederate---and\nhe observed all participants performing the same task under the same\nconditions. The study by Loftus and Pickrell described at the beginning\nof this chapter is also a good example of single-variable research. The\nvariable was whether participants \"remembered\" having experienced mildly\ntraumatic childhood events (e.g., getting lost in a shopping mall) that\nthey had not actually experienced but that the research asked them about\nrepeatedly. In this particular study, nearly a third of the participants\n\"remembered\" at least one event. (As with Milgram's original study, this\nstudy inspired several later experiments on the factors that affect\nfalse memories.)\n\nAs these examples make clear, single-variable research can answer\ninteresting and important questions. What it cannot do, however, is\nanswer questions about statistical relationships between variables. This\nis a point that beginning researchers sometimes miss. Imagine, for\nexample, a group of research methods students interested in the\nrelationship between children's being the victim of bullying and the\nchildren's self-esteem. The first thing that is likely to occur to these\nresearchers is to obtain a sample of middle-school students who have\nbeen bullied and then to measure their self-esteem. But this would be a\nsingle-variable study with self-esteem as the only variable. Although it\nwould tell the researchers something about the self-esteem of children\nwho have been bullied, it would not tell them what they really want to\nknow, which is how the self-esteem of children who have been bullied\n*compares* with the self-esteem of children who have not. Is it lower?\nIs it the same? Could it even be higher? To answer this question, their\nsample would also have to include middle-school students who have not\nbeen bullied.\n\nResearch can also be nonexperimental because it focuses on a statistical\nrelationship between two variables but does not include the manipulation\nof an independent variable, random assignment of participants to\nconditions or orders of conditions, or both. This kind of research takes\ntwo basic forms: correlational research and quasi-experimental research.\nIn [correlational research], the researcher measures the two variables\nof interest with little or no attempt to control extraneous variables\nand then assesses the relationship between them. A research methods\nstudent who finds out whether each of several middle-school students has\nbeen bullied and then measures each student's self-esteem is conducting\ncorrelational research. In [quasi-experimental research], the researcher\nmanipulates an independent variable but does not randomly assign\nparticipants to conditions or orders of conditions. For example, a\nresearcher might start an antibullying program (a kind of treatment) at\none school and compare the incidence of bullying at that school with the\nincidence at a similar school that has no antibullying program.\n\nThe final way in which research can be nonexperimental is that it can be\nqualitative. The types of research we have discussed so far are all\nquantitative, referring to the fact that the data consist of numbers\nthat are analyzed using statistical techniques. In \\[qualitative\nresearch\\], the data are usually nonnumerical and are analyzed using\nnonstatistical techniques. Rosenhan's study of the experience of people\nin a psychiatric ward was primarily qualitative. The data were the notes\ntaken by the \"pseudopatients\"---the people pretending to have heard\nvoices---along with their hospital records. Rosenhan's analysis consists\nmainly of a written description of the experiences of the\npseudopatients, supported by several concrete examples. To illustrate\nthe hospital staff's tendency to \"depersonalize\" their patients, he\nnoted, \"Upon being admitted, I and other pseudopatients took the initial\nphysical examinations in a semipublic room, where staff members went\nabout their own business as if we were not there\" [@rosenhan1973being,\np. 256].\n\n### Internal Validity Revisited {.unnumbered}\n\nRecall that internal validity is the extent to which the design of a\nstudy supports the conclusion that changes in the independent variable\ncaused any observed differences in the dependent variable. Figure\n\\@ref(fig:validity) shows how experimental, quasi-experimental, and\ncorrelational research vary in terms of internal validity. Experimental\nresearch tends to be highest because it addresses the directionality and\nthird-variable problems through manipulation and the control of\nextraneous variables through random assignment. If the average score on\nthe dependent variable in an experiment differs across conditions, it is\nquite likely that the independent variable is responsible for that\ndifference. Correlational research is lowest because it fails to address\neither problem. If the average score on the dependent variable differs\nacross levels of the independent variable, it could be that the\nindependent variable is responsible, but there are other\ninterpretations. In some situations, the direction of causality could be\nreversed. In others, there could be a third variable that is causing\ndifferences in both the independent and dependent variables.\nQuasi-experimental research is in the middle because the manipulation of\nthe independent variable addresses some problems, but the lack of random\nassignment and experimental control fails to address others. Imagine,\nfor example, that a researcher finds two similar schools, starts an\nantibullying program in one, and then finds fewer bullying incidents in\nthat \"treatment school\" than in the \"control school.\" There is no\ndirectionality problem because clearly the number of bullying incidents\ndid not determine which school got the program. However, the lack of\nrandom assignment of children to schools could still mean that students\nin the treatment school differed from students in the control school in\nsome other way that could explain the difference in bullying.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Experiments are generally high in internal validity, quasi-experiments lower, and correlational studies lower still.](images/nonexperiments/validity.png){fig-align='center' fig-alt='Three overlapping rectangles. From left to right, they are labeled Correlational, Quasi-Experimental, and Experimental. The rectangles are over a continuum from lower internal validity on the left to higher internal validity on the right.' width=90%}\n:::\n:::\n\n\n\nNotice also in Figure \\@ref(fig:validity) that there is some overlap in\nthe internal validity of experiments, quasi-experiments, and\ncorrelational studies. For example, a poorly designed experiment that\nincludes many confounding variables can be lower in internal validity\nthan a well designed quasi-experiment with no obvious confounding\nvariables.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Nonexperimental research is research that lacks the manipulation of\n    an independent variable, control of extraneous variables through\n    random assignment, or both.\n-   There are three broad types of nonexperimental research.\n    Single-variable research focuses on a single variable rather than a\n    relationship between variables. Correlational and quasi-experimental\n    research focus on a statistical relationship but lack manipulation\n    or random assignment. Qualitative research focuses on broader\n    research questions, typically involves collecting large amounts of\n    data from a small number of participants, and analyzes the data\n    nonstatistically.\n-   In general, experimental research is high in internal validity,\n    correlational research is low in internal validity, and\n    quasi-experimental research is in between.\n:::\n\n::: exercises\n##### EXERCISE {.unnumbered}\n\n1.  Discussion: For each of the following studies, decide which type of\n    research design it is and explain why.\n    a.  A researcher conducts detailed interviews with unmarried teenage\n        fathers to learn about how they feel and what they think about\n        their role as fathers and summarizes their feelings in a written\n        narrative.\n    b.  A researcher measures the impulsivity of a large sample of\n        drivers and looks at the statistical relationship between this\n        variable and the number of traffic tickets the drivers have\n        received.\n    c.  A researcher randomly assigns patients with low back pain either\n        to a treatment involving hypnosis or to a treatment involving\n        exercise. She then measures their level of low back pain after 3\n        months.\n    d.  A college instructor gives weekly quizzes to students in one\n        section of his course but no weekly quizzes to students in\n        another section to see whether this has an effect on their test\n        performance.\n:::\n\n## Correlational Research\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Define correlational research and give several examples.\n2.  Explain why a researcher might choose to conduct correlational\n    research rather than experimental research or another type of\n    nonexperimental research.\n:::\n\n### What Is Correlational Research? {.unnumbered}\n\nCorrelational research is a type of nonexperimental research in which\nthe researcher measures two variables and assesses the statistical\nrelationship (i.e., the correlation) between them with little or no\neffort to control extraneous variables. There are essentially two\nreasons that researchers interested in statistical relationships between\nvariables would choose to conduct a correlational study rather than an\nexperiment. The first is that they do not believe that the statistical\nrelationship is a causal one. For example, a researcher might evaluate\nthe validity of a brief extraversion test by administering it to a large\ngroup of participants along with a longer extraversion test that has\nalready been shown to be valid. This researcher might then check to see\nwhether participants' scores on the brief test are strongly correlated\nwith their scores on the longer one. Neither test score is thought to\ncause the other, so there is no independent variable to manipulate. In\nfact, the terms *independent variable* and *dependent variable* do not\napply to this kind of research.\n\nThe other reason that researchers would choose to use a correlational\nstudy rather than an experiment is that the statistical relationship of\ninterest is thought to be causal, but the researcher cannot manipulate\nthe independent variable because it is impossible, impractical, or\nunethical. For example, Allen Kanner and his colleagues thought that the\nnumber of \"daily hassles\" (e.g., rude salespeople, heavy traffic) that\npeople experience affects the number of physical and psychological\nsymptoms they have [@kanner1981comparison]. But because they could not\nmanipulate the number of daily hassles their participants experienced,\nthey had to settle for measuring the number of daily hassles---along\nwith the number of symptoms---using self-report questionnaires. Although\nthe strong positive relationship they found between these two variables\nis consistent with their idea that hassles cause symptoms, it is also\nconsistent with the idea that symptoms cause hassles or that some third\nvariable (e.g., neuroticism) causes both.\n\nA common misconception among beginning researchers is that correlational\nresearch must involve two quantitative variables, such as scores on two\nextraversion tests or the number of hassles and number of symptoms\npeople have experienced. However, the defining feature of correlational\nresearch is that the two variables are measured---neither one is\nmanipulated---and this is true regardless of whether the variables are\nquantitative or categorical. Imagine, for example, that a researcher\nadministers the Rosenberg Self-Esteem Scale to 50 American college\nstudents and 50 Japanese college students. Although this \"feels\" like a\nbetween-subjects experiment, it is a correlational study because the\nresearcher did not manipulate the students' nationalities. The same is\ntrue of the study by Cacioppo and Petty comparing college faculty and\nfactory workers in terms of their need for cognition. It is a\ncorrelational study because the researchers did not manipulate the\nparticipants' occupations.\n\nFigure \\@ref(fig:todo) shows data from a hypothetical study on the\nrelationship between whether people make a daily list of things to do (a\n\"to-do list\") and stress. Notice that it is unclear whether this is an\nexperiment or a correlational study because it is unclear whether the\nindependent variable was manipulated. If the researcher randomly\nassigned some participants to make daily to-do lists and others not to,\nthen it is an experiment. If the researcher simply asked participants\nwhether they made daily to-do lists, then it is a correlational study.\nThe distinction is important because if the study was an experiment,\nthen it could be concluded that making the daily to-do lists reduced\nparticipants' stress. But if it was a correlational study, it could only\nbe concluded that these variables are statistically related. Perhaps\nbeing stressed has a negative effect on people's ability to plan ahead\n(the directionality problem). Or perhaps people who are more\nconscientious are more likely to make to-do lists and less likely to be\nstressed (the third-variable problem). The crucial point is that what\ndefines a study as experimental or correlational is not the variables\nbeing studied, nor whether the variables are quantitative or\ncategorical, nor the type of graph or statistics used to analyze the\ndata. It is how the study is conducted.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Results of a hypothetical study on whether people who make daily to-do lists experience less stress than people who do not make such lists.](09-nonexperiments_files/figure-pdf/todo-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n### Data Collection in Correlational Research {.unnumbered}\n\nAgain, the defining feature of correlational research is that neither\nvariable is manipulated. It does not matter how or where the variables\nare measured. A researcher could have participants come to a laboratory\nto complete a computerized backward digit span task and a computerized\nrisky decision-making task and then assess the relationship between\nparticipants' scores on the two tasks. Or a researcher could go to a\nshopping mall to ask people about their attitudes toward the environment\nand their shopping habits and then assess the relationship between these\ntwo variables. Both of these studies would be correlational because no\nindependent variable is manipulated. However, because some approaches to\ndata collection are strongly associated with correlational research, it\nmakes sense to discuss them here. The two we will focus on are\nnaturalistic observation and archival data. A third, survey research, is\ndiscussed in its own chapter.\n\n#### Naturalistic Observation {.unnumbered}\n\n[Naturalistic observation] is an approach to data collection that\ninvolves observing people's behavior in the environment in which it\ntypically occurs. Thus naturalistic observation is a type of field\nresearch (as opposed to a type of laboratory research). It could involve\nobserving shoppers in a grocery store, children on a school playground,\nor psychiatric inpatients in their wards. Researchers engaged in\nnaturalistic observation usually make their observations as\nunobtrusively as possible so that participants are often not aware that\nthey are being studied. Ethically, this is considered to be acceptable\nif the participants remain anonymous and the behavior occurs in a public\nsetting where people would not normally have an expectation of privacy.\nGrocery shoppers putting items into their shopping carts, for example,\nare engaged in public behavior that is easily observable by store\nemployees and other shoppers. For this reason, most researchers would\nconsider it ethically acceptable to observe them for a study. On the\nother hand, one of the arguments against the ethicality of the\nnaturalistic observation of \"bathroom behavior\" discussed earlier in the\nbook is that people have a reasonable expectation of privacy even in a\npublic restroom and that this expectation was violated.\n\nResearchers Robert Levine and Ara Norenzayan used naturalistic\nobservation to study differences in the \"pace of life\" across countries\n[@levine1999pace]. One of their measures involved observing pedestrians\nin a large city to see how long it took them to walk 60 feet. They found\nthat people in some countries walked reliably faster than people in\nother countries. For example, people in the United States and Japan\ncovered 60 feet in about 12 seconds on average, while people in Brazil\nand Romania took close to 17 seconds.\n\nBecause naturalistic observation takes place in the complex and even\nchaotic \"real world,\" there are two closely related issues that\nresearchers must deal with before collecting data. The first is\nsampling. When, where, and under what conditions will the observations\nbe made, and who exactly will be observed? Levine and Norenzayan\ndescribed their sampling process as follows:\n\nMale and female walking speed over a distance of 60 feet was measured in\nat least two locations in main downtown areas in each city. Measurements\nwere taken during main business hours on clear summer days. All\nlocations were flat, unobstructed, had broad sidewalks, and were\nsufficiently uncrowded to allow pedestrians to move at potentially\nmaximum speeds. To control for the effects of socializing, only\npedestrians walking alone were used. Children, individuals with obvious\nphysical handicaps, and window-shoppers were not timed. Thirty-five men\nand 35 women were timed in most cities. (p. 186)\n\nPrecise specification of the sampling process in this way makes data\ncollection manageable for the observers, and it also provides some\ncontrol over important extraneous variables. For example, by making\ntheir observations on clear summer days in all countries, Levine and\nNorenzayan controlled for effects of the weather on people's walking\nspeeds.\n\nThe second issue is measurement. What specific behaviors will be\nobserved? In Levine and Norenzayan's study, measurement was relatively\nstraightforward. They simply measured out a 60-foot distance along a\ncity sidewalk and then used a stopwatch to time participants as they\nwalked over that distance. Often, however, the behaviors of interest are\nnot so obvious or objective. For example, researchers Robert Kraut and\nRobert Johnston wanted to study bowlers' reactions to their shots, both\nwhen they were facing the pins and then when they turned toward their\ncompanions [@kraut1979social]. But what \"reactions\" should they observe?\nBased on previous research and their own pilot testing, Kraut and\nJohnston created a list of reactions that included \"closed smile,\" \"open\nsmile,\" \"laugh,\" \"neutral face,\" \"look down,\" \"look away,\" and \"face\ncover\" (covering one's face with one's hands). The observers committed\nthis list to memory and then practiced by coding the reactions of\nbowlers who had been videotaped. During the actual study, the observers\nspoke into an audio recorder, describing the reactions they observed.\nAmong the most interesting results of this study was that bowlers rarely\nsmiled while they still faced the pins. They were much more likely to\nsmile after they turned toward their companions, suggesting that smiling\nis not purely an expression of happiness but also a form of social\ncommunication.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Naturalistic observation has revealed that bowlers tend to smile when they turn away from the pins and toward their companions, suggesting that smiling is not purely an expression of happiness but also a form of social communication. *Photo by José Juan Rosa on Unsplash.*](images/nonexperiments/bowl.jpeg){fig-align='center' fig-alt='A person with short hair in a striped yellow shirt looks down the lane of a bowling alley, presumably having just bowled.' width=50%}\n:::\n:::\n\n\n\nWhen the observations require a judgment on the part of the\nobservers---as in Kraut and Johnston's study---this process is often\ndescribed as [coding]. Coding generally requires clearly defining a set\nof target behaviors. The observers then categorize participants\nindividually in terms of which behavior they have engaged in and the\nnumber of times they engaged in each behavior. The observers might even\nrecord the duration of each behavior. The target behaviors must be\ndefined in such a way that different observers code them in the same\nway. This is the issue of interrater reliability. Researchers are\nexpected to demonstrate the interrater reliability of their coding\nprocedure by having multiple raters code the same behaviors\nindependently and then showing that the different observers are in close\nagreement. Kraut and Johnston, for example, video recorded a subset of\ntheir participants' reactions and had two observers independently code\nthem. The two observers showed that they agreed on the reactions that\nwere exhibited 97% of the time, indicating good interrater reliability.\n\n#### Archival Data {.unnumbered}\n\nAnother approach to correlational research is the use of [archival\ndata], which are data that have already been collected for some other\npurpose. An example is a study by Brett Pelham and his colleagues on\n\"implicit egotism\"---the tendency for people to prefer people, places,\nand things that are similar to themselves [@pelham2005implicit]. In one\nstudy, they examined Social Security records to show that women with the\nnames Virginia, Georgia, Louise, and Florence were especially likely to\nhave moved to the states of Virginia, Georgia, Louisiana, and Florida,\nrespectively.\n\nAs with naturalistic observation, measurement can be more or less\nstraightforward when working with archival data. For example, counting\nthe number of people named Virginia who live in various states based on\nSocial Security records is relatively straightforward. But consider a\nstudy by Christopher Peterson and his colleagues on the relationship\nbetween optimism and health using data that had been collected many\nyears before for a study on adult development\n[@peterson1988pessimistic]. In the 1940s, healthy male college students\nhad completed an open-ended questionnaire about difficult wartime\nexperiences. In the late 1980s, Peterson and his colleagues reviewed the\nmen's questionnaire responses to obtain a measure of explanatory\nstyle---their habitual ways of explaining bad events that happen to\nthem. More pessimistic people tend to blame themselves and expect\nlong-term negative consequences that affect many aspects of their lives,\nwhile more optimistic people tend to blame outside forces and expect\nlimited negative consequences. To obtain a measure of explanatory style\nfor each participant, the researchers used a procedure in which all\nnegative events mentioned in the questionnaire responses, and any causal\nexplanations for them, were identified and written on index cards. These\nwere given to a separate group of raters who rated each explanation in\nterms of three separate dimensions of optimism-pessimism. These ratings\nwere then averaged to produce an explanatory style score for each\nparticipant. The researchers then assessed the statistical relationship\nbetween the men's explanatory style as college students and archival\nmeasures of their health at approximately 60 years of age. The primary\nresult was that the more optimistic the men were as college students,\nthe healthier they were as older men. Pearson's r was +.25.\n\nThis is an example of [content analysis]---a family of systematic\napproaches to measurement using complex archival data. Just as\nnaturalistic observation requires specifying the behaviors of interest\nand then noting them as they occur, content analysis requires specifying\nkeywords, phrases, or ideas and then finding all occurrences of them in\nthe data. These occurrences can then be counted, timed (e.g., the amount\nof time devoted to entertainment topics on the nightly news show), or\nanalyzed in a variety of other ways.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Correlational research involves measuring two variables and\n    assessing the relationship between them, with no manipulation of an\n    independent variable.\n-   Correlational research is not defined by where or how the data are\n    collected. However, some approaches to data collection are strongly\n    associated with correlational research. These include naturalistic\n    observation (in which researchers observe people's behavior in the\n    context in which it normally occurs) and the use of archival data\n    that were already collected for some other purpose.\n:::\n\n::: exercises\n##### EXERCISE {.unnumbered}\n\n1.  Discussion: For each of the following, decide whether it is most\n    likely that the study described is experimental or correlational and\n    explain why.\n    a.  An educational researcher compares the academic performance of\n        students from the \"rich\" side of town with that of students from\n        the \"poor\" side of town.\n    b.  A cognitive psychologist compares the ability of people to\n        recall words that they were instructed to \"read\" with their\n        ability to recall words that they were instructed to \"imagine.\"\n    c.  A manager studies the correlation between new employees' college\n        grade point averages and their first-year performance reports.\n    d.  An automotive engineer installs different stick shifts in a new\n        car prototype, each time asking several people to rate how\n        comfortable the stick shift feels.\n    e.  A food scientist studies the relationship between the\n        temperature inside people's refrigerators and the amount of\n        bacteria on their food.\n    f.  A social psychologist tells some research participants that they\n        need to hurry over to the next building to complete a study. She\n        tells others that they can take their time. Then she observes\n        whether they stop to help a research assistant who is pretending\n        to be hurt.\n:::\n\n## Survey Research\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Define what survey research is, including its two important\n    characteristics.\n2.  Describe several different ways that survey research can be used and\n    give some examples.\n3.  Describe the cognitive processes involved in responding to a survey\n    item.\n4.  Explain what a context effect is and give some examples.\n5.  Create a simple survey questionnaire based on principles of\n    effective item writing and organization.\n6.  List the four major ways to conduct a survey along with some pros\n    and cons of each.\n:::\n\nShortly after the terrorist attacks in New York City and Washington, DC,\nin September of 2001, researcher Jennifer Lerner and her colleagues\nconducted an Internet-based survey of nearly 2,000 American teens and\nadults ranging in age from 13 to 88 [@lerner2003effects]. They asked\nparticipants about their reactions to the attacks and for their\njudgments of various terrorism-related and other risks. Among the\nresults were that the participants tended to overestimate most risks,\nthat females did so more than males, and that there were no differences\nbetween teens and adults. The most interesting result, however, had to\ndo with the fact that some participants were \"primed\" to feel anger by\nasking them what made them angry about the attacks and by presenting\nthem with a photograph and audio clip intended to evoke anger. Others\nwere primed to feel fear by asking them what made them fearful about the\nattacks and by presenting them with a photograph and audio clip intended\nto evoke fear. As the researchers hypothesized, the participants who\nwere primed to feel anger perceived less risk than the participants who\nhad been primed to feel fear---showing how risk perceptions are strongly\ntied to specific emotions.\n\nThe study by Lerner and her colleagues is an example of survey research\nin psychology---the topic of this chapter. We begin with an overview of\nsurvey research, including its definition, some history, and a bit about\nwho conducts it and why. We then look at survey responding as a\npsychological process and the implications of this for constructing good\nsurvey questionnaires. Finally, we consider some issues related to\nactually conducting survey research, including sampling the participants\nand collecting the data.\n\n### What Is Survey Research? {.unnumbered}\n\n[Survey research] is a quantitative approach that has two important\ncharacteristics. First, the variables of interest are measured using\nself-reports. In essence, survey researchers ask their participants (who\nare often called [respondents](#respondent) in survey research) to\nreport directly on their own thoughts, feelings, and behaviors. Second,\nconsiderable attention is paid to the issue of sampling. In particular,\nsurvey researchers have a strong preference for large random samples\nbecause they provide the most accurate estimates of what is true in the\npopulation. In fact, survey research may be the only approach in\npsychology in which random sampling is routinely used. Beyond these two\ncharacteristics, almost anything goes in survey research. Surveys can be\nlong or short. They can be conducted in person, by telephone, through\nthe mail, or over the Internet. They can be about voting intentions,\nconsumer preferences, social attitudes, health, or anything else that it\nis possible to ask people about and receive meaningful answers.\n\nMost survey research is nonexperimental. It is used to describe single\nvariables (e.g., the percentage of voters who prefer one presidential\ncandidate or another, the prevalence of schizophrenia in the general\npopulation) and also to assess statistical relationships between\nvariables (e.g., the relationship between income and health). But\nsurveys can also be experimental. The study by Lerner and her colleagues\nis a good example. Their use of self-report measures and a large\nnational sample identifies their work as survey research. But their\nmanipulation of an independent variable (anger vs. fear) to assess its\neffect on a dependent variable (risk judgments) also identifies their\nwork as experimental.\n\n#### History and Uses of Survey Research {.unnumbered}\n\nSurvey research may have its roots in English and American \"social\nsurveys\" conducted around the turn of the 20th century by researchers\nand reformers who wanted to document the extent of social problems such\nas poverty [@converse2017survey]. By the 1930s, the US government was\nconducting surveys to document economic and social conditions in the\ncountry. The need to draw conclusions about the entire population helped\nspur advances in sampling procedures. At about the same time, several\nresearchers who had already made a name for themselves in market\nresearch, studying consumer preferences for American businesses, turned\ntheir attention to election polling. A watershed event was the\npresidential election of 1936 between Alf Landon and Franklin Roosevelt.\nA magazine called *Literary Digest* conducted a survey by sending\nballots (which were also subscription requests) to millions of\nAmericans. Based on this \"straw poll,\" the editors predicted that Landon\nwould win in a landslide. At the same time, the new pollsters were using\nscientific methods with much smaller samples to predict just the\nopposite---that Roosevelt would win in a landslide. In fact, one of\nthem, George Gallup, publicly criticized the methods of *Literary\nDigest* before the election and all but guaranteed that his prediction\nwould be correct. And of course it was. (We will consider the reasons\nthat Gallup was right later in this chapter.)\n\nFrom market research and election polling, survey research made its way\ninto several academic fields, including political science, sociology,\nand public health---where it continues to be one of the primary\napproaches to collecting new data. Beginning in the 1930s, psychologists\nmade important advances in questionnaire design, including techniques\nthat are still used today, such as the Likert scale. (See \"What Is a\nLikert Scale?\" in Section \\@ref(Constructing Survey Questionnaires)\nSurvey research has a strong historical association with the social\npsychological study of attitudes, stereotypes, and prejudice. Early\nattitude researchers were also among the first psychologists to seek\nlarger and more diverse samples than the convenience samples of college\nstudents that were routinely used in psychology (and still are).\n\nSurvey research continues to be important in psychology today. For\nexample, survey data have been instrumental in estimating the prevalence\nof various mental disorders and identifying statistical relationships\namong those disorders and with various other factors. The National\nComorbidity Survey is a large-scale mental health survey conducted in\nthe United States (more [here](https://www.hcp.med.harvard.edu/ncs/)).\nIn just one part of this survey, nearly 10,000 adults were given a\nstructured mental health interview in their homes in 2002 and 2003.\nTable \\@ref(tab:comorbidity) presents results on the lifetime prevalence\nof some anxiety, mood, and substance use disorders. Obviously, this kind\nof information can be of great use both to basic researchers seeking to\nunderstand the causes and correlates of mental disorders and also to\nclinicians and policymakers who need to understand exactly how common\nthese disorders are.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Some lifetime prevalence results from the National Comorbidity Survey. The lifetime prevalence of a disorder is the percentage of people in the population that develop that disorder at any time in their lives.\n\n|Disorder                      | Total| Female| Male|\n|:-----------------------------|-----:|------:|----:|\n|Generalized anxiety disorder  |   5.7|    7.1|  4.2|\n|Obsessive-compulsive disorder |   2.3|    3.1|  1.6|\n|Major depressive disorder     |  16.9|   20.2| 13.2|\n|Bipolar disorder              |   4.4|    4.5|  4.3|\n|Alcohol abuse                 |  13.2|    7.5| 19.6|\n|Drug abuse                    |   8.0|    4.8| 11.6|\n:::\n:::\n\n\n\nAnd as the opening example makes clear, survey research can even be used\nto conduct experiments to test specific hypotheses about causal\nrelationships between variables. Such studies, when conducted on large\nand diverse samples, can be a useful supplement to laboratory studies\nconducted on college students. Although this is not a typical use of\nsurvey research, it certainly illustrates the flexibility of this\napproach.\n\n### Constructing Survey Questionnaires\n\nThe heart of any survey research project is the survey questionnaire\nitself. Although it is easy to think of interesting questions to ask\npeople, constructing a good survey questionnaire is not easy at all. The\nproblem is that the answers people give can be influenced in unintended\nways by the wording of the items, the order of the items, the response\noptions provided, and many other factors. At best, these influences add\nnoise to the data. At worst, they result in systematic biases and\nmisleading results. In this section, therefore, we consider some\nprinciples for constructing survey questionnaires to minimize these\nunintended effects and thereby maximize the reliability and validity of\nrespondents' answers.\n\n#### Survey Responding as a Psychological Process {.unnumbered}\n\nBefore looking at specific principles of survey questionnaire\nconstruction, it will help to consider survey responding as a\npsychological process.\n\nFigure \\@ref(fig:cogsurvey) presents a model of the cognitive processes\nthat people engage in when responding to a survey item\n[@sudman1996thinking]. Respondents must interpret the question, retrieve\nrelevant information from memory, form a tentative judgment, convert the\ntentative judgment into one of the response options provided (e.g., a\nrating on a 1-to-7 scale), and finally edit their response as necessary.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Model of the cognitive processes inolved in responding to a survey item.](images/nonexperiments/cogsurvey.png){fig-align='center' width=95%}\n:::\n:::\n\n\n\nConsider, for example, the following questionnaire item:\n\n> How many alcoholic drinks do you consume in a typical day?\n>\n> \\_\\_\\_\\_\\_ a lot more than average\\\n> \\_\\_\\_\\_\\_ somewhat more than average\\\n> \\_\\_\\_\\_\\_ average\\\n> \\_\\_\\_\\_\\_ somewhat fewer than average\\\n> \\_\\_\\_\\_\\_ a lot fewer than average\n\nAlthough this item at first seems straightforward, it poses several\ndifficulties for respondents. First, they must interpret the question.\nFor example, they must decide whether \"alcoholic drinks\" include beer\nand wine (as opposed to just hard liquor) and whether a \"typical day\" is\na typical weekday, typical weekend day, or both. Once they have\ninterpreted the question, they must retrieve relevant information from\nmemory to answer it. But what information should they retrieve, and how\nshould they go about retrieving it? They might think vaguely about some\nrecent occasions on which they drank alcohol, they might carefully try\nto recall and count the number of alcoholic drinks they consumed last\nweek, or they might retrieve some existing beliefs that they have about\nthemselves (e.g., \"I am not much of a drinker\"). Then they must use this\ninformation to arrive at a tentative judgment about how many alcoholic\ndrinks they consume in a typical day. For example, this might mean\ndividing the number of alcoholic drinks they consumed last week by seven\nto come up with an average number per day. Then they must format this\ntentative answer in terms of the response options actually provided. In\nthis case, the options pose additional problems of interpretation. For\nexample, what does \"average\" mean, and what would count as \"somewhat\nmore\" than average? Finally, they must decide whether they want to\nreport the response they have come up with or whether they want to edit\nit in some way. For example, if they believe that they drink much more\nthan average, they might not want to report this for fear of looking bad\nin the eyes of the researcher.\n\nFrom this perspective, what at first appears to be a simple matter of\nasking people how much they drink (and receiving a straightforward\nanswer from them) turns out to be much more complex.\n\n##### Context Effects on Questionnaire Responses {.unnumbered}\n\nAgain, this complexity can lead to unintended influences on respondents'\nanswers. These are often referred to as [context\neffects](#context-effect) because they are not related to the content of\nthe item but to the context in which the item appears\n[@schwarz1991context]. For example, there is an [item-order effect] when\nthe order in which the items are presented affects people's responses.\nOne item can change how participants interpret a later item or change\nthe information that they retrieve to respond to later items. For\nexample, researcher Fritz Strack and his colleagues asked college\nstudents about both their general life satisfaction and their dating\nfrequency [@strack1988priming]. When the life satisfaction item came\nfirst, the correlation between the two was only −.12, suggesting that\nthe two variables are only weakly related. But when the dating frequency\nitem came first, the correlation between the two was +.66, suggesting\nthat those who date more have a strong tendency to be more satisfied\nwith their lives. Reporting the dating frequency first made that\ninformation more accessible in memory so that they were more likely to\nbase their life satisfaction rating on it.\n\nThe response options provided can also have unintended effects on\npeople's responses [@schwarz1999self]. For example, when people are\nasked how often they are \"really irritated\" and given response options\nranging from \"less than once a year\" to \"more than once a month,\" they\ntend to think of major irritations and report being irritated\ninfrequently. But when they are given response options ranging from\n\"less than once a day\" to \"several times a month,\" they tend to think of\nminor irritations and report being irritated frequently. People also\ntend to assume that middle response options represent what is normal or\ntypical. So if they think of themselves as normal or typical, they tend\nto choose middle response options. For example, people are likely to\nreport watching more television when the response options are centered\non a middle option of 4 hours than when centered on a middle option of 2\nhours.\n\n### Writing Survey Questionnaire Items {.unnumbered}\n\n#### Types of Items {.unnumbered}\n\nQuestionnaire items can be either open-ended or closed-ended.\n[Open-ended items](#open-ended-item) simply ask a question and allow\nparticipants to answer in whatever way they choose. The following are\nexamples of open-ended questionnaire items.\n\n> What is the most important thing to teach children to prepare them for\n> life?\n\n> Please describe a time when you were discriminated against because of\n> your age.\n\n> Is there anything else you would like to tell us about?\n\nOpen-ended items are useful when researchers do not know how\nparticipants might respond or want to avoid influencing their responses.\nThey tend to be used when researchers have more vaguely defined research\nquestions---often in the early stages of a research project. Open-ended\nitems are relatively easy to write because there are no response options\nto worry about. However, they take more time and effort on the part of\nparticipants, and they are more difficult for the researcher to analyze\nbecause the answers must be transcribed, coded, and submitted to some\nform of content analysis.\n\n[Closed-ended items](#closed-ended%20item) ask a question and provide a\nset of response options for participants to choose from. The alcohol\nitem just mentioned is an example, as are the following:\n\n> How old are you?\n>\n> \\_\\_\\_\\_\\_ Under 18\\\n> \\_\\_\\_\\_\\_ 18 to 34\\\n> \\_\\_\\_\\_\\_ 35 to 49\\\n> \\_\\_\\_\\_\\_ 50 to 70\\\n> \\_\\_\\_\\_\\_ Over 70\n\n> On a scale of 0 (no pain at all) to 10 (worst pain ever experienced),\n> how much pain are you in right now?\n\n> Have you ever in your adult life been depressed for a period of 2\n> weeks or more?\n\nClosed-ended items are used when researchers have a good idea of the\ndifferent responses that participants might make. They are also used\nwhen researchers are interested in a well-defined variable or construct\nsuch as participants' level of agreement with some statement,\nperceptions of risk, or frequency of a particular behavior. Closed-ended\nitems are more difficult to write because they must include an\nappropriate set of response options. However, they are relatively quick\nand easy for participants to complete. They are also much easier for\nresearchers to analyze because the responses can be easily converted to\nnumbers and entered into a spreadsheet. For these reasons, closed-ended\nitems are much more common.\n\nAll closed-ended items include a set of response options from which a\nparticipant must choose. For categorical variables like sex, race, or\npolitical party preference, the categories are usually listed and\nparticipants choose the one (or ones) that they belong to. For\nquantitative variables, a rating scale is typically provided. A rating\nscale is an ordered set of responses that participants must choose from.\nFigure \\@ref(fig:scales) shows several examples. The number of response\noptions on a typical rating scale ranges from three to 11---although\nfive and seven are probably most common. They can consist entirely of\nverbal labels or they can consist of a set of numbers with verbal labels\nas \"anchors.\" In some cases, the verbal labels or numbers can be\nsupplemented with (or even replaced by) meaningful graphics. The last\nrating scale shown in Figure \\@ref(fig:scales) is a visual-analog scale,\non which participants make a mark somewhere along the horizontal line to\nindicate the magnitude of their response.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Example rating scales for closed-ended questionnaire items.](images/nonexperiments/scales.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n::: fyi\n##### What Is a Likert Scale? {.unnumbered}\n\nIn reading about psychological research, you are likely to encounter the\nterm *Likert scale*. Although this term is sometimes used to refer to\nalmost any rating scale (e.g., a 0-to-10 life satisfaction scale), it\nhas a much more precise meaning.\n\nIn the 1930s, researcher Rensis Likert (pronounced LICK-ert) created a\nnew approach for measuring people's attitudes [@likert1932technique]. It\ninvolves presenting people with several statements---including both\nfavorable and unfavorable statements---about some person, group, or\nidea. Respondents then express their agreement or disagreement with each\nstatement on a 5-point scale: *Strongly Agree, Agree, Neither Agree nor\nDisagree, Disagree, Strongly Disagree*. Numbers are assigned to each\nresponse (with reverse coding as necessary) and then summed across all\nitems to produce a score representing the attitude toward the person,\ngroup, or idea. The entire set of items came to be called a Likert\nscale.\n\nThus unless you are measuring people's attitude toward something by\nassessing their level of agreement with several statements about it, it\nis best to avoid calling it a Likert scale. You are probably just using\na \"rating scale.\"\n:::\n\n### Writing Effective Items {.unnumbered}\n\nWe can now consider some principles of writing questionnaire items that\nminimize unintended context effects and maximize the reliability and\nvalidity of participants' responses. A rough guideline for writing\nquestionnaire items is provided by the BRUSO model\n[@peterson2000constructing]. An acronym, [BRUSO] stands for \"brief,\"\n\"relevant,\" \"unambiguous,\" \"specific,\" and \"objective.\" Effective\nquestionnaire items are *brief* and to the point. They avoid long,\noverly technical, or unnecessary words. This makes them easier for\nrespondents to understand and faster for them to complete. Effective\nquestionnaire items are also *relevant* to the research question. If a\nrespondent's sexual orientation, marital status, or income is not\nrelevant, then items on them should probably not be included. Again,\nthis makes the questionnaire faster to complete, but it also avoids\nannoying respondents with what they will rightly perceive as irrelevant\nor even \"nosy\" questions. Effective questionnaire items are also\n*unambiguous*; they can be interpreted in only one way. Part of the\nproblem with the alcohol item presented earlier in this section is that\ndifferent respondents might have different ideas about what constitutes\n\"an alcoholic drink\" or \"a typical day.\" Effective questionnaire items\nare also *specific*, so that it is clear to respondents what their\nresponse should be about and clear to researchers what it is about. A\ncommon problem here is closed-ended items that are \"double barreled.\"\nThey ask about two conceptually separate issues but allow only one\nresponse. For example, \"Please rate the extent to which you have been\nfeeling anxious and depressed.\" This item should probably be split into\ntwo separate items---one about anxiety and one about depression.\nFinally, effective questionnaire items are *objective* in the sense that\nthey do not reveal the researcher's own opinions or lead participants to\nanswer in a particular way. Table 9.2 \"BRUSO Model of Writing Effective\nQuestionnaire Items, Plus Examples\" shows some examples of poor and\neffective questionnaire items based on the BRUSO criteria.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: BRUSO model of writing effective questionnaire items, plus examples.\n\n|Criterion     |Poor                                                                      |Effective                                                                                                      |\n|:-------------|:-------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------|\n|B—Brief       |“Are you now or have you ever been the possessor of a firearm?”           |“Have you ever owned a gun?”                                                                                   |\n|R—Relevant    |“What is your sexual orientation?”                                        |Do not include this item unless it is clearly relevant to the research.                                        |\n|U—Unambiguous |“Are you a gun person?”                                                   |“Do you currently own a gun?”                                                                                  |\n|S—Specific    |“How much have you read about the new gun control measure and sales tax?” |“How much have you read about the new gun control measure?”  “How much have you read about the new sales tax?” |\n|O—Objective   |“How much do you support the new gun control measure?”                    |“What is your view of the new gun control measure?”                                                            |\n:::\n:::\n\n\n\nFor closed-ended items, it is also important to create an appropriate\nresponse scale. For categorical variables, the categories presented\nshould generally be mutually exclusive and exhaustive. Mutually\nexclusive categories do not overlap. For a religion item, for example,\nthe categories of *Christian* and *Catholic* are not mutually exclusive\nbut *Protestant* and *Catholic* are. Exhaustive categories cover all\npossible responses. Although *Protestant* and *Catholic* are mutually\nexclusive, they are not exhaustive because there are many other\nreligious categories that a respondent might select: *Jewish*, *Hindu*,\n*Buddhist*, and so on. In many cases, it is not feasible to include\nevery possible category, in which case an *Other* category, with a space\nfor the respondent to fill in a more specific response, is a good\nsolution. If respondents could belong to more than one category (e.g.,\nrace), they should be instructed to choose all categories that apply.\n\nFor rating scales, five or seven response options generally allow about\nas much precision as respondents are capable of. However, numerical\nscales with more options can sometimes be appropriate. For dimensions\nsuch as attractiveness, pain, and likelihood, a 0-to-10 scale will be\nfamiliar to many respondents and easy for them to use. Regardless of the\nnumber of response options, the most extreme ones should generally be\n\"balanced\" around a neutral or modal midpoint. An example of an\nunbalanced rating scale measuring perceived likelihood might look like\nthis:\n\n> Unlikely \\| Somewhat Likely \\| Likely \\| Very Likely \\| Extremely\n> Likely\n\nA balanced version might look like this:\n\n> Extremely Unlikely \\| Somewhat Unlikely \\| As Likely as Not \\|\n> Somewhat Likely \\| Extremely Likely\n\nNote, however, that a middle or neutral response option does not have to\nbe included. Researchers sometimes choose to leave it out because they\nwant to encourage respondents to think more deeply about their response\nand not simply choose the middle option by default.\n\nNumerical rating scales often begin at 1 and go up to 5 or 7. However,\nthey can also begin at 0 if the lowest response option means the\ncomplete absence of something (e.g., no pain). They can also have 0 as\ntheir midpoint, but it is important to think about how this might change\npeople's interpretation of the response options. For example, when asked\nto rate how successful in life they have been on a 0-to-10 scale, many\npeople use numbers in the lower half of the scale because they interpret\nthis to mean that they have been only somewhat successful in life. But\nwhen asked to rate how successful they have been in life on a −5 to +5\nscale, very few people use numbers in the lower half of the scale\nbecause they interpret this to mean they have actually been unsuccessful\nin life [@schwarz1999self].\n\n### Formatting the Questionnaire {.unnumbered}\n\nWriting effective items is only one part of constructing a survey\nquestionnaire. For one thing, every survey questionnaire should have a\nwritten or spoken introduction that serves two basic functions\n[@peterson2000constructing]. One is to encourage respondents to\nparticipate in the survey. In many types of research, such encouragement\nis not necessary either because participants do not know they are in a\nstudy (as in naturalistic observation) or because they are part of a\nsubject pool and have already shown their willingness to participate by\nsigning up and showing up for the study. Survey research usually catches\nrespondents by surprise when they answer their phone, go to their\nmailbox, or check their e-mail---and the researcher must make a good\ncase for why they should agree to participate. Thus the introduction\nshould briefly explain the purpose of the survey and its importance,\nprovide information about the sponsor of the survey (university-based\nsurveys tend to generate higher response rates), acknowledge the\nimportance of the respondent's participation, and describe any\nincentives for participating.\n\nThe second function of the introduction is to establish informed\nconsent. Remember that this means describing to respondents everything\nthat might affect their decision to participate. This includes the\ntopics covered by the survey, the amount of time it is likely to take,\nthe respondent's option to withdraw at any time, confidentiality issues,\nand so on. Written consent forms are not typically used in survey\nresearch, so it is important that this part of the introduction be well\ndocumented and presented clearly and in its entirety to every\nrespondent.\n\nThe introduction should be followed by the substantive questionnaire\nitems. But first, it is important to present clear instructions for\ncompleting the questionnaire, including examples of how to use any\nunusual response scales. Remember that this is the point at which\nrespondents are usually most interested and least fatigued, so it is\ngood practice to start with the most important items for purposes of the\nresearch and proceed to less important items. Items should also be\ngrouped by topic or by type. For example, items using the same rating\nscale (e.g., a 5-point agreement scale) should be grouped together if\npossible to make things faster and easier for respondents. Demographic\nitems are often presented last because they are least interesting to\nparticipants but also easy to answer in the event respondents have\nbecome tired or bored. Of course, any survey should end with an\nexpression of appreciation to the respondent.\n\n### Conducting the Survey {.unnumbered}\n\nThe four main ways to conduct surveys are through in-person interviews,\nby telephone, through the mail, and over the Internet. As with other\naspects of survey design, the choice depends on both the researcher's\ngoals and the budget. In-person interviews have the highest response\nrates and provide the closest personal contact with respondents.\nPersonal contact can be important, for example, when the interviewer\nmust see and make judgments about respondents, as is the case with some\nmental health interviews. But in-person interviewing is by far the most\ncostly approach. Telephone surveys have lower response rates and still\nprovide some personal contact with respondents. They can also be costly\nbut are generally less so than in-person interviews. Traditionally,\ntelephone directories have provided fairly comprehensive sampling\nframes. Mail surveys are less costly still but generally have even lower\nresponse rates---making them most susceptible to nonresponse bias.\n\nNot surprisingly, Internet surveys are becoming more common. They are\nincreasingly easy to construct and use (see \"Online Survey Creation\").\nAlthough initial contact can be made by mail with a link provided to the\nsurvey, this approach does not necessarily produce higher response rates\nthan an ordinary mail survey. A better approach is to make initial\ncontact by e-mail with a link directly to the survey. This can work well\nwhen the population consists of the members of an organization who have\nknown e-mail addresses and regularly use them (e.g., a university\ncommunity). For other populations, it can be difficult or impossible to\nfind a comprehensive list of e-mail addresses to serve as a sampling\nframe. Alternatively, a request to participate in the survey with a link\nto it can be posted on websites known to be visited by members of the\npopulation. But again it is very difficult to get anything approaching a\nrandom sample this way because the members of the population who visit\nthe websites are likely to be different from the population as a whole.\nHowever, Internet survey methods are in rapid development. Because of\ntheir low cost, and because more people are online than ever before,\nInternet surveys are likely to become the dominant approach to survey\ndata collection in the near future.\n\n::: fyi\n##### Online Survey Creation {.unnumbered}\n\nThere are now several online tools for creating online questionnaires.\nAfter a questionnaire is created, a link to it can then be e-mailed to\npotential respondents or embedded in a web page. The following websites\nare among those that offer free accounts. Although the free accounts\nlimit the number of questionnaire items and the number of respondents,\nthey can be useful for doing small-scale surveys and for practicing the\nprinciples of good questionnaire construction.\n\n-   LimeSurvey---https://www.limesurvey.org\n-   Qualtrics---https://www.qualtrics.com\n-   Alchemer---https://www.alchemer.com\n-   SurveyMonkey---https://www.surveymonkey.com\n:::\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Responding to a survey item is itself a complex cognitive process\n    that involves interpreting the question, retrieving information,\n    making a tentative judgment, putting that judgment into the required\n    response format, and editing the response.\n-   Survey questionnaire responses are subject to numerous context\n    effects due to question wording, item order, response options, and\n    other factors. Researchers should be sensitive to such effects when\n    constructing surveys and interpreting survey results.\n-   Survey questionnaire items are either open-ended or closed-ended.\n    Open-ended items simply ask a question and allow respondents to\n    answer in whatever way they want. Closed-ended items ask a question\n    and provide several response options that respondents must choose\n    from.\n-   According to the BRUSO model, questionnaire items should be brief,\n    relevant, unambiguous, specific, and objective.\n-   Survey research is a quantitative approach that features the use of\n    self-report measures on carefully selected samples. It is a flexible\n    approach that can be used to study a wide variety of basic and\n    applied research questions.\n-   Survey research has its roots in applied social research, market\n    research, and election polling. It has since become an important\n    approach in many academic disciplines, including political science,\n    sociology, public health, and, of course, psychology.\n-   Surveys can be conducted in person, by telephone, through the mail,\n    and on the Internet. In-person interviewing has the highest response\n    rates but is the most expensive. Mail and Internet surveys are less\n    expensive but have much lower response rates. Internet surveys are\n    likely to become the dominant approach because of their low cost.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Discussion: Write a survey item and then write a short description\n    of how someone might respond to that item based on the cognitive\n    model of survey responding (or choose any item on the [Rosenberg\n    Self-Esteem\n    Scale](https://socy.umd.edu/about-us/using-rosenberg-self-esteem-scale).\n2.  Practice: Write survey questionnaire items for each of the following\n    general questions. In some cases, a series of items, rather than a\n    single item, might be necessary.\n    a.  How much does the respondent use Facebook?\n    b.  How much exercise does the respondent get?\n    c.  How likely does the respondent think it is that the incumbent\n        will be reelected in the next presidential election?\n    d.  To what extent does the respondent experience \"road rage\"?\n3.  Discussion: Think of a question that each of the following\n    professionals might try to answer using survey research.\n    a.  a social psychologist\n    b.  an educational researcher\n    c.  a market researcher who works for a supermarket chain\n    d.  the mayor of a large city\n    e.  the head of a university police force\n4.  Practice: Use one of the online survey creation tools to create a\n    10-item survey questionnaire on a topic of your choice.\n:::\n\n## Quasi-Experimental Research\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Explain what quasi-experimental research is and distinguish it\n    clearly from both experimental and correlational research.\n2.  Describe three different types of quasi-experimental research\n    designs (nonequivalent groups, pretest-posttest, and interrupted\n    time series) and identify examples of each one.\n:::\n\nThe prefix *quasi* means \"resembling.\" Thus quasi-experimental research\nis research that resembles experimental research but is not true\nexperimental research. Although the independent variable is manipulated,\nparticipants are not randomly assigned to conditions or orders of\nconditions [@cook1979quasi]. Because the independent variable is\nmanipulated before the dependent variable is measured,\nquasi-experimental research eliminates the directionality problem. But\nbecause participants are not randomly assigned---making it likely that\nthere are other differences between conditions---quasi-experimental\nresearch does not eliminate the problem of confounding variables. In\nterms of internal validity, therefore, quasi-experiments are generally\nsomewhere between correlational studies and true experiments.\n\nQuasi-experiments are most likely to be conducted in field settings in\nwhich random assignment is difficult or impossible. They are often\nconducted to evaluate the effectiveness of a treatment---perhaps a type\nof psychotherapy or an educational intervention. There are many\ndifferent kinds of quasi-experiments, but we will discuss just a few of\nthe most common ones here.\n\n### Nonequivalent Groups Design {.unnumbered}\n\nRecall that when participants in a between-subjects experiment are\nrandomly assigned to conditions, the resulting groups are likely to be\nquite similar. In fact, researchers consider them to be equivalent. When\nparticipants are not randomly assigned to conditions, however, the\nresulting groups are likely to be dissimilar in some ways. For this\nreason, researchers consider them to be nonequivalent. A [nonequivalent\ngroups design], then, is a between-subjects design in which participants\nhave not been randomly assigned to conditions.\n\nImagine, for example, a researcher who wants to evaluate a new method of\nteaching fractions to third graders. One way would be to conduct a study\nwith a treatment group consisting of one class of third-grade students\nand a control group consisting of another class of third-grade students.\nThis would be a nonequivalent groups design because the students are not\nrandomly assigned to classes by the researcher, which means there could\nbe important differences between them. For example, the parents of\nhigher achieving or more motivated students might have been more likely\nto request that their children be assigned to Ms. Williams's class. Or\nthe principal might have assigned the \"troublemakers\" to Mr. Jones's\nclass because he is a stronger disciplinarian. Of course, the teachers'\nstyles, and even the classroom environments, might be very different and\nmight cause different levels of achievement or motivation among the\nstudents. If at the end of the study there was a difference in the two\nclasses' knowledge of fractions, it might have been caused by the\ndifference between the teaching methods---but it might have been caused\nby any of these confounding variables.\n\nOf course, researchers using a nonequivalent groups design can take\nsteps to ensure that their groups are as similar as possible. In the\npresent example, the researcher could try to select two classes at the\nsame school, where the students in the two classes have similar scores\non a standardized math test and the teachers are the same sex, are close\nin age, and have similar teaching styles. Taking such steps would\nincrease the internal validity of the study because it would eliminate\nsome of the most important confounding variables. But without true\nrandom assignment of the students to conditions, there remains the\npossibility of other important confounding variables that the researcher\nwas not able to control.\n\n### Pretest-Posttest Design {.unnumbered}\n\nIn a [pretest-posttest design], the dependent variable is measured once\nbefore the treatment is implemented and once after it is implemented.\nImagine, for example, a researcher who is interested in the\neffectiveness of an antidrug education program on elementary school\nstudents' attitudes toward illegal drugs. The researcher could measure\nthe attitudes of students at a particular elementary school during one\nweek, implement the antidrug program during the next week, and finally,\nmeasure their attitudes again the following week. The pretest-posttest\ndesign is much like a within-subjects experiment in which each\nparticipant is tested first under the control condition and then under\nthe treatment condition. It is unlike a within-subjects experiment,\nhowever, in that the order of conditions is not counterbalanced because\nit typically is not possible for a participant to be tested in the\ntreatment condition first and then in an \"untreated\" control condition.\n\nIf the average posttest score is better than the average pretest score,\nthen it makes sense to conclude that the treatment might be responsible\nfor the improvement. Unfortunately, one often cannot conclude this with\na high degree of certainty because there may be other explanations for\nwhy the posttest scores are better. One category of alternative\nexplanations goes under the name of [history]. Other things might have\nhappened between the pretest and the posttest. Perhaps an antidrug\nprogram aired on television and many of the students watched it, or\nperhaps a celebrity died of a drug overdose and many of the students\nheard about it. Another category of alternative explanations goes under\nthe name of [maturation]. Participants might have changed between the\npretest and the posttest in ways that they were going to anyway because\nthey are growing and learning. If it were a yearlong program,\nparticipants might become less impulsive or better reasoners and this\nmight be responsible for the change.\n\nAnother alternative explanation for a change in the dependent variable\nin a pretest-posttest design is [regression to the mean]. This refers to\nthe statistical fact that an individual who scores extremely on a\nvariable on one occasion will tend to score less extremely on the next\noccasion. For example, a bowler with a long-term average of 150 who\nsuddenly bowls a 220 will almost certainly score lower in the next game.\nHer score will \"regress\" toward her mean score of 150. Regression to the\nmean can be a problem when participants are selected for further study\nbecause of their extreme scores. Imagine, for example, that only\nstudents who scored especially low on a test of fractions are given a\nspecial training program and then retested. Regression to the mean all\nbut guarantees that their scores will be higher even if the training\nprogram has no effect. A closely related concept---and an extremely\nimportant one in psychological research---is [spontaneous remission].\nThis is the tendency for many medical and psychological problems to\nimprove over time without any form of treatment. The common cold is a\ngood example. If one were to measure symptom severity in 100 common cold\nsufferers today, give them a bowl of chicken soup every day, and then\nmeasure their symptom severity again in a week, they would probably be\nmuch improved. This does not mean that the chicken soup was responsible\nfor the improvement, however, because they would have been much improved\nwithout any treatment at all. The same is true of many psychological\nproblems. A group of severely depressed people today is likely to be\nless depressed on average in 6 months. In reviewing the results of\nseveral studies of treatments for depression, researchers Michael\nPosternak and Ivan Miller found that participants in waitlist control\nconditions improved an average of 10 to 15% before they received any\ntreatment at all [@posternak2001untreated]. Thus one must generally be\nvery cautious about inferring causality from pretest-posttest designs.\n\n::: fyi\n##### Does Psychotherapy Work? {.unnumbered}\n\nEarly studies on the effectiveness of psychotherapy tended to use\npretest-posttest designs. In a classic 1952 article, researcher Hans\nEysenck summarized the results of 24 such studies showing that about two\nthirds of patients improved between the pretest and the posttest\n[@eysenck1952effects]. But Eysenck also compared these results with\narchival data from state hospital and insurance company records showing\nthat similar patients recovered at about the same rate without receiving\npsychotherapy. This suggested to Eysenck that the improvement that\npatients showed in the pretest-posttest studies might be no more than\nspontaneous remission. Note that Eysenck did not conclude that\npsychotherapy was ineffective. He merely concluded that there was no\nevidence that it was, and he wrote of \"the necessity of properly planned\nand executed experimental studies into this important field\" (p. 323).\nYou can read the entire article\n[here](http://psychclassics.yorku.ca/Eysenck/psychotherapy.htm).\n\nFortunately, many other researchers took up Eysenck's challenge, and by\n1980 hundreds of experiments had been conducted in which participants\nwere randomly assigned to treatment and control conditions, and the\nresults were summarized in a classic book by Mary Lee Smith, Gene Glass,\nand Thomas Miller [@smith1980benefits]. They found that overall\npsychotherapy was quite effective, with about 80% of treatment\nparticipants improving more than the average control participant.\nSubsequent research has focused more on the conditions under which\ndifferent types of psychotherapy are more or less effective.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![In a classic 1952 article, researcher Hans Eysenck pointed out the shortcomings of the simple pretest-posttest design for evaluating the effectiveness of psychotherapy. *Source: Sirswindon on Wikimedia Commons.*](images/nonexperiments/eysenck.jpg){fig-align='center' fig-alt='A photograph of Hans Eysenck wearing brown square glasses, a tan blazer, a brown shirt, and a brown tie.' width=35%}\n:::\n:::\n\n\n:::\n\n### Interrupted Time Series Design {.unnumbered}\n\nA variant of the pretest-posttest design is the [interrupted time-series\ndesign](#interrupted-time-series-design-1). A time series is a set of\nmeasurements taken at intervals over a period of time. For example, a\nmanufacturing company might measure its workers' productivity each week\nfor a year. In an interrupted time series-design, a time series like\nthis is \"interrupted\" by a treatment. In one classic example, the\ntreatment was the reduction of the work shifts in a factory from 10\nhours to 8 hours [@cook1979quasi]. Because productivity increased rather\nquickly after the shortening of the work shifts, and because it remained\nelevated for many months afterward, the researcher concluded that the\nshortening of the shifts caused the increase in productivity. Notice\nthat the interrupted time-series design is like a pretest-posttest\ndesign in that it includes measurements of the dependent variable both\nbefore and after the treatment. It is unlike the pretest-posttest\ndesign, however, in that it includes multiple pretest and posttest\nmeasurements.\n\nFigure \\@ref(fig:timeseries) shows data from a hypothetical interrupted\ntime-series study. The dependent variable is the number of student\nabsences per week in a research methods course. The treatment is that\nthe instructor begins publicly taking attendance each day so that\nstudents know that the instructor is aware of who is present and who is\nabsent. The top panel of Figure \\@ref(fig:timeseries) shows how the data\nmight look if this treatment worked. There is a consistently high number\nof absences before the treatment, and there is an immediate and\nsustained drop in absences after the treatment. The bottom panel of\nFigure \\@ref(fig:timeseries) shows how the data might look if this\ntreatment did not work. On average, the number of absences after the\ntreatment is about the same as the number before. This figure also\nillustrates an advantage of the interrupted time-series design over a\nsimpler pretest-posttest design. If there had been only one measurement\nof absences before the treatment at Week 7 and one afterward at Week 8,\nthen it would have looked as though the treatment were responsible for\nthe reduction. The multiple measurements both before and after the\ntreatment suggest that the reduction between Weeks 7 and 8 is nothing\nmore than normal week-to-week variation.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): conversion failure on '←\ntreatment' in 'mbcsToSbcs': dot substituted for <e2>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): conversion failure on '←\ntreatment' in 'mbcsToSbcs': dot substituted for <86>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): conversion failure on '←\ntreatment' in 'mbcsToSbcs': dot substituted for <90>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): font metrics unknown for\nUnicode character U+2190\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): conversion failure on '←\ntreatment' in 'mbcsToSbcs': dot substituted for <e2>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): conversion failure on '←\ntreatment' in 'mbcsToSbcs': dot substituted for <86>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): conversion failure on '←\ntreatment' in 'mbcsToSbcs': dot substituted for <90>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in text.default(8.8, 7.5, \"← treatment\"): font metrics unknown for\nUnicode character U+2190\n```\n:::\n\n::: {.cell-output-display}\n![Hypothetical interrupted time-series design. The top panel shows data that suggest that the treatment caused a reduction in absences. The bottom panel shows data that suggest that it did not.](09-nonexperiments_files/figure-pdf/timeseries-1.pdf){fig-align='center' fig-alt='Two line graphs. The x-axes on both are labeled Week and range from 0 to 14. The y-axes on both are labeled Absences and range from 0 to 8. Between weeks 7 and 8 a vertical dotted line indicates when a treatment was introduced. Both graphs show generally high levels of absences from weeks 1 through 7 (before the treatment) and only 2 absences in week 8 (the first observation after the treatment). The top graph shows the absence level staying low from weeks 9 to 14. The bottom graph shows the absence level for weeks 9 to 15 bouncing around at the same high levels as before the treatment.' width=60%}\n:::\n\n::: {.cell-output-display}\n![Hypothetical interrupted time-series design. The top panel shows data that suggest that the treatment caused a reduction in absences. The bottom panel shows data that suggest that it did not.](09-nonexperiments_files/figure-pdf/timeseries-2.pdf){fig-align='center' fig-alt='Two line graphs. The x-axes on both are labeled Week and range from 0 to 14. The y-axes on both are labeled Absences and range from 0 to 8. Between weeks 7 and 8 a vertical dotted line indicates when a treatment was introduced. Both graphs show generally high levels of absences from weeks 1 through 7 (before the treatment) and only 2 absences in week 8 (the first observation after the treatment). The top graph shows the absence level staying low from weeks 9 to 14. The bottom graph shows the absence level for weeks 9 to 15 bouncing around at the same high levels as before the treatment.' width=60%}\n:::\n:::\n\n\n\n### Combination Designs {.unnumbered}\n\nA type of quasi-experimental design that is generally better than either\nthe nonequivalent groups design or the pretest-posttest design is one\nthat combines elements of both. There is a treatment group that is given\na pretest, receives a treatment, and then is given a posttest. But at\nthe same time there is a control group that is given a pretest, does\n*not* receive the treatment, and then is given a posttest. The question,\nthen, is not simply whether participants who receive the treatment\nimprove but whether they improve *more* than participants who do not\nreceive the treatment.\n\nImagine, for example, that students in one school are given a pretest on\ntheir attitudes toward drugs, then are exposed to an antidrug program,\nand finally are given a posttest. Students in a similar school are given\nthe pretest, not exposed to an antidrug program, and finally are given a\nposttest. Again, if students in the treatment condition become more\nnegative toward drugs, this could be an effect of the treatment, but it\ncould also be a matter of history or maturation. If it really is an\neffect of the treatment, then students in the treatment condition should\nbecome more negative than students in the control condition. But if it\nis a matter of history (e.g., news of a celebrity drug overdose) or\nmaturation (e.g., improved reasoning), then students in the two\nconditions would be likely to show similar amounts of change. This type\nof design does not completely eliminate the possibility of confounding\nvariables, however. Something could occur at one of the schools but not\nthe other (e.g., a student drug overdose), so students at the first\nschool would be affected by it while students at the other school would\nnot.\n\nFinally, if participants in this kind of design are randomly assigned to\nconditions, it becomes a true experiment rather than a quasi experiment.\nIn fact, it is the kind of experiment that Eysenck called for---and that\nhas now been conducted many times---to demonstrate the effectiveness of\npsychotherapy.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Quasi-experimental research involves the manipulation of an\n    independent variable without the random assignment of participants\n    to conditions or orders of conditions. Among the important types are\n    nonequivalent groups designs, pretest-posttest, and interrupted\n    time-series designs.\n-   Quasi-experimental research eliminates the directionality problem\n    because it involves the manipulation of the independent variable. It\n    does not eliminate the problem of confounding variables, however,\n    because it does not involve random assignment to conditions. For\n    these reasons, quasi-experimental research is generally higher in\n    internal validity than correlational studies but lower than true\n    experiments.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: Imagine that two college professors decide to test the\n    effect of giving daily quizzes on student performance in a\n    statistics course. They decide that Professor A will give quizzes\n    but Professor B will not. They will then compare the performance of\n    students in their two sections on a common final exam. List five\n    other variables that might differ between the two sections that\n    could affect the results.\n2.  Discussion: Imagine that a group of obese children is recruited for\n    a study in which their weight is measured, then they participate for\n    3 months in a program that encourages them to be more active, and\n    finally their weight is measured again. Explain how each of the\n    following might affect the results:\n    a.  regression to the mean\n    b.  spontaneous remission\n    c.  history\n    d.  maturation\n:::\n\n## Glossary\n\n##### archival data {.unnumbered}\n\nExisting data that were collected or created for some other purpose.\nThey can include school and hospital records, newspaper and magazine\narticles, Internet content, television shows, and many other things.\n\n##### BRUSO {.unnumbered}\n\nA prescriptive model for writing good questionnaire items. They should\nbe brief, relevant, unambiguous, specific, and objective.\n\n##### closed-ended item {.unnumbered}\n\nA questionnaire item that asks a question and provides a set of response\noptions for respondents to choose from.\n\n##### coding {.unnumbered}\n\nAn approach to measurement in naturalistic observation, in which target\nbehaviors are specified ahead of time and observers watch for and record\nthose specific behaviors.\n\n##### content analysis {.unnumbered}\n\nA family of techniques for analyzing archival data that generally\ninvolves identifying specific words, phrases, ideas, or other content in\nthe data and then counting or summarizing their occurrence in other\nquantitative ways.\n\n##### correlational research {.unnumbered}\n\nResearch in which two or more variables are measured and the statistical\nrelationships among them are assessed. There is no manipulated\nindependent variable and usually very little attempt to control\nextraneous variables.\n\n##### history {.unnumbered}\n\nRefers collectively to extraneous events that can occur between a\npretest and posttest or between the first and last measurements in a\ntime series. It can provide alternative explanations for an observed\nchange in the dependent variable.\n\n##### interrupted time-series design {#interrupted-time-series-design-1 .unnumbered}\n\nA research design in which a series of measurements of the dependent\nvariable are taken both before and after a treatment.\n\n##### item-order effect {.unnumbered}\n\nThe effect of responding to one survey item on responses to a later\nsurvey item.\n\n##### maturation {.unnumbered}\n\nRefers collectively to extraneous developmental changes in participants\nthat can occur between a pretest and posttest or between the first and\nlast measurements in a time series. It can provide an alternative\nexplanation for an observed change in the dependent variable.\n\n##### naturalistic observation {.unnumbered}\n\nAn approach to data collection in which the behavior of interest is\nobserved in the environment in which it typically occurs.\n\n##### nonequivalent groups design {.unnumbered}\n\nA between-subjects research design in which participants are not\nrandomly assigned to conditions, usually because participants are in\npreexisting groups (e.g., students at different schools).\n\n##### nonexperimental research {.unnumbered}\n\nResearch that lacks the manipulation of an independent variable or the\nrandom assignment of participants to conditions or orders of conditions.\n\n##### open-ended item {#open-ended-item .unnumbered}\n\nA questionnaire item that asks a question and allows respondents to\nrespond in whatever way they want.\n\n##### pretest-posttest design {.unnumbered}\n\nA research design in which the dependent variable is measured (the\npretest), a treatment is given, and the dependent variable is measured\nagain (the posttest) to see if there is a change in the dependent\nvariable from pretest to posttest.\n\n##### quasi-experimental research {.unnumbered}\n\nResearch that involves the manipulation of an independent variable but\nlacks the random assignment of participants to conditions or orders of\nconditions. It is generally used in field settings to test the\neffectiveness of a treatment.\n\n##### rating scale {.unnumbered}\n\nAn ordered set of response options to a closed-ended questionnaire item.\n\n##### respondent {#respondent .unnumbered}\n\nA term often used to refer to a participant in survey research.\n\n##### regression to the mean {.unnumbered}\n\nThe statistical fact that an individual who scores extremely on one\noccasion will tend to score less extremely on the next occasion.\n\n##### single-variable research {.unnumbered}\n\nResearch that focuses on a single variable rather than on a statistical\nrelationship between variables.\n\n##### spontaneous remission {.unnumbered}\n\nImprovement in a psychological or medical problem over time without any\ntreatment.\n\n##### survey research {.unnumbered}\n\nA quantitative research approach that uses self-report measures and\nlarge, carefully selected samples.\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "09-nonexperiments_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}