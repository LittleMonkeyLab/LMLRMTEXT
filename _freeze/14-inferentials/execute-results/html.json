{
  "hash": "08fbf24d6120d43878a9baa8ac5af632",
  "result": {
    "markdown": "# Inferential Statistics\n\nRecall that Matias Mehl and his colleagues, in their study of sex\ndifferences in talkativeness, found that the women in their sample spoke\na mean of 16,215 words per day and the men a mean of 15,669 words per\nday [@mehl2007women]. But despite this sex difference in their sample,\nthey concluded that there was no evidence of a sex difference in\ntalkativeness in the population. Recall also that Allen Kanner and his\ncolleagues, in their study of the relationship between daily hassles and\nsymptoms, found a correlation of +.60 in their sample\n[@kanner1981comparison]. But they concluded that this means there *is* a\nrelationship between hassles and symptoms in the population. This raises\nthe question of how researchers can say whether their sample result\nreflects something that is true of the population.\n\nThe answer to this question is that they use a set of techniques called\ninferential statistics, which is what this chapter is about. We focus,\nin particular, on null hypothesis testing, the most common approach to\ninferential statistics in psychological research. We begin with a\nconceptual overview of null hypothesis testing, including its purpose\nand basic logic. Then we look at several null hypothesis testing\ntechniques for drawing conclusions about differences between means and\nabout correlations between quantitative variables. Finally, we consider\na few other important ideas related to null hypothesis testing,\nincluding some that can be helpful in planning new studies and\ninterpreting results. We also look at some long-standing criticisms of\nnull hypothesis testing and some ways of dealing with these criticisms.\n\n## Understanding Null Hypothesis Testing\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Explain the purpose of null hypothesis testing, including the role\n    of sampling error.\n2.  Describe the basic logic of null hypothesis testing.\n3.  Describe the role of relationship strength and sample size in\n    determining statistical significance and make reasonable judgments\n    about statistical significance based on these two factors.\n:::\n\n### The Purpose of Null Hypothesis Testing {.unnumbered}\n\nAs we have seen, psychological research typically involves measuring one\nor more variables for a sample and computing descriptive statistics for\nthat sample. In general, however, the researcher's goal is not to draw\nconclusions about that sample but to draw conclusions about the\npopulation that the sample was selected from. Thus researchers must use\nsample statistics to draw conclusions about the corresponding values in\nthe population. These corresponding values in the population are called\n[parameters](#parameter). Imagine, for example, that a researcher\nmeasures the number of depressive symptoms exhibited by each of 50\nclinically depressed adults and computes the mean number of symptoms.\nThe researcher probably wants to use this sample statistic (the mean\nnumber of symptoms for the sample) to draw conclusions about the\ncorresponding population parameter (the mean number of symptoms for\nclinically depressed adults).\n\nUnfortunately, sample statistics are not perfect estimates of their\ncorresponding population parameters. This is because there is a certain\namount of random variability in any statistic from sample to sample. The\nmean number of depressive symptoms might be 8.73 in one sample of\nclinically depressed adults, 6.45 in a second sample, and 9.44 in a\nthird---even though these samples are selected randomly from the same\npopulation. Similarly, the correlation (Pearson's *r*) between two\nvariables might be +.24 in one sample, −.04 in a second sample, and +.15\nin a third---again, even though these samples are selected randomly from\nthe same population. This random variability in a statistic from sample\nto sample is called [sampling error]. (Note that the term error here\nrefers to random variability and does not imply that anyone has made a\nmistake. No one \"commits a sampling error.\")\n\nOne implication of this is that when there is a statistical relationship\nin a sample, it is not always clear that there is a statistical\nrelationship in the population. A small difference between two group\nmeans in a sample might indicate that there is a small difference\nbetween the two group means in the population. But it could also be that\nthere is no difference between the means in the population and that the\ndifference in the sample is just a matter of sampling error. Similarly,\na Pearson's *r* value of −.29 in a sample might mean that there is a\nnegative relationship in the population. But it could also be that there\nis no relationship in the population and that the relationship in the\nsample is just a matter of sampling error.\n\nIn fact, any statistical relationship in a sample can be interpreted in\ntwo ways:\n\n1.  There is a relationship in the population, and the relationship in\n    the sample reflects this.\n2.  There is no relationship in the population, and the relationship in\n    the sample reflects only sampling error.\n\nThe purpose of null hypothesis testing is simply to help researchers\ndecide between these two interpretations.\n\n### The Logic of Null Hypothesis Testing {.unnumbered}\n\n[Null hypothesis testing] is a formal approach to deciding between two\ninterpretations of a statistical relationship in a sample. One\ninterpretation is called the [null hypothesis] (often symbolized *H*~0~\nand read as \"H-naught\"). This is the idea that there is no relationship\nin the population and that the relationship in the sample reflects only\nsampling error. Informally, the null hypothesis is that the sample\nrelationship \"occurred by chance.\" The other interpretation is called\nthe [alternative hypothesis] (often symbolized as *H*~1~). This is the\nidea that there is a relationship in the population and that the\nrelationship in the sample reflects this relationship in the population.\n\nAgain, every statistical relationship in a sample can be interpreted in\neither of these two ways: It might have occurred by chance, or it might\nreflect a relationship in the population. So researchers need a way to\ndecide between them. Although there are many specific null hypothesis\ntesting techniques, they are all based on the same general logic. The\nsteps are as follows:\n\n1.  Assume for the moment that the null hypothesis is true. There is no\n    relationship between the variables in the population.\n2.  Determine how likely the sample relationship would be if the null\n    hypothesis were true.\n3.  If the sample relationship would be extremely unlikely, then [reject\n    the null hypothesis] in favor of the alternative hypothesis. If it\n    would not be extremely unlikely, then [retain the null hypothesis].\n\nFollowing this logic, we can begin to understand why Mehl and his\ncolleagues concluded that there is no difference in talkativeness\nbetween women and men in the population. In essence, they asked the\nfollowing question: \"If there were no difference in the population, how\nlikely is it that we would find a small difference of *d* = 0.06 in our\nsample?\" Their answer to this question was that this sample relationship\nwould be fairly likely if the null hypothesis were true. Therefore, they\nretained the null hypothesis---concluding that there is no evidence of a\nsex difference in the population. We can also see why Kanner and his\ncolleagues concluded that there is a correlation between hassles and\nsymptoms in the population. They asked, \"If the null hypothesis were\ntrue, how likely is it that we would find a strong correlation of +.60\nin our sample?\" Their answer to this question was that this sample\nrelationship would be fairly unlikely if the null hypothesis were true.\nTherefore, they rejected the null hypothesis in favor of the alternative\nhypothesis---concluding that there is a positive correlation between\nthese variables in the population.\n\nA crucial step in null hypothesis testing is finding the likelihood of\nthe sample result if the null hypothesis were true. This probability is\ncalled the [*p* value]. A low *p* value means that the sample result\nwould be unlikely if the null hypothesis were true and leads to the\nrejection of the null hypothesis. A high *p* value means that the sample\nresult would be likely if the null hypothesis were true and leads to the\nretention of the null hypothesis. But how low must the *p* value be\nbefore the sample result is considered unlikely enough to reject the\nnull hypothesis? In null hypothesis testing, this criterion is called [α\n(alpha)](#α-alpha) and is almost always set to .05. If there is less\nthan a 5% chance of a result as extreme as the sample result if the null\nhypothesis were true, then the null hypothesis is rejected. When this\nhappens, the result is said to be [statistically significant]. If there\nis greater than a 5% chance of a result as extreme as the sample result\nwhen the null hypothesis is true, then the null hypothesis is retained.\nThis does not necessarily mean that the researcher accepts the null\nhypothesis as true---only that there is not currently enough evidence to\nconclude that it is true. Researchers often use the expression \"fail to\nreject the null hypothesis\" rather than \"retain the null hypothesis,\"\nbut they never use the expression \"accept the null hypothesis.\"\n\n::: fyi\n##### The Misunderstood *p* Value {.unnumbered}\n\nThe *p* value is one of the most misunderstood quantities in\npsychological research [@cohen1994earth]. Even professional researchers\nmisinterpret it, and it is not unusual for such misinterpretations to\nappear in statistics textbooks!\n\nThe most common misinterpretation is that the *p* value is the\nprobability that the null hypothesis is true---that the sample result\noccurred by chance. For example, a misguided researcher might say that\nbecause the *p* value is .02, there is only a 2% chance that the result\nis due to chance and a 98% chance that it reflects a real relationship\nin the population. But this is *incorrect.* The *p* value is really the\nprobability of a result at least as extreme as the sample result *if*\nthe null hypothesis *were* true. So a *p* value of .02 means that if the\nnull hypothesis were true, a sample result this extreme would occur only\n2% of the time.\n\nYou can avoid this misunderstanding by remembering that the *p* value is\nnot the probability that any particular *hypothesis* is true or false.\nInstead, it is the probability of obtaining the *sample result* if the\nnull hypothesis were true.\n:::\n\n### Role of Sample Size and Relationship Strength {.unnumbered}\n\nRecall that null hypothesis testing involves answering the question, \"If\nthe null hypothesis were true, what is the probability of a sample\nresult as extreme as this one?\" In other words, \"What is the *p* value?\"\nIt can be helpful to see that the answer to this question depends on\njust two considerations: the strength of the relationship and the size\nof the sample. Specifically, the stronger the sample relationship and\nthe larger the sample, the less likely the result would be if the null\nhypothesis were true. That is, the lower the *p* value. This should make\nsense. Imagine a study in which a sample of 500 women is compared with a\nsample of 500 men in terms of some psychological characteristic, and\nCohen's *d* is a strong 0.50. If there were really no sex difference in\nthe population, then a result this strong based on such a large sample\nshould seem highly unlikely. Now imagine a similar study in which a\nsample of three women is compared with a sample of three men, and\nCohen's *d* is a weak 0.10. If there were no sex difference in the\npopulation, then a relationship this weak based on such a small sample\nshould seem likely. And this is precisely why the null hypothesis would\nbe rejected in the first example and retained in the second.\n\nOf course, sometimes the result can be weak and the sample large, or the\nresult can be strong and the sample small. In these cases, the two\nconsiderations trade off against each other so that a weak result can be\nstatistically significant if the sample is large enough and a strong\nrelationship can be statistically significant even if the sample is\nsmall. Table \\@ref(tab:strength) shows roughly how relationship strength\nand sample size combine to determine whether a sample result is\nstatistically significant. The columns of the table represent the three\nlevels of relationship strength: weak, medium, and strong. The rows\nrepresent four sample sizes that can be considered small, medium, large,\nand extra large in the context of psychological research. Thus each cell\nin the table represents a combination of relationship strength and\nsample size. If a cell contains the word Yes, then this combination\nwould be statistically significant for both Cohen's *d* and Pearson's\n*r.* If it contains the word No, then it would not be statistically\nsignificant for either. There is one cell where the decision for *d* and\n*r* would be different and another where it might be different depending\non some additional considerations, which are discussed later in the\nchapter.\n\n| Sample Size             | Weak                | Medium | Strong                 |\n|:------------------------|:--------------------|:-------|:-----------------------|\n| Small (*N* = 20)        | No                  | No     | *d* = Maybe; *r* = Yes |\n| Medium (*N* = 50)       | No                  | Yes    | Yes                    |\n| Large (*N* = 100)       | *d* = Yes; *r* = No | Yes    | Yes                    |\n| Extra large (*N* = 500) | Yes                 | Yes    | Yes                    |\n\n: (#tab:strength) How relationship strength (weak, medium, strong) and\nsample size combine to determine whether a result is statistically\nsignificant.\n\nAlthough Table \\@ref(tab:strength) provides only a rough guideline, it\nshows very clearly that weak relationships based on medium or small\nsamples are never statistically significant and that strong\nrelationships based on medium or larger samples are always statistically\nsignificant. If you keep this in mind, you will often know whether a\nresult is statistically significant based on the descriptive statistics\nalone. It is extremely useful to be able to develop this kind of\nintuitive judgment. One reason is that it allows you to develop\nexpectations about how your formal null hypothesis tests are going to\ncome out, which in turn allows you to detect problems in your analyses.\nFor example, if your sample relationship is strong and your sample is\nmedium, then you would expect to reject the null hypothesis. If for some\nreason your formal null hypothesis test indicates otherwise, then you\nneed to double-check your computations and interpretations. A second\nreason is that the ability to make this kind of intuitive judgment is an\nindication that you understand the basic logic of this approach in\naddition to being able to do the computations.\n\n### Statistical Significance Versus Practical Significance {.unnumbered}\n\nTable \\@ref(tab:strength) illustrates another extremely important point.\nA statistically significant result is not necessarily a strong one. Even\na very weak result can be statistically significant if it is based on a\nlarge enough sample. This is closely related to Janet Shibley Hyde's\nargument about sex differences [@hyde2007new]. The differences between\nwomen and men in mathematical problem solving and leadership ability are\nstatistically significant. But the word *significant* can cause people\nto interpret these differences as strong and important---perhaps even\nimportant enough to influence the college courses they take or even who\nthey vote for. As we have seen, however, these statistically significant\ndifferences are actually quite weak---perhaps even \"trivial.\"\n\nThis is why it is important to distinguish between the *statistical*\nsignificance of a result and the *practical* significance of that\nresult. [Practical significance] refers to the importance or usefulness\nof the result in some real-world context. Many sex differences are\nstatistically significant---and may even be interesting for purely\nscientific reasons---but they are not practically significant. In\nclinical practice, this same concept is often referred to as \"clinical\nsignificance.\" For example, a study on a new treatment for social phobia\nmight show that it produces a statistically significant positive effect.\nYet this effect still might not be strong enough to justify the time,\neffort, and other costs of putting it into practice---especially if\neasier and cheaper treatments that work almost as well already exist.\nAlthough statistically significant, this result would be said to lack\npractical or clinical significance.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n1.  Null hypothesis testing is a formal approach to deciding whether a\n    statistical relationship in a sample reflects a real relationship in\n    the population or is just due to chance.\n2.  The logic of null hypothesis testing involves assuming that the null\n    hypothesis is true, finding how likely the sample result would be if\n    this assumption were correct, and then making a decision. If the\n    sample result would be unlikely if the null hypothesis were true,\n    then it is rejected in favor of the alternative hypothesis. If it\n    would not be unlikely, then the null hypothesis is retained.\n3.  The probability of obtaining the sample result if the null\n    hypothesis were true (the *p* value) is based on two considerations:\n    relationship strength and sample size. Reasonable judgments about\n    whether a sample relationship is statistically significant can often\n    be made by quickly considering these two factors.\n4.  Statistical significance is not the same as relationship strength or\n    importance. Even weak relationships can be statistically significant\n    if the sample size is large enough. It is important to consider\n    relationship strength and the practical significance of a result in\n    addition to its statistical significance.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Discussion: Imagine a study showing that people who eat more\n    broccoli tend to be happier. Explain for someone who knows nothing\n    about statistics why the researchers would conduct a null hypothesis\n    test.\n2.  Practice: Use Table \\@ref(tab:strength) to decide whether each of\n    the following results is statistically significant.\n    a.  The correlation between two variables is *r* = −.78 based on a\n        sample size of 137.\n    b.  The mean score on a psychological characteristic for women is 25\n        (*SD* = 5) and the mean score for men is 24 (*SD* = 5). There\n        were 12 women and 10 men in this study.\n    c.  In a memory experiment, the mean number of items recalled by the\n        40 participants in Condition A was 0.50 standard deviations\n        greater than the mean number recalled by the 40 participants in\n        Condition B.\n    d.  In another memory experiment, the mean scores for participants\n        in Condition A and Condition B came out exactly the same!\n    e.  A student finds a correlation of *r* = .04 between the number of\n        units the students in his research methods class are taking and\n        the students' level of stress.\n:::\n\n## Some Basic Null Hypothesis Tests\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Conduct and interpret one-sample, dependent-samples, and\n    independent-samples *t* tests.\n2.  Interpret the results of one-way, repeated measures, and factorial\n    ANOVAs.\n3.  Conduct and interpret null hypothesis tests of Pearson's *r*.\n:::\n\nIn this section, we look at several common null hypothesis testing\nprocedures. The emphasis here is on providing enough information to\nallow you to conduct and interpret the most basic versions. In most\ncases, the online statistical analysis tools mentioned in the chapter on\n\"Descriptive Statistics\" will handle the computations---as will programs\nsuch as Microsoft Excel and SPSS.\n\n### The *t* Test {.unnumbered}\n\nAs we have seen throughout this book, many studies in psychology focus\non the difference between two means. The most common null hypothesis\ntest for this type of statistical relationship is the [*t* test]. In\nthis section, we look at three types of *t* tests that are used for\nslightly different research designs: the one-sample *t* test, the\ndependent-samples *t* test, and the independent-samples *t* test.\n\n#### One-Sample *t* Test {.unnumbered}\n\nThe [one-sample *t* test] is used to compare a sample mean (*M*) with a\nhypothetical population mean (μ~0~) that provides some interesting\nstandard of comparison. The null hypothesis is that the mean for the\npopulation (µ) is equal to the hypothetical population mean: μ = μ~0~.\nThe alternative hypothesis is that the mean for the population is\ndifferent from the hypothetical population mean: μ ≠ μ~0~. To decide\nbetween these two hypotheses, we need to find the probability of\nobtaining the sample mean (or one more extreme) if the null hypothesis\nwere true. But finding this *p* value requires first computing a test\nstatistic called t. (A [test statistic] is a statistic that is computed\nonly to help find the *p* value.) The formula for *t* is as follows:\n\n$t=\\frac{M-\\mu_0}{\\frac{SD}{\\sqrt{N}}}$\n\nAgain, *M* is the sample mean and µ0 is the hypothetical population mean\nof interest. *SD* is the sample standard deviation and N is the sample\nsize.\n\nThe reason the *t* statistic (or any test statistic) is useful is that\nwe know how it is distributed when the null hypothesis is true. As shown\nin Figure \\@ref(fig:tdist), this distribution is unimodal and\nsymmetrical, and it has a mean of 0. Its precise shape depends on a\nstatistical concept called the degrees of freedom, which for a\none-sample *t* test is N − 1. (There are 24 degrees of freedom for the\ndistribution shown in Figure \\@ref(fig:tdist).) The important point is\nthat knowing this distribution makes it possible to find the *p* value\nfor any *t* score. Consider, for example, a *t* score of +1.50 based on\na sample of 25. The probability of a *t* score at least this extreme is\ngiven by the proportion of *t* scores in the distribution that are at\nleast this extreme. For now, let us define extreme as being far from\nzero in either direction. Thus the *p* value is the proportion of *t*\nscores that are +1.50 or above or that are −1.50 or below---a value that\nturns out to be .14.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Distribution of *t* scores (with 24 degrees of freedom) when the null hypothesis is true. The red vertical lines represent the two-tailed critical values, and the green vertical lines the one-tailed critical values when α = .05.](14-inferentials_files/figure-html/tdist-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nFortunately, we do not have to deal directly with the distribution of\n*t* scores. If we were to enter our sample data and hypothetical mean of\ninterest into one of the online statistical tools in the chapter on\n\"Descriptive Statistics\" or into a program like SPSS (Excel does not\nhave a one-sample *t* test function), the output would include both the\n*t* score and the *p* value. At this point, the rest of the procedure is\nsimple. If *p* is less than .05, we reject the null hypothesis and\nconclude that the population mean differs from the hypothetical mean of\ninterest. If *p* is greater than .05, we retain the null hypothesis and\nconclude that there is not enough evidence to say that the population\nmean differs from the hypothetical mean of interest. (Again,\ntechnically, we conclude only that we do not have enough evidence to\nconclude that it *does* differ.)\n\nIf we were to compute the *t* score by hand, we could use a table like\nTable \\@ref(tab:tcrit) to make the decision. This table does not provide\nactual *p* values. Instead, it provides the critical values of *t* for\ndifferent degrees of freedom (df) when α is .05. For now, let us focus\non the two-tailed critical values in the last column of the table. Each\nof these values should be interpreted as a pair of values: one positive\nand one negative. For example, the two-tailed critical values when there\nare 24 degrees of freedom are +2.064 and −2.064. These are represented\nby the red vertical lines in Figure \\@ref(fig:tdist). The idea is that\nany *t* score below the lower critical value (the left-hand red line in\nFigure \\@ref(fig:tdist)) is in the lowest 2.5% of the distribution,\nwhile any *t* score above the upper critical value (the right-hand red\nline) is in the highest 2.5% of the distribution. This means that any\n*t* score beyond the critical value in either direction is in the most\nextreme 5% of *t* scores when the null hypothesis is true and therefore\nhas a *p* value less than .05. Thus if the *t* score we compute is\nbeyond the critical value in either direction, then we reject the null\nhypothesis. If the *t* score we compute is between the upper and lower\ncritical values, then we retain the null hypothesis.\n\n|  df | One-tailed | Two-tailed |\n|----:|-----------:|-----------:|\n|   3 |      2.353 |      3.182 |\n|   4 |      2.132 |      2.776 |\n|   5 |      2.015 |      2.571 |\n|   6 |      1.943 |      2.447 |\n|   7 |      1.895 |      2.365 |\n|   8 |      1.860 |      2.306 |\n|   9 |      1.833 |      2.262 |\n|  10 |      1.812 |      2.228 |\n|  11 |      1.796 |      2.201 |\n|  12 |      1.782 |      2.179 |\n|  13 |      1.771 |      2.160 |\n|  14 |      1.761 |      2.145 |\n|  15 |      1.753 |      2.131 |\n|  16 |      1.746 |      2.120 |\n|  17 |      1.740 |      2.110 |\n|  18 |      1.734 |      2.101 |\n|  19 |      1.729 |      2.093 |\n|  20 |      1.725 |      2.086 |\n|  21 |      1.721 |      2.080 |\n|  22 |      1.717 |      2.074 |\n|  23 |      1.714 |      2.069 |\n|  24 |      1.711 |      2.064 |\n|  25 |      1.708 |      2.060 |\n|  30 |      1.697 |      2.042 |\n|  35 |      1.690 |      2.030 |\n|  40 |      1.684 |      2.021 |\n|  45 |      1.679 |      2.014 |\n|  50 |      1.676 |      2.009 |\n|  60 |      1.671 |      2.000 |\n|  70 |      1.667 |      1.994 |\n|  80 |      1.664 |      1.990 |\n|  90 |      1.662 |      1.987 |\n| 100 |      1.660 |      1.984 |\n\n: (#tab:tcrit) Table of critical values of *t* when α = .05.\n\nThus far, we have considered what is called a [two-tailed test], where\nwe reject the null hypothesis if the *t* score for the sample is extreme\nin either direction. This makes sense when we believe that the sample\nmean might differ from the hypothetical population mean but we do not\nhave good reason to expect the difference to go in a particular\ndirection. But it is also possible to do a [one-tailed test], where we\nreject the null hypothesis only if the *t* score for the sample is\nextreme in one direction that we specify before collecting the data.\nThis makes sense when we have good reason to expect the sample mean will\ndiffer from the hypothetical population mean in a particular direction.\n\nHere is how it works. Each one-tailed critical value in Table\n\\@ref(tab:tcrit) can again be interpreted as a pair of values: one\npositive and one negative. A *t* score below the lower critical value is\nin the lowest 5% of the distribution, and a *t* score above the upper\ncritical value is in the highest 5% of the distribution. For 24 degrees\nof freedom, these values are −1.711 and +1.711. (These are represented\nby the green vertical lines in Figure \\@ref(fig:tdist) However, for a\none-tailed test, we must decide before collecting data whether we expect\nthe sample mean to be lower than the hypothetical population mean, in\nwhich case we would use only the lower critical value, or we expect the\nsample mean to be greater than the hypothetical population mean, in\nwhich case we would use only the upper critical value. Notice that we\nstill reject the null hypothesis when the *t* score for our sample is in\nthe most extreme 5% of the *t* scores we would expect if the null\nhypothesis were true---so α remains at .05. We have simply redefined\n*extreme* to refer only to one tail of the distribution. The advantage\nof the one-tailed test is that critical values are less extreme. If the\nsample mean differs from the hypothetical population mean in the\nexpected direction, then we have a better chance of rejecting the null\nhypothesis. The disadvantage is that if the sample mean differs from the\nhypothetical population mean in the unexpected direction, then there is\nno chance at all of rejecting the null hypothesis.\n\n##### Example One-Sample *t* Test {.unnumbered}\n\nImagine that a health psychologist is interested in the accuracy of\ncollege students' estimates of the number of calories in a chocolate\nchip cookie. He shows the cookie to a sample of 10 students and asks\neach one to estimate the number of calories in it. Because the actual\nnumber of calories in the cookie is 250, this is the hypothetical\npopulation mean of interest (µ0). The null hypothesis is that the mean\nestimate for the population (μ) is 250. Because he has no real sense of\nwhether the students will underestimate or overestimate the number of\ncalories, he decides to do a two-tailed test. Now imagine further that\nthe participants' actual estimates are as follows:\n\n> 250, 280, 200, 150, 175, 200, 200, 220, 180, 250.\n\nThe mean estimate for the sample (*M*) is 212.00 calories and the\nstandard deviation (*SD*) is 39.17. The health psychologist can now\ncompute the *t* score for his sample:\n\n$t=\\frac{212-250}{\\frac{39.17}{\\sqrt{10}}}=-3.07$\n\nIf he enters the data into one of the online analysis tools or uses\nSPSS, it would also tell him that the two-tailed *p* value for this *t*\nscore (with 10 − 1 = 9 degrees of freedom) is .013. Because this is less\nthan .05, the health psychologist would reject the null hypothesis and\nconclude that college students tend to underestimate the number of\ncalories in a chocolate chip cookie. If he computes the *t* score by\nhand, he could look at Table \\@ref(tab:tcrit) and see that the critical\nvalue of *t* for a two-tailed test with 9 degrees of freedom is ±2.262.\nThe fact that his *t* score was more extreme than this critical value\nwould tell him that his *p* value is less than .05 and that he should\nreject the null hypothesis.\n\nFinally, if this researcher had gone into this study with good reason to\nexpect that college students underestimate the number of calories, then\nhe could have done a one-tailed test instead of a two-tailed test. The\nonly thing this would change is the critical value, which would be\n−1.833. This slightly less extreme value would make it a bit easier to\nreject the null hypothesis. However, if it turned out that college\nstudents overestimate the number of calories---no matter how much they\noverestimate it---the researcher would not have been able to reject the\nnull hypothesis.\n\n#### The Dependent-Samples *t* Test {.unnumbered}\n\nThe [dependent-samples *t* test] (sometimes called the paired-samples\n*t* test) is used to compare two means for the same sample tested at two\ndifferent times or under two different conditions. This makes it\nappropriate for pretest-posttest designs or within-subjects experiments.\nThe null hypothesis is that the means at the two times or under the two\nconditions are the same in the population. The alternative hypothesis is\nthat they are not the same. This test can also be one-tailed if the\nresearcher has good reason to expect the difference goes in a particular\ndirection.\n\nIt helps to think of the dependent-samples *t* test as a special case of\nthe one-sample *t* test. However, the first step in the\ndependent-samples *t* test is to reduce the two scores for each\nparticipant to a single [difference score] by taking the difference\nbetween them. At this point, the dependent-samples *t* test becomes a\none-sample *t* test on the difference scores. The hypothetical\npopulation mean (µ~0~) of interest is 0 because this is what the mean\ndifference score would be if there were no difference on average between\nthe two times or two conditions. We can now think of the null hypothesis\nas being that the mean difference score in the population is 0 (µ~0~ =\n0) and the alternative hypothesis as being that the mean difference\nscore in the population is not 0 (µ~0~ ≠ 0).\n\n##### Example Dependent-Samples *t* Test {.unnumbered}\n\nImagine that the health psychologist now knows that people tend to\nunderestimate the number of calories in junk food and has developed a\nshort training program to improve their estimates. To test the\neffectiveness of this program, he conducts a pretest-posttest study in\nwhich 10 participants estimate the number of calories in a chocolate\nchip cookie before the training program and then again afterward.\nBecause he expects the program to increase the participants' estimates,\nhe decides to do a one-tailed test. Now imagine further that the pretest\nestimates are\n\n> 230, 250, 280, 175, 150, 200, 180, 210, 220, 190\n\nand that the posttest estimates (for the same participants in the same\norder) are\n\n> 250, 260, 250, 200, 160, 200, 200, 180, 230, 240.\n\nThe difference scores, then, are as follows:\n\n> +20, +10, −30, +25, +10, 0, +20, −30, +10, +50.\n\nNote that it does not matter whether the first set of scores is\nsubtracted from the second or the second from the first as long as it is\ndone the same way for all participants. In this example, it makes sense\nto subtract the pretest estimates from the posttest estimates so that\npositive difference scores mean that the estimates went up after the\ntraining and negative difference scores mean the estimates went down.\n\nThe mean of the difference scores is 8.50 with a standard deviation of\n27.27. The health psychologist can now compute the *t* score for his\nsample as follows:\n\n$t=\\frac{8.5-0}{\\frac{27.27}{\\sqrt{10}}}=-1.11$\n\nIf he enters the data into one of the online analysis tools or uses\nExcel or SPSS, it would tell him that the one-tailed *p* value for this\n*t* score (again with 10 − 1 = 9 degrees of freedom) is .148. Because\nthis is greater than .05, he would retain the null hypothesis and\nconclude that the training program does not increase people's calorie\nestimates. If he were to compute the *t* score by hand, he could look at\nTable \\@ref(tab:tcrit) and see that the critical value of *t* for a\none-tailed test with 9 degrees of freedom is +1.833. (It is positive\nthis time because he was expecting a positive mean difference score.)\nThe fact that his *t* score was less extreme than this critical value\nwould tell him that his *p* value is greater than .05 and that he should\nfail to reject the null hypothesis.\n\n#### The Independent-Samples *t* Test {.unnumbered}\n\nThe [independent-samples *t* test] is used to compare the means of two\nseparate samples (*M*~1~ and *M*~2~). The two samples might have been\ntested under different conditions in a between-subjects experiment, or\nthey could be preexisting groups in a correlational design (e.g., women\nand men, extroverts and introverts). The null hypothesis is that the\nmeans of the two populations are the same: µ~1~ = µ~2~. The alternative\nhypothesis is that they are not the same: µ~1~ ≠ µ~2~. Again, the test\ncan be one-tailed if the researcher has good reason to expect the\ndifference goes in a particular direction.\n\nThe *t* statistic here is a bit more complicated because it must take\ninto account two sample means, two standard deviations, and two sample\nsizes. The formula is as follows:\n\n$t=\\frac{M_1-M_2}{\\sqrt{\\frac{SD_1^2}{n_1}+\\frac{SD_2^2}{n_2}}}$\n\nNotice that this formula includes squared standard deviations (the\nvariances) that appear inside the square root symbol. Also, lowercase\n*n*~1~ and *n*~2~ refer to the sample sizes in the two groups or\ncondition (as opposed to capital *N*, which generally refers to the\ntotal sample size). The only additional thing to know here is that there\nare *N* − 2 degrees of freedom for the independent-samples *t* test.\n\n##### Example Independent-Samples *t* Test {.unnumbered}\n\nNow the health psychologist wants to compare the calorie estimates of\npeople who regularly eat junk food with the estimates of people who\nrarely eat junk food. He believes the difference could come out in\neither direction so he decides to conduct a two-tailed test. He collects\ndata from a sample of eight participants who eat junk food regularly and\nseven participants who rarely eat junk food. The data are as follows:\n\n> Junk food eaters: 180, 220, 150, 85, 200, 170, 150, 190\n>\n> Non--junk food eaters: 200, 240, 190, 175, 200, 300, 240\n\nThe mean for the junk food eaters is 220.71 with a standard deviation of\n41.23. The mean for the non--junk food eaters is 168.12 with a standard\ndeviation of 42.66. He can now compute his *t* score as follows:\n\n$t=\\frac{220.71-168.12}{\\sqrt{\\frac{41.23^2}{8}+\\frac{42.66^2}{7}}}=2.42$\n\nIf he enters the data into one of the online analysis tools or uses\nExcel or SPSS, it would tell him that the two-tailed *p* value for this\n*t* score (with 15 − 2 = 13 degrees of freedom) is .015. Because this is\nless than .05, the health psychologist would reject the null hypothesis\nand conclude that people who eat junk food regularly make lower calorie\nestimates than people who eat it rarely. If he were to compute the *t*\nscore by hand, he could look at Table \\@ref(tab:tcrit) and see that the\ncritical value of *t* for a two-tailed test with 13 degrees of freedom\nis ±2.160. The fact that his *t* score was more extreme than this\ncritical value would tell him that his *p* value is less than .05 and\nthat he should fail to retain the null hypothesis.\n\n### The Analysis of Variance {.unnumbered}\n\nWhen there are more than two groups or condition means to be compared,\nthe most common null hypothesis test is the [analysis of variance\n(ANOVA)](#analysis-of-variance-anova). In this section, we look\nprimarily at the [one-way ANOVA], which is used for between-subjects\ndesigns with a single independent variable. We then briefly consider\nsome other versions of the ANOVA that are used for within-subjects and\nfactorial research designs.\n\n#### One-Way ANOVA {.unnumbered}\n\nThe one-way ANOVA is used to compare the means of more than two samples\n(*M*~1~, *M*~2~...*M*~G~) in a between-subjects design. The null\nhypothesis is that all the means are equal in the population: µ~1~= µ~2~\n=...= µ~G~. The alternative hypothesis is that not all the means in the\npopulation are equal.\n\nThe test statistic for the ANOVA is called F. It is a ratio of two\nestimates of the population variance based on the sample data. One\nestimate of the population variance is called the [mean squares between\ngroups (*MS~B~*)](#mean-squares-between-groups-msb) and is based on the\ndifferences among the sample means. The other is called the [mean\nsquares within groups (*MS~W~*)](#mean-squares-within-groups-msw) and is\nbased on the differences among the scores within each group. The *F*\nstatistic is the ratio of the *MS~B~* to the *MS~W~* and can therefore\nbe expressed as follows:\n\n$F=\\frac{MS_B}{MS_W}$\n\nAgain, the reason that *F* is useful is that we know how it is\ndistributed when the null hypothesis is true. As shown in Figure\n\\@ref(fig:fdist), this distribution is unimodal and positively skewed\nwith values that cluster around 1. The precise shape of the distribution\ndepends on both the number of groups and the sample size, and there is a\ndegrees of freedom value associated with each of these. The\nbetween-groups degrees of freedom is the number of groups minus one:\n*df~B~* = (*G* − 1). The within-groups degrees of freedom is the total\nsample size minus the number of groups: *df~W~* = *N* − *G*. Again,\nknowing the distribution of *F* when the null hypothesis is true allows\nus to find the *p* value.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Distribution of *F* ratio with 3 and 35 degrees of freedom when the null hypothesis is true. The red vertical line represents the critical value when α is .05.](14-inferentials_files/figure-html/fdist-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nThe online tools in the chapter on \"Descriptive Statistics\" and\nstatistical software such as Excel and SPSS will compute *F* and find\nthe *p* value. If *p* is less than .05, then we reject the null\nhypothesis and conclude that there are differences among the group means\nin the population. If *p* is greater than .05, then we retain the null\nhypothesis and conclude that there is not enough evidence to say that\nthere are differences. In the unlikely event that we would compute *F*\nby hand, we can use a table of critical values like Table\n\\@ref(tab:fcrit) to make the decision. The idea is that any *F* ratio\ngreater than the critical value has a *p* value of less than .05. Thus\nif the *F* ratio we compute is beyond the critical value, then we reject\nthe null hypothesis. If the *F* ratio we compute is less than the\ncritical value, then we retain the null hypothesis.\n\n|         |     2 |     3 |     4 |\n|--------:|------:|------:|------:|\n|   **8** | 4.459 | 4.066 | 3.838 |\n|   **9** | 4.256 | 3.863 | 3.633 |\n|  **10** | 4.103 | 3.708 | 3.478 |\n|  **11** | 3.982 | 3.587 | 3.357 |\n|  **12** | 3.885 | 3.490 | 3.259 |\n|  **13** | 3.806 | 3.411 | 3.179 |\n|  **14** | 3.739 | 3.344 | 3.112 |\n|  **15** | 3.682 | 3.287 | 3.056 |\n|  **16** | 3.634 | 3.239 | 3.007 |\n|  **17** | 3.592 | 3.197 | 2.965 |\n|  **18** | 3.555 | 3.160 | 2.928 |\n|  **19** | 3.522 | 3.127 | 2.895 |\n|  **20** | 3.493 | 3.098 | 2.866 |\n|  **21** | 3.467 | 3.072 | 2.840 |\n|  **22** | 3.443 | 3.049 | 2.817 |\n|  **23** | 3.422 | 3.028 | 2.796 |\n|  **24** | 3.403 | 3.009 | 2.776 |\n|  **25** | 3.385 | 2.991 | 2.759 |\n|  **30** | 3.316 | 2.922 | 2.690 |\n|  **35** | 3.267 | 2.874 | 2.641 |\n|  **40** | 3.232 | 2.839 | 2.606 |\n|  **45** | 3.204 | 2.812 | 2.579 |\n|  **50** | 3.183 | 2.790 | 2.557 |\n|  **55** | 3.165 | 2.773 | 2.540 |\n|  **60** | 3.150 | 2.758 | 2.525 |\n|  **65** | 3.138 | 2.746 | 2.513 |\n|  **70** | 3.128 | 2.736 | 2.503 |\n|  **75** | 3.119 | 2.727 | 2.494 |\n|  **80** | 3.111 | 2.719 | 2.486 |\n|  **85** | 3.104 | 2.712 | 2.479 |\n|  **90** | 3.098 | 2.706 | 2.473 |\n|  **95** | 3.092 | 2.700 | 2.467 |\n| **100** | 3.087 | 2.696 | 2.463 |\n\n: (#tab:fcrit) Table of Critical Values of *F* When α = .05. Values in\nthe first column represent *df~W~*; values in the first row represent\n*df~B~*.\n\n##### Example One-Way ANOVA {.unnumbered}\n\nImagine that the health psychologist wants to compare the calorie\nestimates of psychology majors, nutrition majors, and professional\ndieticians. He collects the following data:\n\n> Psych majors: 200, 180, 220, 160, 150, 200, 190, 200\\\n> Nutrition majors: 190, 220, 200, 230, 160, 150, 200, 210, 195\\\n> Dieticians: 220, 250, 240, 275, 250, 230, 200, 240\n\nThe means are 187.50 (*SD* = 23.14), 195.00 (*SD* = 27.77), and 238.13\n(*SD* = 22.35), respectively. So it appears that dieticians made\nsubstantially more accurate estimates on average. The researcher would\nalmost certainly enter these data into a program such as Excel or SPSS,\nwhich would compute *F* for him and find the *p* value. Table\n\\@ref(tab:anovaoutput) shows the output of the one-way ANOVA function in\nExcel for these data. This is referred to as an ANOVA table. It shows\nthat *MS~B~* is 5,971.88, *MS~W~* is 602.23, and their ratio, *F*, is\n9.92. The *p* value is .0009. Because this is below .05, the researcher\nwould reject the null hypothesis and conclude that the mean calorie\nestimates for the three groups are not the same in the population.\nNotice that the ANOVA table also includes the \"sum of squares\" (*SS*)\nfor between groups and for within groups. These values are computed on\nthe way to finding *MS~B~* and *MS~W~* but are not typically reported by\nthe researcher. Finally, if the researcher were to compute the *F* ratio\nby hand, he could look at Table \\@ref(tab:fcrit) and see that the\ncritical value of *F* with 2 and 21 degrees of freedom is 3.467 (the\nsame value in Table \\@ref(tab:anovaoutput) under Fcrit). The fact that\nhis *t* score was more extreme than this critical value would tell him\nthat his *p* value is less than .05 and that he should reject the null\nhypothesis.\n\n| *Source of variation* | *SS*      | *df* | *MS*      | F        | *p*-value | *F*~crit~ |\n|:----------------------|:----------|:-----|:----------|:---------|:----------|:----------|\n| Between groups        | 11,943.75 | 2    | 5,971.875 | 9.916234 | 0.000928  | 3.4668    |\n| Within groups         | 12,646.88 | 21   | 602.2321  |          |           |           |\n| Total                 | 24,590.63 | 23   |           |          |           |           |\n\n: (#tab:anovaoutput) Typical one-way ANOVA output from Excel\n\n#### ANOVA Elaborations {.unnumbered}\n\n##### Post Hoc Comparisons {.unnumbered}\n\nWhen we reject the null hypothesis in a one-way ANOVA, we conclude that\nthe group means are not all the same in the population. But this can\nindicate different things. With three groups, it can indicate that all\nthree means are significantly different from each other. Or it can\nindicate that one of the means is significantly different from the other\ntwo, but the other two are not significantly different from each other.\nIt could be, for example, that the mean calorie estimates of psychology\nmajors, nutrition majors, and dieticians are all significantly different\nfrom each other. Or it could be that the mean for dieticians is\nsignificantly different from the means for psychology and nutrition\nmajors, but the means for psychology and nutrition majors are not\nsignificantly different from each other. For this reason, statistically\nsignificant one-way ANOVA results are typically followed up with a\nseries of [post hoc comparisons] of selected pairs of group means to\ndetermine which are different from which others.\n\nOne approach to post hoc comparisons would be to conduct a series of\nindependent-samples *t* tests comparing each group mean to each of the\nother group means. But there is a problem with this approach. In\ngeneral, if we conduct a *t* test when the null hypothesis is true, we\nhave a 5% chance of mistakenly rejecting the null hypothesis (see\nsection \"Additional Considerations\" below for more on such Type I\nerrors). If we conduct several *t* tests when the null hypothesis is\ntrue, the chance of mistakenly rejecting *at least one* null hypothesis\nincreases with each test we conduct. Thus researchers do not usually\nmake post hoc comparisons using standard *t* tests because there is too\ngreat a chance that they will mistakenly reject at least one null\nhypothesis. Instead, they use one of several modified *t* test\nprocedures---among them the Bonferonni procedure, Fisher's least\nsignificant difference (LSD) test, and Tukey's honestly significant\ndifference (HSD) test. The details of these approaches are beyond the\nscope of this book, but it is important to understand their purpose. It\nis to keep the risk of mistakenly rejecting a true null hypothesis to an\nacceptable level (close to 5%).\n\n##### Repeated-Measures ANOVA {.unnumbered}\n\nRecall that the one-way ANOVA is appropriate for between-subjects\ndesigns in which the means being compared come from separate groups of\nparticipants. It is not appropriate for within-subjects designs in which\nthe means being compared come from the same participants tested under\ndifferent conditions or at different times. This requires a slightly\ndifferent approach, called the [repeated-measures ANOVA]. The basics of\nthe repeated-measures ANOVA are the same as for the one-way ANOVA. The\nmain difference is that measuring the dependent variable multiple times\nfor each participant allows for a more refined measure of *MS~W~*.\nImagine, for example, that the dependent variable in a study is a\nmeasure of reaction time. Some participants will be faster or slower\nthan others because of stable individual differences in their nervous\nsystems, muscles, and other factors. In a between-subjects design, these\nstable individual differences would simply add to the variability within\nthe groups and increase the value of *MS~W~*. In a within-subjects\ndesign, however, these stable individual differences can be measured and\nsubtracted from the value of *MS~W~*. This lower value of *MS~W~* means\na higher value of *F* and a more sensitive test.\n\n##### Factorial ANOVA {.unnumbered}\n\nWhen more than one independent variable is included in a factorial\ndesign, the appropriate approach is the [factorial ANOVA]. Again, the\nbasics of the factorial ANOVA are the same as for the one-way and\nrepeated-measures ANOVAs. The main difference is that it produces an *F*\nratio and *p* value for each main effect and for each interaction.\nReturning to our calorie estimation example, imagine that the health\npsychologist tests the effect of participant major (psychology vs.\nnutrition) and food type (cookie vs. hamburger) in a factorial design. A\nfactorial ANOVA would produce separate *F* ratios and *p* values for the\nmain effect of major, the main effect of food type, and the interaction\nbetween major and food. Appropriate modifications must be made depending\non whether the design is between subjects, within subjects, or mixed.\n\n### Testing Pearson's *r* {.unnumbered}\n\nFor relationships between quantitative variables, where Pearson's *r* is\nused to describe the strength of those relationships, the appropriate\nnull hypothesis test is a test of Pearson's *r.* The basic logic is\nexactly the same as for other null hypothesis tests. In this case, the\nnull hypothesis is that there is no relationship in the population. We\ncan use the Greek lowercase rho (ρ) to represent the relevant parameter:\nρ = 0. The alternative hypothesis is that there is a relationship in the\npopulation: ρ ≠ 0. As with the *t* test, this test can be two-tailed if\nthe researcher has no expectation about the direction of the\nrelationship or one-tailed if the researcher expects the relationship to\ngo in a particular direction.\n\nIt is possible to use Pearson's *r* for the sample to compute a *t*\nscore with *N* − 2 degrees of freedom and then to proceed as for a *t*\ntest. However, because of the way it is computed, Pearson's *r* can also\nbe treated as its own test statistic. The online statistical tools and\nstatistical software such as Excel and SPSS generally compute Pearson's\n*r* and provide the *p* value associated with that value of Pearson's\n*r.* As always, if the *p* value is less than .05, we reject the null\nhypothesis and conclude that there is a relationship between the\nvariables in the population. If the *p* value is greater than .05, we\nretain the null hypothesis and conclude that there is not enough\nevidence to say there is a relationship in the population. If we compute\nPearson's *r* by hand, we can use a table like Table \\@ref(tab:rcrit),\nwhich shows the critical values of *r* for various samples sizes when α\nis .05. A sample value of Pearson's *r* that is more extreme than the\ncritical value is statistically significant.\n\n| *N* | One-tailed | Two-tailed |\n|:----|:-----------|:-----------|\n| 5   | .805       | .878       |\n| 10  | .549       | .632       |\n| 15  | .441       | .514       |\n| 20  | .378       | .444       |\n| 25  | .337       | .396       |\n| 30  | .306       | .361       |\n| 35  | .283       | .334       |\n| 40  | .264       | .312       |\n| 45  | .248       | .294       |\n| 50  | .235       | .279       |\n| 55  | .224       | .266       |\n| 60  | .214       | .254       |\n| 65  | .206       | .244       |\n| 70  | .198       | .235       |\n| 75  | .191       | .227       |\n| 80  | .185       | .220       |\n| 85  | .180       | .213       |\n| 90  | .174       | .207       |\n| 95  | .170       | .202       |\n| 100 | .165       | .197       |\n\n: (#tab:rcrit) Table of critical values of Pearson's *r* when α = .05\n\n#### Example Test of Pearson's *r* {.unnumbered}\n\nImagine that the health psychologist is interested in the correlation\nbetween people's calorie estimates and their weight. He has no\nexpectation about the direction of the relationship, so he decides to\nconduct a two-tailed test. He computes the correlation for a sample of\n22 college students and finds that Pearson's *r* is −.21. The\nstatistical software he uses tells him that the *p* value is .348. It is\ngreater than .05, so he retains the null hypothesis and concludes that\nthere is no relationship between people's calorie estimates and their\nweight. If he were to compute Pearson's *r* by hand, he could look at\nTable \\@ref(tab:rcrit) and see that the critical value for 22 − 2 = 20\ndegrees of freedom is .444. The fact that Pearson's *r* for the sample\nis less extreme than this critical value tells him that the *p* value is\ngreater than .05 and that he should retain the null hypothesis.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   To compare two means, the most common null hypothesis test is the\n    *t* test. The one-sample *t* test is used for comparing one sample\n    mean with a hypothetical population mean of interest, the\n    dependent-samples *t* test is used to compare two means in a\n    within-subjects design, and the independent-samples *t* test is used\n    to compare two means in a between-subjects design.\n-   To compare more than two means, the most common null hypothesis test\n    is the analysis of variance (ANOVA). The one-way ANOVA is used for\n    between-subjects designs with one independent variable, the\n    repeated-measures ANOVA is used for within-subjects designs, and the\n    factorial ANOVA is used for factorial designs.\n-   A null hypothesis test of Pearson's *r* is used to compare a sample\n    value of Pearson's *r* with a hypothetical population value of 0.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: Use one of the online tools, Excel, or SPSS to reproduce\n    the one-sample *t* test, dependent-samples *t* test,\n    independent-samples *t* test, and one-way ANOVA for the four sets of\n    calorie estimation data presented in this section.\n2.  Practice: A sample of 25 college students rated their friendliness\n    on a scale of 1 (*Much Lower Than Average*) to 7 (*Much Higher Than\n    Average*). Their mean rating was 5.30 with a standard deviation of\n    1.50. Conduct a one-sample *t* test comparing their mean rating with\n    a hypothetical mean rating of 4 (*Average*). The question is whether\n    college students have a tendency to rate themselves as friendlier\n    than average.\n3.  Practice: Decide whether each of the following Pearson's *r* values\n    is statistically significant for both a one-tailed and a two-tailed\n    test. (a) The correlation between height and IQ is +.13 in a sample\n    of 35. (b) For a sample of 88 college students, the correlation\n    between how disgusted they felt and the harshness of their moral\n    judgments was +.23. (c) The correlation between the number of daily\n    hassles and positive mood is −.43 for a sample of 30 middle-aged\n    adults.\n:::\n\n## Additional Considerations\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Define Type I and Type II errors, explain why they occur, and\n    identify some steps that can be taken to minimize their likelihood.\n2.  Define statistical power, explain its role in the planning of new\n    studies, and use online tools to compute the statistical power of\n    simple research designs.\n3.  List some criticisms of conventional null hypothesis testing, along\n    with some ways of dealing with these criticisms.\n:::\n\nIn this section, we consider a few other issues related to null\nhypothesis testing, including some that are useful in planning studies\nand interpreting results. We even consider some long-standing criticisms\nof null hypothesis testing, along with some steps that researchers in\npsychology have taken to address them.\n\n### Errors in Null Hypothesis Testing {.unnumbered}\n\nIn null hypothesis testing, the researcher tries to draw a reasonable\nconclusion about the population based on the sample. Unfortunately, this\nconclusion is not guaranteed to be correct. This is illustrated by\nFigure \\@ref(fig:errors). The rows of this table represent the two\npossible decisions that we can make in null hypothesis testing: to\nreject or retain the null hypothesis. The columns represent the two\npossible states of the world: The null hypothesis is false or it is\ntrue. The four cells of the table, then, represent the four distinct\noutcomes of a null hypothesis test. Two of the outcomes---rejecting the\nnull hypothesis when it is false and retaining it when it is true---are\ncorrect decisions. The other two---rejecting the null hypothesis when it\nis true and retaining it when it is false---are errors.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Two types of correct decisions and two types of errors in null hypothesis testing.](images/ch13/errors.png){fig-align='center' width=50%}\n:::\n:::\n\n\nRejecting the null hypothesis when it is true is called a [Type I\nerror]. This means that we have concluded that there is a relationship\nin the population when in fact there is not. Type I errors occur because\neven when there is no relationship in the population, sampling error\nalone will occasionally produce an extreme result. In fact, when the\nnull hypothesis is true and α is .05, we will mistakenly reject the null\nhypothesis 5% of the time. (This is why α is sometimes referred to as\nthe \"Type I error rate.\") Retaining the null hypothesis when it is false\nis called a [Type II error]. This means that we have concluded that\nthere is no relationship in the population when in fact there is. In\npractice, Type II errors occur primarily because the research design\nlacks adequate statistical power to detect the relationship (e.g., the\nsample is too small). We will have more to say about statistical power\nshortly.\n\nIn principle, it is possible to reduce the chance of a Type I error by\nsetting α to something less than .05. Setting it to .01, for example,\nwould mean that if the null hypothesis is true, then there is only a 1%\nchance of mistakenly rejecting it. But making it harder to reject true\nnull hypotheses also makes it harder to reject false ones and therefore\nincreases the chance of a Type II error. Similarly, it is possible to\nreduce the chance of a Type II error by setting α to something greater\nthan .05 (e.g., .10). But making it easier to reject false null\nhypotheses also makes it easier to reject true ones and therefore\nincreases the chance of a Type I error. This provides some insight into\nwhy the convention is to set α to .05. There is some agreement among\nresearchers that level of α keeps the rates of both Type I and Type II\nerrors at acceptable levels.\n\nThe possibility of committing Type I and Type II errors has several\nimportant implications for interpreting the results of our own and\nothers' research. One is that we should be cautious about interpreting\nthe results of any individual study because there is a chance that it\nreflects a Type I or Type II error. This is why researchers consider it\nimportant to replicate their studies. Each time researchers replicate a\nstudy and find a similar result, they rightly become more confident that\nthe result represents a real phenomenon and not just a Type I or Type II\nerror.\n\nAnother issue related to Type I errors is the so-called [file drawer\nproblem] [@rosenthal1979file]. The idea is that when researchers obtain\nstatistically significant results, they tend to submit them for\npublication, and journal editors and reviewers tend to accept them. But\nwhen researchers obtain nonsignificant results, they tend not to submit\nthem for publication, or if they do submit them, journal editors and\nreviewers tend not to accept them. Researchers end up putting these\nnonsignificant results away in a file drawer (or nowadays, in a folder\non their hard drive). One effect of this is that the published\nliterature probably contains a higher proportion of Type I errors than\nwe might expect on the basis of statistical considerations alone. Even\nwhen there is a relationship between two variables in the population,\nthe published research literature is likely to overstate the strength of\nthat relationship. Imagine, for example, that the relationship between\ntwo variables in the population is positive but weak (e.g., ρ = +.10).\nIf several researchers conduct studies on this relationship, sampling\nerror is likely to produce results ranging from weak negative\nrelationships (e.g., *r* = −.10) to moderately strong positive ones\n(e.g., *r* = +.40). But because of the file drawer problem, it is likely\nthat only those studies producing moderate to strong positive\nrelationships are published. The result is that the effect reported in\nthe published literature tends to be stronger than it really is in the\npopulation.\n\nThe file drawer problem is a difficult one because it is a product of\nthe way scientific research has traditionally been conducted and\npublished. One solution might be for journal editors and reviewers to\nevaluate research submitted for publication without knowing the results\nof that research. The idea is that if the research question is judged to\nbe interesting and the method judged to be sound, then a nonsignificant\nresult should be just as important and worthy of publication as a\nsignificant one. Short of such a radical change in how research is\nevaluated for publication, researchers can still take pains to keep\ntheir nonsignificant results and share them as widely as possible (e.g.,\nat professional conferences). Many scientific disciplines now have\njournals devoted to publishing nonsignificant results. In psychology,\nfor example, there is the *Journal of Articles in Support of the Null\nHypothesis* (http://www.jasnh.com).\n\n### Statistical Power {.unnumbered}\n\nThe [statistical power] of a research design is the probability of\nrejecting the null hypothesis given the sample size and expected\nrelationship strength. For example, the statistical power of a study\nwith 50 participants and an expected Pearson's *r* of +.30 in the\npopulation is .59. That is, there is a 59% chance of rejecting the null\nhypothesis if indeed the population correlation is +.30. Statistical\npower is the complement of the probability of committing a Type II\nerror. So in this example, the probability of committing a Type II error\nwould be 1 − .59 = .41. Clearly, researchers should be interested in the\npower of their research designs if they want to avoid making Type II\nerrors. In particular, they should make sure their research design has\nadequate power before collecting data. A common guideline is that a\npower of .80 is adequate. This means that there is an 80% chance of\nrejecting the null hypothesis for the expected relationship strength.\n\nThe topic of how to compute power for various research designs and null\nhypothesis tests is beyond the scope of this book. However, there are\nonline tools that allow you to do this by entering your sample size,\nexpected relationship strength, and α level for various hypothesis tests\n(see box \"Computing Power\"). In addition, Table \\@ref(tab:power) shows\nthe sample size needed to achieve a power of .80 for weak, medium, and\nstrong relationships for a two-tailed independent-samples *t* test and\nfor a two-tailed test of Pearson's *r.* Notice that this table amplifies\nthe point made earlier about relationship strength, sample size, and\nstatistical significance. In particular, weak relationships require very\nlarge samples to provide adequate statistical power.\n\n| Relationship Strength         | Independent-Samples *t* Test | Test of Pearson's r |\n|:------------------------------|-----------------------------:|--------------------:|\n| Strong (*d* = .80, *r* = .50) |                           52 |                  28 |\n| Medium (*d* = .50, *r* = .30) |                          128 |                  84 |\n| Weak (*d* = .20, *r* = .10)   |                          788 |                 782 |\n\n: (#tab:power) Sample sizes needed to achieve statistical power of .80\nfor different expected relationship strengths for an independent-samples\n*t* test and a test of pearson's *r*.\n\nWhat should you do if you discover that your research design does not\nhave adequate power? Imagine, for example, that you are conducting a\nbetween-subjects experiment with 20 participants in each of two\nconditions and that you expect a medium difference (*d* = .50) in the\npopulation. The statistical power of this design is only .34. That is,\neven if there is a medium difference in the population, there is only\nabout a one in three chance of rejecting the null hypothesis and about a\ntwo in three chance of committing a Type II error. Given the time and\neffort involved in conducting the study, this probably seems like an\nunacceptably low chance of rejecting the null hypothesis and an\nunacceptably high chance of committing a Type II error.\n\nGiven that statistical power depends primarily on relationship strength\nand sample size, there are essentially two steps you can take to\nincrease statistical power: increase the strength of the relationship or\nincrease the sample size. Increasing the strength of the relationship\ncan sometimes be accomplished by using a stronger manipulation or by\nmore carefully controlling extraneous variables to reduce the amount of\nnoise in the data (e.g., by using a within-subjects design rather than a\nbetween-subjects design). The usual strategy, however, is to increase\nthe sample size. For any expected relationship strength, there will\nalways be some sample large enough to achieve adequate power.\n\n::: fyi\n##### Computing Power {.unnumbered}\n\n[G\\*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower)\nis free downloadable software that allows you to compute statistical\npower for various research designs and null hypothesis tests by entering\ninformation about the expected relationship strength, the sample size,\nand the α level.It also allows you to compute the sample size necessary\nto achieve your desired level of power (e.g., .80).\n:::\n\n### Problems With Null Hypothesis Testing, and Some Solutions {.unnumbered}\n\nAgain, null hypothesis testing is the most common approach to\ninferential statistics in psychology. It is not without its critics,\nhowever. In fact, in recent years the criticisms have become so\nprominent that the American Psychological Association convened a task\nforce to make recommendations about how to deal with them (Wilkinson &\nTask Force on Statistical Inference, 1999).Wilkinson, L., & Task Force\non Statistical Inference. (1999). Statistical methods in psychology\njournals: Guidelines and explanations. American Psychologist, 54,\n594--604. In this section, we consider some of the criticisms and some\nof the recommendations.\n\n#### Criticisms of Null Hypothesis Testing {.unnumbered}\n\nSome criticisms of null hypothesis testing focus on researchers'\nmisunderstanding of it. We have already seen, for example, that the *p*\nvalue is widely misinterpreted as the probability that the null\nhypothesis is true. (Recall that it is really the probability of the\nsample result if the null hypothesis were true.) A closely related\nmisinterpretation is that 1 − *p* is the probability of replicating a\nstatistically significant result. In one study, 60% of a sample of\nprofessional researchers thought that a *p* value of .01---for an\nindependent-samples *t* test with 20 participants in each sample---meant\nthere was a 99% chance of replicating the statistically significant\nresult [@oaks1986statistical]. Our earlier discussion of power should\nmake it clear that this is far too optimistic. As Table \\@ref(tab:power)\nshows, even if there were a large difference between means in the\npopulation, it would require 26 participants per sample to achieve a\npower of .80. And the program G\\*Power shows that it would require 59\nparticipants per sample to achieve a power of .99.\n\nAnother set of criticisms focuses on the logic of null hypothesis\ntesting. To many, the strict convention of rejecting the null hypothesis\nwhen *p* is less than .05 and retaining it when *p* is greater than .05\nmakes little sense. This criticism does not have to do with the specific\nvalue of .05 but with the idea that there should be any rigid dividing\nline between results that are considered significant and results that\nare not. Imagine two studies on the same statistical relationship with\nsimilar sample sizes. One has a *p* value of .04 and the other a *p*\nvalue of .06. Although the two studies have produced essentially the\nsame result, the former is likely to be considered interesting and\nworthy of publication and the latter simply not significant. This\nconvention is likely to prevent good research from being published and\nto contribute to the file drawer problem.\n\nYet another set of criticisms focus on the idea that null hypothesis\ntesting---even when understood and carried out correctly---is simply not\nvery informative. Recall that the null hypothesis is that there is no\nrelationship between variables in the population (e.g., Cohen's *d* or\nPearson's *r* is precisely 0). So to reject the null hypothesis is\nsimply to say that there is *some* nonzero relationship in the\npopulation. But this is not really saying very much. Imagine if\nchemistry could tell us only that there is *some* relationship between\nthe temperature of a gas and its volume---as opposed to providing a\nprecise equation to describe that relationship. Some critics even argue\nthat the relationship between two variables in the population is never\nprecisely 0 if it is carried out to enough decimal places. In other\nwords, the null hypothesis is never literally true. So rejecting it does\nnot tell us anything we did not already know!\n\nTo be fair, many researchers have come to the defense of null hypothesis\ntesting. One of them, Robert Abelson, has argued that when it is\ncorrectly understood and carried out, null hypothesis testing does serve\nan important purpose [@abelson2012statistics]. Especially when dealing\nwith new phenomena, it gives researchers a principled way to convince\nothers that their results should not be dismissed as mere chance\noccurrences.\n\n#### What to Do? {.unnumbered}\n\nEven those who defend null hypothesis testing recognize many of the\nproblems with it. But what should be done? Some suggestions now appear\nin the *Publication Manual*. One is that each null hypothesis test\nshould be accompanied by an effect size measure such as Cohen's *d* or\nPearson's *r.* By doing so, the researcher provides an estimate of how\nstrong the relationship in the population is---not just whether there is\none or not. (Remember that the *p* value cannot substitute as a measure\nof relationship strength because it also depends on the sample size.\nEven a very weak result can be statistically significant if the sample\nis large enough.)\n\nAnother suggestion is to use confidence intervals rather than null\nhypothesis tests. A [confidence interval] around a statistic is a range\nof values that is computed in such a way that some percentage of the\ntime (usually 95%) the population parameter will lie within that range.\nFor example, a sample of 20 college students might have a mean calorie\nestimate for a chocolate chip cookie of 200 with a 95% confidence\ninterval of 160 to 240. In other words, there is a very good chance that\nthe mean calorie estimate for the population of college students lies\nbetween 160 and 240. Advocates of confidence intervals argue that they\nare much easier to interpret than null hypothesis tests. Another\nadvantage of confidence intervals is that they provide the information\nnecessary to do null hypothesis tests should anyone want to. In this\nexample, the sample mean of 200 is significantly different at the .05\nlevel from any hypothetical population mean that lies outside the\nconfidence interval. So the confidence interval of 160 to 240 tells us\nthat the sample mean is statistically significantly different from a\nhypothetical population mean of 250.\n\nFinally, there are more radical solutions to the problems of null\nhypothesis testing that involve using very different approaches to\ninferential statistics. [Bayesian statistics], for example, is an\napproach in which the researcher specifies the probability that the null\nhypothesis and any important alternative hypotheses are true before\nconducting the study, conducts the study, and then updates the\nprobabilities based on the data. It is too early to say whether this\napproach will become common in psychological research. For now, null\nhypothesis testing---supported by effect size measures and confidence\nintervals---remains the dominant approach.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   The decision to reject or retain the null hypothesis is not\n    guaranteed to be correct. A Type I error occurs when one rejects the\n    null hypothesis when it is true. A Type II error occurs when one\n    fails to reject the null hypothesis when it is false.\n-   The statistical power of a research design is the probability of\n    rejecting the null hypothesis given the expected relationship\n    strength in the population and the sample size. Researchers should\n    make sure that their studies have adequate statistical power before\n    conducting them.\n-   Null hypothesis testing has been criticized on the grounds that\n    researchers misunderstand it, that it is illogical, and that it is\n    uninformative. Others argue that it serves an important\n    purpose---especially when used with effect size measures, confidence\n    intervals, and other techniques. It remains the dominant approach to\n    inferential statistics in psychology.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Discussion: A researcher compares the effectiveness of two forms of\n    psychotherapy for social phobia using an independent-samples *t*\n    test.\n    a.  Explain what it would mean for the researcher to commit a Type I\n        error.\n    b.  Explain what it would mean for the researcher to commit a Type\n        II error.\n2.  Discussion: Imagine that you conduct a *t* test and the *p* value is\n    .02. How could you explain what this *p* value means to someone who\n    is not already familiar with null hypothesis testing? Be sure to\n    avoid the common misinterpretations of the *p* value.\n:::\n\n## Glossary\n\n##### α (alpha) {#α-alpha .unnumbered}\n\nIn null hypothesis testing, the criterion for deciding that a *p* value\nis low enough to reject the null hypothesis. In psychological research,\nit is almost always set to .05.\n\n##### alternative hypothesis {.unnumbered}\n\nThe idea that there is a statistical relationship between two variables\nin the population and that any relationship in a sample reflects that\nreal relationship. Often abbreviated *H*~1~.\n\n##### analysis of variance (ANOVA) {#analysis-of-variance-anova .unnumbered}\n\nA null hypothesis test used to compare means for more than two groups or\nconditions.\n\n##### Bayesian statistics {.unnumbered}\n\nAn alternative approach to inferential statistics in which the\nresearcher specifies the probability that the null hypothesis and\nimportant alternative hypotheses are true before conducting a study,\nconducts the study, and then computes revised probabilities based on the\ndata.\n\n##### confidence interval {.unnumbered}\n\nA range of values computed in such a way that some specified percentage\nof the time (usually 95%) the population parameter of interest will lie\nwithin that range.\n\n##### critical values {.unnumbered}\n\nIn null hypothesis testing, the value or values of a test statistic that\ncorrespond to a *p* value of .05 and therefore serve as a cutoff for\ndeciding to reject the null hypothesis.\n\n##### dependent-samples *t* test {.unnumbered}\n\nA null hypothesis test used to compare two means for one sample measured\nat two different times or under two different conditions---as in a\npretest-posttest or within-subjects design.\n\n##### difference score {.unnumbered}\n\nThe difference between an individual's score at one time or under one\ncondition and that individual's score at a second time or under a second\ncondition. The dependent-samples *t* test is in essence a one-sample *t*\ntest on a set of difference scores.\n\n##### factorial ANOVA {.unnumbered}\n\nA null hypothesis test used to test both main effects and interactions\nin a factorial design.\n\n##### file drawer problem {.unnumbered}\n\nThe fact that statistically significant results are more likely to be\nsubmitted and accepted for publication than nonsignificant results.\n\n##### independent-samples *t* test {.unnumbered}\n\nA null hypothesis test used to compare means for two separate\nsamples---as in a between-subjects design.\n\n##### mean squares between groups (*MS~B~*) {#mean-squares-between-groups-msb .unnumbered}\n\nIn an analysis of variance, an estimate for the population variance\nbased only on differences among the group or condition means.\n\n##### mean squares within groups (*MS~W~*) {#mean-squares-within-groups-msw .unnumbered}\n\nIn an analysis of variance, an estimate of the population variance based\non the variability within each group or condition.\n\n##### null hypothesis {.unnumbered}\n\nThe idea that there is no statistical relationship between two variables\nin the population and that any relationship in a sample is due to\nchance. Often abbreviated *H*~0~.\n\n##### null hypothesis testing {.unnumbered}\n\nA formal approach to deciding whether a sample relationship is due to\nchance (the null hypothesis) or reflects a real relationship in the\npopulation (the alternative hypothesis).\n\n##### one-sample *t* test {.unnumbered}\n\nA null hypothesis test used to compare one sample mean with a\nhypothetical population mean that provides an interesting standard of\ncomparison.\n\n##### one-tailed test {.unnumbered}\n\nA null hypothesis test (e.g., a *t* test or test of Pearson's *r*) in\nwhich the null hypothesis is rejected only if the sample result is\nextreme in one direction specified before the data are collected. Used\nwhen the researcher has a strong expectation about the direction of the\nrelationship.\n\n##### one-way ANOVA {.unnumbered}\n\nA null hypothesis test used to compare more than two means in a\nbetween-subjects design with one independent variable.\n\n##### *p* value {.unnumbered}\n\nIn null hypothesis testing, the probability of a sample result at least\nas extreme as the one obtained if the null hypothesis were true.\n\n##### parameter {#parameter .unnumbered}\n\nA numerical summary (e.g., mean, standard deviation) of a population. A\nnumerical summary of a sample is called a \"statistic.\"\n\n##### post hoc comparison {.unnumbered}\n\nStatistical comparison of selected pairs of group or condition means\nfollowing a statistically significant ANOVA result. Usually done using\none of several modified *t* test procedures.\n\n##### practical significance {.unnumbered}\n\nThe importance of a research result in some real-world context. Research\nresults can be statistically significant without having any practical\nsignificance. In clinical practice, practical significance is called\n\"clinical significance.\"\n\n##### reject the null hypothesis {.unnumbered}\n\nIn null hypothesis testing, the conclusion that the null hypothesis is\nfalse. The sample relationship reflects a real relationship in the\npopulation.\n\n##### repeated-measures ANOVA {.unnumbered}\n\nA null hypothesis test used to compare means for one sample at more than\ntwo times or under more than two conditions in a within-subjects design.\n\n##### retain the null hypothesis {.unnumbered}\n\nIn null hypothesis testing, the tentative conclusion that the null\nhypothesis is true. The sample relationship is due to chance. Often\nexpressed as \"fail to reject the null hypothesis\" (although never as\n\"accept the null hypothesis\").\n\n##### sampling error {.unnumbered}\n\nRandom variation in a statistic from sample to sample.\n\n##### statistical power {.unnumbered}\n\nThe probability of rejecting the null hypothesis for a given sample size\nand expected relationship strength.\n\n##### statistically significant {.unnumbered}\n\nUsed to describe a result for which the null hypothesis has been\nrejected.\n\n##### *t* test {.unnumbered}\n\nA family of null hypothesis tests used to compare two means.\n\n##### test statistic {.unnumbered}\n\nIn null hypothesis testing, a statistic such as *t* or *F* that is\ncomputed only to help find the *p* value for the sample result.\n\n##### two-tailed test {.unnumbered}\n\nA null hypothesis test (e.g., a *t* test or test of Pearson's *r*) in\nwhich the null hypothesis is rejected if the sample result is extreme in\neither direction. Used when the researcher does not have a strong\nexpectation about the direction of the relationship.\n\n##### type I error {.unnumbered}\n\nIn null hypothesis testing, rejecting the null hypothesis when it is\ntrue.\n\n##### type II error {.unnumbered}\n\nIn null hypothesis testing, failing to reject the null hypothesis when\nit is false.\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "14-inferentials_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}