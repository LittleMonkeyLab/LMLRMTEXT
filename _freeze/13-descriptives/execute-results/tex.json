{
  "hash": "a004e0ae32c91570063b3dc09d7e5dda",
  "result": {
    "markdown": "# Analyzing Data\n\n# Descriptive Statistics\n\nAt this point, we need to consider the basics of data analysis in\npsychological research in more detail. In this chapter, we focus on\ndescriptive statistics---a set of techniques for summarizing and\ndisplaying the data from your sample. We look first at some of the most\ncommon techniques for describing single variables, followed by some of\nthe most common techniques for describing statistical relationships\nbetween variables. We then look at how to present descriptive statistics\nin writing and also in the form of tables and graphs that would be\nappropriate for an American Psychological Association (APA)-style\nresearch report. We end with some practical advice for organizing and\ncarrying out your analyses.\n\n## Describing Single Variables\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Use frequency tables and histograms to display and interpret the\n    distribution of a variable.\n2.  Compute and interpret the mean, median, and mode of a distribution\n    and identify situations in which the mean, median, or mode is the\n    most appropriate measure of central tendency.\n3.  Compute and interpret the range and standard deviation of a\n    distribution.\n4.  Compute and interpret percentile ranks and *z* scores.\n:::\n\n[Descriptive statistics] refers to a set of techniques for summarizing\nand displaying data. Let us assume here that the data are quantitative\nand consist of scores on one or more variables for each of several study\nparticipants. Although in most cases the primary research question will\nbe about one or more statistical relationships between variables, it is\nalso important to describe each variable individually. For this reason,\nwe begin by looking at some of the most common techniques for describing\nsingle variables.\n\n### The Distribution of a Variable {.unnumbered}\n\nEvery variable has a [distribution], which is the way the scores are\ndistributed across the levels of that variable. For example, in a sample\nof 100 college students, the distribution of the variable \"number of\nsiblings\" might be such that 10 of them have no siblings, 30 have one\nsibling, 40 have two siblings, and so on. In the same sample, the\ndistribution of the variable \"sex\" might be such that 44 have a score of\n\"male\" and 56 have a score of \"female.\"\n\n#### Frequency Tables {.unnumbered}\n\nOne way to display the distribution of a variable is in a [frequency\ntable]. Table \\@ref(tab:frequency), for example, is a frequency table\nshowing a hypothetical distribution of scores on the Rosenberg\nSelf-Esteem Scale for a sample of 40 college students. The first column\nlists the values of the variable---the possible scores on the Rosenberg\nscale---and the second column lists the frequency of each score. This\ntable shows that there were three students who had self-esteem scores of\n24, five who had self-esteem scores of 23, and so on. From a frequency\ntable like this, one can quickly see several important aspects of a\ndistribution, including the range of scores (from 15 to 24), the most\nand least common scores (22 and 17, respectively), and any extreme\nscores that stand out from the rest.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Frequency table showing a hypothetical distribution of scores on the Rosenberg Self-Esteem Scale.\n\n| Self-esteem | Frequency |\n|:-----------:|:---------:|\n|     24      |     3     |\n|     23      |     5     |\n|     22      |    10     |\n|     21      |     8     |\n|     20      |     5     |\n|     19      |     3     |\n|     18      |     3     |\n|     17      |     0     |\n|     16      |     2     |\n|     15      |     1     |\n:::\n:::\n\n\n\nThere are a few other points worth noting about frequency tables. First,\nthe levels listed in the first column usually go from the highest at the\ntop to the lowest at the bottom, and they usually do not extend beyond\nthe highest and lowest scores in the data. For example, although scores\non the Rosenberg scale can vary from a high of 30 to a low of 0, Table\n\\@ref(tab:frequency) only includes levels from 24 to 15 because that\nrange includes all the scores in this particular data set. Second, when\nthere are many different scores across a wide range of values, it is\noften better to create a grouped frequency table, in which the first\ncolumn lists ranges of values and the second column lists the frequency\nof scores in each range. Table \\@ref(tab:frequencybin), for example, is\na grouped frequency table showing a hypothetical distribution of simple\nreaction times for a sample of 20 participants. In a grouped frequency\ntable, the ranges must all be of equal width, and there are usually\nbetween five and 15 of them. Finally, frequency tables can also be used\nfor categorical variables, in which case the levels are category labels.\nThe order of the category labels is somewhat arbitrary, but they are\noften listed from the most frequent at the top to the least frequent at\nthe bottom.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: A grouped frequency table showing a hypothetical distribution of reaction times.\n\n| Reaction time (ms) | Frequency |\n|:------------------:|:---------:|\n|      241–260       |     1     |\n|      221–240       |     2     |\n|      201–220       |     2     |\n|      181–200       |     9     |\n|      161–180       |     4     |\n|      141–160       |     2     |\n:::\n:::\n\n\n\n#### Histograms {.unnumbered}\n\nA histogram is a graphical display of a distribution. It presents the\nsame information as a frequency table but in a way that is even quicker\nand easier to grasp. The histogram in Figure \\@ref(fig:hist) presents\nthe distribution of self-esteem scores in Table \\@ref(tab:frequency).\nThe x-axis of the histogram represents the variable and the y-axis\nrepresents frequency. Above each level of the variable on the x-axis is\na vertical bar that represents the number of individuals with that\nscore. When the variable is quantitative, as in this example, there is\nusually no gap between the bars. When the variable is categorical,\nhowever, there is usually a small gap between them. (The gap at 17 in\nthis histogram reflects the fact that there were no scores of 17 in this\ndata set.)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Histogram showing the distribution of self-esteem scores presented in the frequency table above.](13-descriptives_files/figure-pdf/hist-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\n#### Distribution Shapes {.unnumbered}\n\nWhen the distribution of a quantitative variable is displayed in a\nhistogram, it has a shape. The shape of the distribution of self-esteem\nscores in Figure \\@ref(fig:hist) is typical. There is a peak somewhere\nnear the middle of the distribution and \"tails\" that taper in either\ndirection from the peak. The distribution of Figure \\@ref(fig:hist) is\nunimodal, meaning it has one distinct peak, but distributions can also\nbe bimodal, meaning they have two distinct peaks. Figure\n\\@ref(fig:bimodal), for example, shows a hypothetical bimodal\ndistribution of scores on the Beck Depression Inventory. Distributions\ncan also have more than two distinct peaks, but these are relatively\nrare in psychological research.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Histogram showing a hypothetical bimodal distribution of scores on the Beck Depression Inventory.](13-descriptives_files/figure-pdf/bimodal-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\nAnother characteristic of the shape of a distribution is whether it is\nsymmetrical or skewed. The distribution in the center of Figure\n\\@ref(fig:skew) is symmetrical. Its left and right halves are mirror\nimages of each other. The distribution on the left is negatively skewed,\nwith its peak shifted toward the upper end of its range and a relatively\nlong negative tail. The distribution on the right is positively skewed,\nwith its peak toward the lower end of its range and a relatively long\npositive tail.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Histograms showing negatively skewed, symmetrical, and positively skewed distributions.](13-descriptives_files/figure-pdf/skew-1.pdf){fig-align='center' width=33%}\n:::\n\n::: {.cell-output-display}\n![Histograms showing negatively skewed, symmetrical, and positively skewed distributions.](13-descriptives_files/figure-pdf/skew-2.pdf){fig-align='center' width=33%}\n:::\n\n::: {.cell-output-display}\n![Histograms showing negatively skewed, symmetrical, and positively skewed distributions.](13-descriptives_files/figure-pdf/skew-3.pdf){fig-align='center' width=33%}\n:::\n:::\n\n\n\nAn [outlier] is an extreme score that is much higher or lower than the\nrest of the scores in the distribution. Sometimes outliers represent\ntruly extreme scores on the variable of interest. For example, on the\nBeck Depression Inventory, a single clinically depressed person might be\nan outlier in a sample of otherwise happy and high-functioning peers.\nHowever, outliers can also represent errors or misunderstandings on the\npart of the researcher or participant, equipment malfunctions, or\nsimilar problems. We will say more about how to interpret outliers and\nwhat to do about them later in this chapter.\n\n### Measures of Central Tendency and Variability {.unnumbered}\n\nIt is also useful to be able to describe the characteristics of a\ndistribution more precisely. Here we look at how to do this in terms of\ntwo important characteristics: their central tendency and their\nvariability.\n\n#### Central Tendency {.unnumbered}\n\nThe [central tendency] of a distribution is its middle---the point\naround which the scores in the distribution tend to cluster. (Another\nterm for central tendency is average.) Looking back at Figure\n\\@ref(fig:hist), for example, we can see that the self-esteem scores\ntend to cluster around the values of 20 to 22. Here we will consider the\nthree most common measures of central tendency: the mean, the median,\nand the mode.\n\nThe mean of a distribution (symbolized $M$) is the sum of the scores\ndivided by the number of scores. As a formula, it looks like this:\n\n$M = \\frac{\\sum X}{N}$\n\nIn this formula, the symbol $Σ$ (the Greek letter sigma) is the\nsummation sign and means to sum across the values of the variable $X$.\n$N$ represents the number of scores. The mean is by far the most common\nmeasure of central tendency, and there are some good reasons for this.\nIt usually provides a good indication of the central tendency of a\ndistribution, and it is easily understood by most people. In addition,\nthe mean has statistical properties that make it especially useful in\ndoing inferential statistics.\n\nAn alternative to the mean is the median. The [median] is the middle\nscore in the sense that half the scores in the distribution are less\nthan it and half are greater than it. The simplest way to find the\nmedian is to organize the scores from lowest to highest and locate the\nscore in the middle. Consider, for example, the following set of seven\nscores:\n\n> 8   4   12   14   3   2   3\n\nTo find the median, simply rearrange the scores from lowest to highest\nand locate the one in the middle.\n\n> 2   3   3   **4**   8   12   14\n\nIn this case, the median is 4 because there are three scores lower than\n4 and three scores higher than 4. When there is an even number of\nscores, there are two scores in the middle of the distribution, in which\ncase the median is the value halfway between them. For example, if we\nwere to add a score of 15 to the preceding data set, there would be two\nscores (both 4 and 8) in the middle of the distribution, and the median\nwould be halfway between them (6).\n\nOne final measure of central tendency is the [mode]. The mode is the\nmost frequent score in a distribution. In the self-esteem distribution\npresented in Table \\@ref(tab:frequency) and Figure \\@ref(fig:hist), for\nexample, the mode is 22. More students had that score than any other.\nThe mode is the only measure of central tendency that can also be used\nfor categorical variables.\n\nIn a distribution that is both unimodal and symmetrical, the mean,\nmedian, and mode will be very close to each other at the peak of the\ndistribution. In a bimodal or asymmetrical distribution, the mean,\nmedian, and mode can be quite different. In a bimodal distribution, the\nmean and median will tend to be between the peaks, while the mode will\nbe at the tallest peak. In a skewed distribution, the mean will differ\nfrom the median in the direction of the skew (i.e., the direction of the\nlonger tail). For highly skewed distributions, the mean can be pulled so\nfar in the direction of the skew that it is no longer a good measure of\nthe central tendency of that distribution. Imagine, for example, a set\nof four simple reaction times of 200, 250, 280, and 250 milliseconds\n(ms). The mean is 245 ms. But the addition of one more score of 5,000\nms---perhaps because the participant was not paying attention---would\nraise the mean to 1,445 ms. Not only is this measure of central tendency\ngreater than 80% of the scores in the distribution, but it also does not\nseem to represent the behavior of anyone in the distribution very well.\nThis is why researchers often prefer the median for highly skewed\ndistributions (such as distributions of reaction times).\n\nKeep in mind, though, that you are not required to choose a single\nmeasure of central tendency in analyzing your data. Each one provides\nslightly different information, and all of them can be useful.\n\n#### Measures of Variability {.unnumbered}\n\nThe variability of a distribution is the extent to which the scores vary\naround their central tendency. Consider the two distributions in Figure\n\\@ref(fig:variability), both of which have the same central tendency.\nThe mean, median, and mode of each distribution are 10. Notice, however,\nthat the two distributions differ in terms of their variability. The top\none has relatively low variability, with all the scores relatively close\nto the center. The bottom one has relatively high variability, with the\nscores are spread across a greater range.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Histograms showing hypothetical distributions of 1,000 observations with the same mean, median, and mode (10) but with low variability (top) and high variability (bottom).](13-descriptives_files/figure-pdf/variability-1.pdf){fig-align='center' width=60%}\n:::\n\n::: {.cell-output-display}\n![Histograms showing hypothetical distributions of 1,000 observations with the same mean, median, and mode (10) but with low variability (top) and high variability (bottom).](13-descriptives_files/figure-pdf/variability-2.pdf){fig-align='center' width=60%}\n:::\n:::\n\n\n\nOne simple measure of variability is the [range], which is simply the\ndifference between the highest and lowest scores in the distribution.\nThe range of the self-esteem scores in Table \\@ref(tab:frequency), for\nexample, is the difference between the highest score (24) and the lowest\nscore (15). That is, the range is 24 − 15 = 9. Although the range is\neasy to compute and understand, it can be misleading when there are\noutliers. Imagine, for example, an exam on which all the students scored\nbetween 90 and 100. It has a range of 10. But if there was a single\nstudent who scored 20, the range would increase to 80---giving the\nimpression that the scores were quite variable when in fact only one\nstudent differed substantially from the rest.\n\nBy far the most common measure of variability is the standard deviation.\nThe [standard deviation] of a distribution is, roughly speaking, the\naverage distance between the scores and the mean. For example, the\nstandard deviations of the distributions in Figure\n\\@ref(fig:variability) are 1.3 for the top distribution and 3.0 for the\nbottom one. That is, while the scores in the top distribution differ\nfrom the mean by about 1.3 units on average, the scores in the bottom\ndistribution differ from the mean by about 3.0 units on average.\n\nComputing the standard deviation involves a slight complication.\nSpecifically, it involves finding the difference between each score and\nthe mean, squaring each difference, finding the mean of these squared\ndifferences, and finally finding the square root of that mean. The\nformula looks like this:\n\n$SD=\\sqrt{\\frac{\\sum(X-M)^2}{N}}$\n\nThe computations for the standard deviation are illustrated for a small\nset of data in Table \\@ref(tab:sd). The first column is a set of eight\nscores that has a mean of 5. The second column is the difference between\neach score and the mean. The third column is the square of each of these\ndifferences. Notice that although the differences can be negative, the\nsquared differences are always positive---meaning that the standard\ndeviation is always positive. At the bottom of the third column is the\nmean of the squared differences, which is also called the [variance]\n(symbolized $SD^2$). Although the variance is itself a measure of\nvariability, it generally plays a larger role in inferential statistics\nthan in descriptive statistics. Finally, below the variance is the\nsquare root of the variance, which is the standard deviation.\n\n|     $X$     |    $X-M$    |        $X-M^2$        |\n|:-----------:|:-----------:|:---------------------:|\n|      3      |     -2      |           4           |\n|      5      |      0      |           0           |\n|      4      |     -1      |           1           |\n|      2      |     -3      |           9           |\n|      7      |      2      |           4           |\n|      6      |      1      |           1           |\n|      5      |      0      |           0           |\n|      8      |      3      |           9           |\n|   $M$ = 5   |             |    $SD^2=28/8=3.5$    |\n|             |             | $SD=\\sqrt{3.50}=1.87$ |\n\n: (#tab:sd) Computations for the standard deviation.\n\n::: fyi\n##### *N* or *N*-1? {.unnumbered}\n\nIf you have already taken a statistics course, you may have learned to\ndivide the sum of the squared differences by *N* − 1 rather than by *N*\nwhen you compute the variance and standard deviation. Why is this?\n\nBy definition, the standard deviation is the square root of the mean of\nthe squared differences. This implies dividing the sum of squared\ndifferences by *N*, as in the formula just presented. Computing the\nstandard deviation this way is appropriate when your goal is simply to\ndescribe the variability in a sample. And learning it this way\nemphasizes that the variance is in fact the *mean* of the squared\ndifferences---and the standard deviation is the square root of this\n*mean.*\n\nHowever, most calculators and software packages divide the sum of\nsquared differences by *N* − 1. This is because the standard deviation\nof a sample tends to be a bit lower than the standard deviation of the\npopulation the sample was selected from. Dividing the sum of squares by\n*N* − 1 corrects for this tendency and results in a better estimate of\nthe population standard deviation. Because researchers generally think\nof their data as representing a sample selected from a larger\npopulation---and because they are generally interested in drawing\nconclusions about the population---it makes sense to routinely apply\nthis correction.\n:::\n\n#### Percentile Ranks and *z* Scores {.unnumbered}\n\nIn many situations, it is useful to have a way to describe the location\nof an individual score within its distribution. One approach is the\n[percentile rank]. The percentile rank of a score is the percentage of\nscores in the distribution that are lower than that score. Consider, for\nexample, the distribution in Table \\@ref(tab:frequency). For any score\nin the distribution, we can find its percentile rank by counting the\nnumber of scores in the distribution that are lower than that score and\nconverting that number to a percentage of the total number of scores.\nNotice, for example, that five of the students represented by the data\nin Table \\@ref(tab:frequency) had self-esteem scores of 23. In this\ndistribution, 32 of the 40 scores (80%) are lower than 23. Thus each of\nthese students has a percentile rank of 80. (It can also be said that\nthey scored \"at the 80th percentile.\") Percentile ranks are often used\nto report the results of standardized tests of ability or achievement.\nIf your percentile rank on a test of verbal ability were 40, for\nexample, this would mean that you scored higher than 40% of the people\nwho took the test.\n\nAnother approach is the *z* score. The [*z* score] for a particular\nindividual is the difference between that individual's score and the\nmean of the distribution, divided by the standard deviation of the\ndistribution:\n\n$z=\\frac{(X-M)}{SD}$\n\nA *z* score indicates how far above or below the mean a raw score is,\nbut it expresses this in terms of the standard deviation. For example,\nin a distribution of intelligence quotient (IQ) scores with a mean of\n100 and a standard deviation of 15, an IQ score of 110 would have a *z*\nscore of (110 − 100) / 15 = +0.67. In other words, a score of 110 is\n0.67 standard deviations (approximately two thirds of a standard\ndeviation) above the mean. Similarly, a raw score of 85 would have a *z*\nscore of (85 − 100) / 15 = −1.00. In other words, a score of 85 is one\nstandard deviation below the mean.\n\nThere are several reasons that *z* scores are important. Again, they\nprovide a way of describing where an individual's score is located\nwithin a distribution and are sometimes used to report the results of\nstandardized tests. They also provide one way of defining outliers. For\nexample, outliers are sometimes defined as scores that have *z* scores\nless than −3.00 or greater than +3.00. In other words, they are defined\nas scores that are more than three standard deviations from the mean.\nFinally, *z* scores play an important role in understanding and\ncomputing other statistics, as we will see shortly.\n\n::: fyi\n##### Online Descriptive Statistics {.unnumbered}\n\nAlthough many researchers use commercially available software such as\nSPSS and Excel to analyze their data, there are several free online\nanalysis tools that can also be extremely useful. Many allow you to\nenter or upload your data and then make one click to conduct several\ndescriptive statistical analyses. Among them are the following.\n\njamovi: https://cloud.jamovi.org/\n\nVassarStats: http://vassarstats.net/\n\nBright Stat: https://secure.brightstat.com/\n\nFor a list, see https://statpages.info/index.html.\n:::\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Every variable has a distribution---a way that the scores are\n    distributed across the levels. The distribution can be described\n    using a frequency table and histogram. It can also be described in\n    words in terms of its shape, including whether it is unimodal or\n    bimodal, and whether it is symmetrical or skewed.\n-   The central tendency, or middle, of a distribution can be described\n    precisely using three statistics---the mean, median, and mode. The\n    mean is the sum of the scores divided by the number of scores, the\n    median is the middle score, and the mode is the most common score.\n-   The variability, or spread, of a distribution can be described\n    precisely using the range and standard deviation. The range is the\n    difference between the highest and lowest scores, and the standard\n    deviation is roughly the average amount by which the scores differ\n    from the mean.\n-   The location of a score within its distribution can be described\n    using percentile ranks or *z* scores. The percentile rank of a score\n    is the percentage of scores below that score, and the *z* score is\n    the difference between the score and the mean divided by the\n    standard deviation.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: Make a frequency table and histogram for the following\n    data. Then write a short description of the shape of the\n    distribution in words.\\\n    11, 8, 9, 12, 9, 10, 12, 13, 11, 13, 12, 6, 10, 17, 13, 11, 12, 12,\n    14, 14\n2.  Practice: For the data in Exercise 1, compute the mean, median,\n    mode, standard deviation, and range.\n3.  Practice: Using the data in Exercises 1 and 2, find (a) the\n    percentile ranks for scores of 9 and 14 and (b) the *z* scores for\n    scores of 8 and 12.\n:::\n\n## Describing Statistical Relationships\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Describe differences between groups in terms of their means and\n    standard deviations, and in terms of Cohen's *d*.\n2.  Describe correlations between quantitative variables in terms of\n    Pearson's *r*.\n:::\n\nAs we have seen throughout this book, most interesting research\nquestions in psychology are about statistical relationships between\nvariables. Recall that there is a statistical relationship between two\nvariables when the average score on one differs systematically across\nthe levels of the other. In this section, we revisit the two basic forms\nof statistical relationship introduced earlier in the book---differences\nbetween groups or conditions and relationships between quantitative\nvariables---and we consider how to describe them in more detail.\n\n### Differences Between Groups or Conditions {.unnumbered}\n\nDifferences between groups or conditions are usually described in terms\nof the mean and standard deviation of each group or condition. For\nexample, Thomas Ollendick and his colleagues conducted a study in which\nthey evaluated two one-session treatments for simple phobias in children\n[@ollendick2009one]. They randomly assigned children with an intense\nfear (e.g., to dogs) to one of three conditions. In the exposure\ncondition, the children actually confronted the object of their fear\nunder the guidance of a trained therapist. In the education condition,\nthey learned about phobias and some strategies for coping with them. In\nthe waitlist control condition, they were waiting to receive a treatment\nafter the study was over. The severity of each child's phobia was then\nrated on a 1-to-8 scale by a clinician who did not know which treatment\nthe child had received. (This was one of several dependent variables.)\nThe mean fear rating in the education condition was 4.83 with a standard\ndeviation of 1.52, while the mean fear rating in the exposure condition\nwas 3.47 with a standard deviation of 1.77. The mean fear rating in the\ncontrol condition was 5.56 with a standard deviation of 1.21. In other\nwords, both treatments worked, but the exposure treatment worked better\nthan the education treatment. As we have seen, differences between group\nor condition means can be presented in a bar graph like that in Figure\n\\@ref(fig:phobia), where the heights of the bars represent the group or\ncondition means. We will look more closely at creating American\nPsychological Association (APA)-style bar graphs shortly.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Bar graph showing mean clinician phobia ratings for children in two treatment conditions.](13-descriptives_files/figure-pdf/phobia-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\nIt is also important to be able to describe the strength of a\nstatistical relationship, which is often referred to as the [effect\nsize]. The most widely used measure of effect size for differences\nbetween group or condition means is called [Cohen's *d*](#cohens-d),\nwhich is the difference between the two means divided by the standard\ndeviation:\n\n$d=\\frac{(M_1-M_2)}{SD}$\n\nIn this formula, it does not really matter which mean is $M_1$ and which\nis $M_2$. If there is a treatment group and a control group, the\ntreatment group mean is usually $M_1$ and the control group mean is\n$M_2$. Otherwise, the larger mean is usually $M_1$ and the smaller mean\n$M_2$ so that Cohen's *d* turns out to be positive. The standard\ndeviation in this formula is usually a kind of average of the two group\nstandard deviations called the pooled-within groups standard deviation.\nTo compute the pooled within-groups standard deviation, add the sum of\nthe squared differences for Group 1 to the sum of squared differences\nfor Group 2, divide this by the sum of the two sample sizes, and then\ntake the square root of that. Informally, however, the standard\ndeviation of either group can be used instead.\n\nConceptually, Cohen's *d* is the difference between the two means\nexpressed in standard deviation units. (Notice its similarity to a *z*\nscore, which expresses the difference between an individual score and a\nmean in standard deviation units.) A Cohen's *d* of 0.50 means that the\ntwo group means differ by 0.50 standard deviations (half a standard\ndeviation). A Cohen's *d* of 1.20 means that they differ by 1.20\nstandard deviations. But how should we interpret these values in terms\nof the strength of the relationship or the size of the difference\nbetween the means? Table \\@ref(tab:guidelines) presents some guidelines\nfor interpreting Cohen's *d* values in psychological research\n[@cohen1992power]. Values near 0.20 are considered small, values near\n0.50 are considered medium, and values near 0.80 are considered large.\nThus a Cohen's *d* value of 0.50 represents a medium-sized difference\nbetween two means, and a Cohen's *d* value of 1.20 represents a very\nlarge difference in the context of psychological research. In the\nresearch by Ollendick and his colleagues, there was a large difference\n(*d* = 0.82) between the exposure and education conditions.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Guidelines for referring to Cohen’s *d* and Pearson’s *r* values as “strong,” “medium,” or “weak”.\n\n| Relationship strength | Cohen's *d* | Pearson's *r* |\n|:---------------------:|:-----------:|:-------------:|\n|     Strong/large      |   ± 0.80    |    ± 0.50     |\n|        Medium         |   ± 0.50    |    ± 0.30     |\n|      Weak/small       |   ± 0.20    |    ± 0.10     |\n:::\n:::\n\n\n\nCohen's *d* is useful because it has the same meaning regardless of the\nvariable being compared or the scale it was measured on. A Cohen's *d*\nof 0.20 means that the two group means differ by 0.20 standard\ndeviations whether we are talking about scores on the Rosenberg\nSelf-Esteem scale, reaction time measured in milliseconds, number of\nsiblings, or diastolic blood pressure measured in millimeters of\nmercury. Not only does this make it easier for researchers to\ncommunicate with each other about their results, it also makes it\npossible to combine and compare results across different studies using\ndifferent measures.\n\nBe aware that the term *effect size* can be misleading because it\nsuggests a causal relationship---that the difference between the two\nmeans is an \"effect\" of being in one group or condition as opposed to\nanother. Imagine, for example, a study showing that a group of\nexercisers is happier on average than a group of nonexercisers, with an\n\"effect size\" of *d* = 0.35. If the study was an experiment---with\nparticipants randomly assigned to exercise and no-exercise\nconditions---then one could conclude that exercising caused a small to\nmedium-sized increase in happiness. If the study was correlational,\nhowever, then one could conclude only that the exercisers were happier\nthan the nonexercisers by a small to medium-sized amount. In other\nwords, simply calling the difference an \"effect size\" does not make the\nrelationship a causal one.\n\n::: fyi\n##### Sex Differences Expressed as Cohen's *d* {.unnumbered}\n\nResearcher Janet Shibley Hyde has looked at the results of numerous\nstudies on psychological sex differences and expressed the results in\nterms of Cohen's *d* [@hyde2007new]. Following are a few of the values\nshe has found, averaging across several studies in each case. (Note that\nbecause she always treats the mean for men as $M_1$ and the mean for\nwomen as $M_2$, positive values indicate that men score higher and\nnegative values indicate that women score higher.)\n\n| Attribute                    |  *d*  |\n|:-----------------------------|:-----:|\n| Mathematical problem solving | +0.08 |\n| Reading comprehension        | −0.09 |\n| Smiling                      | −0.40 |\n| Aggression                   | +0.50 |\n| Attitudes toward casual sex  | +0.81 |\n| Leadership effectiveness     | −0.02 |\n\nHyde points out that although men and women differ by a large amount on\nsome variables (e.g., attitudes toward casual sex), they differ by only\na small amount on the vast majority. In many cases, Cohen's *d* is less\nthan 0.10, which she terms a \"trivial\" difference. (The difference in\ntalkativeness discussed in the @mehl2007women paper was also trivial:\n*d* = 0.06.) Although researchers and nonresearchers alike often\nemphasize sex differences, Hyde has argued that it makes at least as\nmuch sense to think of men and women as fundamentally similar. She\nrefers to this as the \"gender similarities hypothesis.\"\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Research on psychological sex differences has shown that there is essentially no difference in the leadership effectiveness of women and men. *Photo by Jason Goodman on Unsplash.*](images/ch12/leader.jpeg){fig-align='center' width=60%}\n:::\n:::\n\n\n:::\n\n### Correlations Between Quantitative Variables {.unnumbered}\n\nAs we have seen throughout the book, many interesting statistical\nrelationships take the form of correlations between quantitative\nvariables. For example, researchers Kurt Carlson and Jacqueline Conard\nconducted a study on the relationship between the alphabetical position\nof the first letter of people's last names (from A = 1 to Z = 26) and\nhow quickly those people responded to consumer appeals\n[@carlson2011last]. In one study, they sent e-mails to a large group of\nMBA students, offering free basketball tickets from a limited supply.\nThe result was that the further toward the end of the alphabet students'\nlast names were, the faster they tended to respond. These results are\nsummarized in Figure \\@ref(fig:line).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Line graph showing the relationship between the alphabetical position of people’s last names and how quickly those people respond to offers of consumer goods.](13-descriptives_files/figure-pdf/line-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\nSuch relationships are often presented using line graphs or\nscatterplots, which show how the level of one variable differs across\nthe range of the other. In the line graph in Figure \\@ref(fig:line), for\nexample, each point represents the mean response time for participants\nwith last names in the first, second, third, and fourth quartiles (or\nquarters) of the name distribution. It clearly shows how response time\ntends to decline as people's last names get closer to the end of the\nalphabet. The scatterplot in Figure \\@ref(fig:scatter2), which is\nreproduced from the Measurement chapter, shows the relationship between\n25 research methods students' scores on the Rosenberg Self-Esteem Scale\ngiven on two occasions a week apart. Here the points represent\nindividuals, and we can see that the higher students scored on the first\noccasion, the higher they tended to score on the second occasion. In\ngeneral, line graphs are used when the variable on the x-axis has (or is\norganized into) a small number of distinct values, such as the four\nquartiles of the name distribution. Scatterplots are used when the\nvariable on the x-axis has a large number of values, such as the\ndifferent possible self-esteem scores.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Statistical relationship between several college students’ scores on the Rosenberg Self-Esteem Scale given on two occasions a week apart.](13-descriptives_files/figure-pdf/scatter2-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\nThe data presented in Figure \\@ref(fig:scatter2) provide a good example\nof a positive relationship, in which higher scores on one variable tend\nto be associated with higher scores on the other (so that the points go\nfrom the lower left to the upper right of the graph). The data presented\nin Figure \\@ref(fig:line) provide a good example of a negative\nrelationship, in which higher scores on one variable tend to be\nassociated with lower scores on the other (so that the points go from\nthe upper left to the lower right).\n\nBoth of these examples are also linear relationships, in which the\npoints are reasonably well fit by a single straight line. [Nonlinear\nrelationships](#nonlinear-relationship) are those in which the points\nare better fit by a curved line. Figure \\@ref(fig:nonlinear2), for\nexample, shows a hypothetical relationship between the amount of sleep\npeople get per night and their level of depression. In this example, the\nline that best fits the points is a curve---a kind of upside down\n\"U\"---because people who get about eight hours of sleep tend to be the\nleast depressed, while those who get too little sleep and those who get\ntoo much sleep tend to be more depressed. Nonlinear relationships are\nnot uncommon in psychology, but a detailed discussion of them is beyond\nthe scope of this book.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hypothetical nonlinear relationship between sleep and depression.](13-descriptives_files/figure-pdf/nonlinear2-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\nAs we saw earlier in the book, the strength of a correlation between\nquantitative variables is typically measured using a statistic called\nPearson's *r*. As Figure \\@ref(fig:pearson2) shows, its possible values\nrange from −1.00, through zero, to +1.00. A value of 0 means there is no\nrelationship between the two variables. In addition to his guidelines\nfor interpreting Cohen's *d*, Cohen offered guidelines for interpreting\nPearson's *r* in psychological research (see Table\n\\@ref(tab:guidelines)). Values near ±.10 are considered small, values\nnear ± .30 are considered medium, and values near ±.50 are considered\nlarge. Notice that the sign of Pearson's *r* is unrelated to its\nstrength. Pearson's *r* values of +.30 and −.30, for example, are\nequally strong; it is just that one represents a moderate positive\nrelationship and the other a moderate negative relationship. Like\nCohen's *d*, Pearson's *r* is also referred to as a measure of \"effect\nsize\" even though the relationship may not be a causal one.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Pearson’s *r* ranges from −1.00 (representing the strongest possible negative relationship), through 0 (representing no relationship), to +1.00 (representing the strongest possible positive relationship).](images/ch12/pearson.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\nThe computations for Pearson's *r* are more complicated than those for\nCohen's *d*. Although you may never have to do them by hand, it is still\ninstructive to see how. Computationally, Pearson's *r* is the \"mean\ncross-product of *z* scores.\" To compute it, one starts by transforming\nall the scores to *z* scores. For the *X* variable, subtract the mean of\n*X* from each score and divide each difference by the standard deviation\nof *X*. For the *Y* variable, subtract the mean of *Y* from each score\nand divide each difference by the standard deviation of Y. Then, for\neach individual, multiply the two *z* scores together to form a\ncross-product. Finally, take the mean of the cross-products. The formula\nlooks like this:\n\n$r=\\frac{\\sum(z_xz_y)}{N}$\n\nTable \\@ref(tab:pearsonmath) illustrates these computations for a small\nset of data. The first column lists the scores for the *X* variable,\nwhich has a mean of 4.00 and a standard deviation of 1.90. The second\ncolumn is the *z* score for each of these raw scores. The third and\nfourth columns list the raw scores for the *Y* variable, which has a\nmean of 40 and a standard deviation of 11.78, and the corresponding *z*\nscores. The fifth column lists the cross-products. For example, the\nfirst one is 0.00 multiplied by −0.85, which is equal to 0.00. The\nsecond is 1.58 multiplied by 1.19, which is equal to 1.88. The mean of\nthese cross-products, shown at the bottom of that column, is Pearson's\n*r*, which in this case is +.53. There are other formulas for computing\nPearson's *r* by hand that may be quicker. This approach, however, is\nmuch clearer in terms of communicating conceptually what Pearson's *r*\nis.\n\n|      $X$      | $z_X$ |      $Y$       | $z_Y$ |  $z_Xz_Y$  |\n|:-------------:|:-----:|:--------------:|:-----:|:----------:|\n|       4       | 0.00  |       30       | -0.85 |    0.00    |\n|       7       | 1.58  |       54       | 1.19  |    1.88    |\n|       2       | -1.05 |       23       | -1.44 |    1.53    |\n|       5       | 0.53  |       43       | 0.26  |    0.13    |\n|       2       | -1.05 |       50       | 0.85  |   -0.89    |\n| $M_X$ = 4.00  |       | $M_Y$ = 40.00  |       | $r$ = 0.53 |\n| $SD_X$ = 1.90 |       | $SD_Y$ = 11.78 |       |            |\n\n: (#tab:pearsonmath) Sample computations for Pearson's *r*.\n\nThere are two common situations in which the value of Pearson's *r* can\nbe misleading. One is when the relationship under study is nonlinear.\nEven though Figure \\@ref(fig:nonlinear2) shows a fairly strong\nrelationship between depression and sleep, Pearson's *r* would be close\nto zero because the points in the scatterplot are not well fit by a\nsingle straight line. This means that it is important to make a\nscatterplot and confirm that a relationship is approximately linear\nbefore using Pearson's *r*. The other is when one or both of the\nvariables have a limited range in the sample relative to the population.\nThis is referred to as [restriction of range]. Assume, for example, that\nthere is a strong negative correlation between people's age and their\nenjoyment of hip hop music as shown by the scatterplot in Figure\n\\@ref(fig:range). Pearson's *r* here is −.77. However, if we were to\ncollect data only from 18- to 24-year-olds---represented by the shaded\narea of Figure \\@ref(fig:range)---then the relationship would seem to be\nquite weak. In fact, Pearson's *r* for this restricted range of ages is\n0. It is a good idea, therefore, to design studies to avoid restriction\nof range. For example, if age is one of your primary variables, then you\ncan plan to collect data from people of a wide range of ages. Because\nrestriction of range is not always anticipated or easily avoidable,\nhowever, it is good practice to examine your data for possible\nrestriction of range and to interpret Pearson's *r* in light of it.\n(There are also statistical methods to correct Pearson's *r* for\nrestriction of range, but they are beyond the scope of this book).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hypothetical data showing how a strong overall correlation can appear to be weak when one variable has a restricted range. The overall correlation here is −.77, but the correlation for the 18- to 24-year-olds (in the blue box) is 0.](13-descriptives_files/figure-pdf/range-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Differences between groups or conditions are typically described in\n    terms of the means and standard deviations of the groups or\n    conditions or in terms of Cohen's *d* and are presented in bar\n    graphs.\n-   Cohen's *d* is a measure of relationship strength (or effect size)\n    for differences between two group or condition means. It is the\n    difference of the means divided by the standard deviation. In\n    general, values of ±0.20, ±0.50, and ±0.80 can be considered small,\n    medium, and large, respectively.\n-   Correlations between quantitative variables are typically described\n    in terms of Pearson's *r* and presented in line graphs or\n    scatterplots.\n-   Pearson's *r* is a measure of relationship strength (or effect size)\n    for relationships between quantitative variables. It is the mean\n    cross-product of the two sets of *z* scores. In general, values of\n    ±.10, ±.30, and ±.50 can be considered small, medium, and large,\n    respectively.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: The following data represent scores on the Rosenberg\n    Self-Esteem Scale for a sample of 10 Japanese college students and\n    10 American college students. (Although hypothetical, these data are\n    consistent with empirical findings [@schmitt2005simultaneous]).\n    Compute the means and standard deviations of the two groups, make a\n    bar graph, compute Cohen's *d*, and describe the strength of the\n    relationship in words.\n\n| Japan | United States |\n|:-----:|:-------------:|\n|  25   |      27       |\n|  20   |      30       |\n|  24   |      34       |\n|  28   |      37       |\n|  30   |      26       |\n|  32   |      24       |\n|  21   |      28       |\n|  24   |      35       |\n|  20   |      33       |\n|  26   |      36       |\n\n2.  Practice: The hypothetical data that follow are extroversion scores\n    and the number of Facebook friends for 15 college students. Make a\n    scatterplot for these data, compute Pearson's *r*, and describe the\n    relationship in words.\n\n| Extroversion | Facebook Friends |\n|:------------:|:----------------:|\n|      8       |        75        |\n|      10      |       315        |\n|      4       |        28        |\n|      6       |       214        |\n|      12      |       176        |\n|      14      |        95        |\n|      10      |       120        |\n|      11      |       150        |\n|      4       |        32        |\n|      13      |       250        |\n|      5       |        99        |\n|      7       |       136        |\n|      8       |       185        |\n|      11      |        88        |\n|      10      |       144        |\n:::\n\n## Expressing Your Results\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Write out simple descriptive statistics in American Psychological\n    Association (APA) style.\n2.  Interpret and create simple APA-style graphs---including bar graphs,\n    line graphs, and scatterplots.\n3.  Interpret and create simple APA-style tables---including tables of\n    group or condition means and correlation matrixes.\n:::\n\nOnce you have conducted your descriptive statistical analyses, you will\nneed to present them to others. In this section, we focus on presenting\ndescriptive statistical results in writing, in graphs, and in\ntables---following American Psychological Association (APA) guidelines\nfor written research reports. These principles can be adapted easily to\nother presentation formats such as posters and slide show presentations.\n\n### Presenting Descriptive Statistics in Writing {.unnumbered}\n\nWhen you have a small number of results to report, it is often most\nefficient to write them out. There are a few important APA style\nguidelines here. First, statistical results are always presented in the\nform of numerals rather than words and are usually rounded to two\ndecimal places (e.g., \"2.00\" rather than \"two\" or \"2\"). They can be\npresented either in the narrative description of the results or\nparenthetically---much like reference citations. Here are some examples:\n\n> The mean age of the participants was 22.43 years with a standard\n> deviation of 2.34.\n\n> Among the low self-esteem participants, those in a negative mood\n> expressed stronger intentions to have unprotected sex (*M* = 4.05,\n> *SD* = 2.32) than those in a positive mood (*M* = 2.15, *SD* = 2.27).\n\n> The treatment group had a mean of 23.40 (*SD* = 9.33), while the\n> control group had a mean of 20.87 (*SD* = 8.45).\n\n> The test-retest correlation was .96.\n\n> There was a moderate negative correlation between the alphabetical\n> position of respondents' last names and their response time (*r* =\n> −.27).\n\nNotice that when presented in the narrative, the terms *mean* and\n*standard deviation* are written out, but when presented\nparenthetically, the symbols *M* and *SD* are used instead. Notice also\nthat it is especially important to use parallel construction to express\nsimilar or comparable results in similar ways. The third example is\n*much* better than the following nonparallel alternative:\n\n> The treatment group had a mean of 23.40 (*SD* = 9.33), while 20.87 was\n> the mean of the control group, which had a standard deviation of 8.45.\n\n### Presenting Descriptive Statistics in Graphs {.unnumbered}\n\nWhen you have a large number of results to report, you can often do it\nmore clearly and efficiently with a graph. When you prepare graphs for\nan APA-style research report, there are some general guidelines that you\nshould keep in mind. First, the graph should always add important\ninformation rather than repeat information that already appears in the\ntext or in a table. (If a graph presents information more clearly or\nefficiently, then you should keep the graph and eliminate the text or\ntable.) Second, graphs should be as simple as possible. For example, the\n*Publication Manual* discourages the use of color in figures for print\npublication unless absolutely necessary (although color can still be an\neffective element in posters, slide show presentations, or textbooks.)\nThird, graphs should be interpretable on their own. A reader should be\nable to understand the basic result based only on the graph and its\ncaption and should not have to refer to the text for an explanation.\n\nThere are also several more technical guidelines for graphs that include\nthe following:\n\n-   Layout\n    -   The graph should be slightly wider than it is tall.\n    -   The independent variable should be plotted on the x-axis and the\n        dependent variable on the y-axis.\n    -   Values should increase from left to right on the x-axis and from\n        bottom to top on the y-axis.\n-   Axis Labels and Legends\n    -   Axis labels should be clear and concise and include the units of\n        measurement if they do not appear in the caption.\n    -   Axis labels should be parallel to the axis.\n    -   Legends should appear within the boundaries of the graph.\n    -   Text should be in the same simple font throughout and differ by\n        no more than four points.\n-   Captions\n    -   Captions should briefly describe the figure, explain any\n        abbreviations, and include the units of measurement if they do\n        not appear in the axis labels.\n\n#### Bar Graphs {.unnumbered}\n\nAs we have seen throughout this book, [bar graphs](#bar-graph) are\ngenerally used to present and compare the mean scores for two or more\ngroups or conditions. The bar graph in Figure \\@ref(fig:apabar) is an\nAPA-style version of Figure \\@ref(fig:phobia). Notice that it conforms\nto all the guidelines listed. A new element in Figure \\@ref(fig:apabar)\nis the smaller vertical bars that extend both upward and downward from\nthe top of each main bar. These are [error bars], and they represent the\nvariability in each group or condition. Although they sometimes extend\none standard deviation in each direction, they are more likely to extend\none standard error in each direction (as in Figure \\@ref(fig:apabar)).\nThe [standard error] is the standard deviation of the group divided by\nthe square root of the sample size of the group. The standard error is\nused because, in general, a difference between the means of two group\nthat is greater than two standard errors is statistically significant.\nThus one can \"see\" whether a difference is statistically significant\nbased on a bar graph with standard error bars. It is also common for\nerror bars to represent the 95% confidence interval of the man. These\nerror bars are interpreted differently: if there is any overlap between\nerror bars that represent the 95% confidence interval when comparing a\nplot of two means, the difference is *not* statistically significant.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sample APA-style bar graph, with error bars representing the standard errors, based on research by Ollendick and colleagues.](images/ch12/apabar.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n#### Line Graphs {.unnumbered}\n\n[Line graphs](#line-graph) are used to present correlations between\nquantitative variables when the independent variable has, or is\norganized into, a relatively small number of distinct levels. Each point\nin a line graph represents the mean score on the dependent variable for\nparticipants at one level of the independent variable. Figure\n\\@ref(fig:apaline) is an APA-style version of the results of Carlson and\nConard. Notice that it includes error bars representing the standard\nerror and conforms to all the stated guidelines.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sample APA-style line graph based on research by Carlson and Conard.](images/ch12/apaline.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nIn most cases, the information in a line graph could just as easily be\npresented in a bar graph. In Figure \\@ref(fig:apabar), for example, one\ncould replace each point with a bar that reaches up to the same level\nand leave the error bars right where they are. This emphasizes the\nfundamental similarity of the two types of statistical relationship.\nBoth are differences in the average score on one variable across levels\nof another. The convention followed by most researchers, however, is to\nuse a bar graph when the variable plotted on the x-axis is categorical\nand a line graph when it is quantitative.\n\n#### Scatterplots {.unnumbered}\n\n[Scatterplots](#scatterplot) are used to present relationships between\nquantitative variables when the variable on the x-axis (typically the\nindependent variable) has a large number of levels. Each point in a\nscatterplot represents an individual rather than the mean for a group of\nindividuals, and there are no lines connecting the points. The graph in\nFigure \\@ref(fig:apascatter) is an APA-style version of Figure\n\\@ref(fig:scatter2), which illustrates a few additional points. First,\nwhen the variables on the x-axis and y-axis are conceptually similar and\nmeasured on the same scale---as here, where they are measures of the\nsame variable on two different occasions---this can be emphasized by\nmaking the axes the same length. Second, when two or more individuals\nfall at exactly the same point on the graph, one way this can be\nindicated is by offsetting the points slightly along the x-axis. Other\nways are by displaying the number of individuals in parentheses next to\nthe point or by making the point larger or darker in proportion to the\nnumber of individuals. Finally, the straight line that best fits the\npoints in the scatterplot, which is called the regression line, can also\nbe included.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sample APA-style scatterplot.](images/ch12/apascatter.png){fig-align='center' width=55%}\n:::\n:::\n\n\n\n::: fyi\n##### Jitter plots {.unnumbered}\n\nDevelopments in data visualization software have led to new options for\ndisplaying results. One data visualization method that has increased in\npopularity is the **jitter plot**. Like scatterplots, jitter plots show\nindividual data points. The data visualization software that generates\nthe plots allows the points to shift slightly in a random direction in\norder to reduce data point overlap -- hence the word *jitter* in the\nname. These plots can display the same data as bar and line graphs\n(i.e., means and error bars) while helping the viewer grasp the full\nvariability in the data. Figure \\@ref(fig:apajitter) shows an example in\nAPA format.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sample APA-style jitter plot showing hypothetical data.](images/ch12/apajitter.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\nThe jitter plot above was generated using the free online application\n[ShowMyData](https://www.showmydata.org/).\n:::\n\n### Expressing Descriptive Statistics in Tables {.unnumbered}\n\nLike graphs, tables can be used to present large amounts of information\nclearly and efficiently. The same general principles apply to tables as\napply to graphs. They should add important information to the\npresentation of your results, be as simple as possible, and be\ninterpretable on their own. Again, we focus here on tables for an\nAPA-style manuscript.\n\nThe most common use of tables is to present several means and standard\ndeviations---usually for complex research designs with multiple\nindependent and dependent variables. Figure \\@ref(fig:apatable), for\nexample, shows the results of a hypothetical study similar to the one by\n@macdonald2002self. (The means in Figure \\@ref(fig:apatable) are the\nmeans reported by MacDonald and Martineau, but the standard errors are\nnot). Recall that these researchers categorized participants as having\nlow or high self-esteem, put them into a negative or positive mood, and\nmeasured their intentions to have unprotected sex. They also measured\nparticipants' attitudes toward unprotected sex. Notice that the table\nincludes horizontal lines spanning the entire table at the top and\nbottom, and just beneath the column headings. Furthermore, every column\nhas a heading---including the leftmost column---and there are additional\nheadings that span two or more columns that help to organize the\ninformation and present it more efficiently. Finally, notice that\nAPA-style tables are numbered consecutively starting at 1 (Table 1,\nTable 2, and so on) and given a brief but clear and descriptive title.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sample APA-style table presenting means and standard deviations.](images/ch12/apatable.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\nAnother common use of tables is to present correlations---usually\nmeasured by Pearson's *r*---among several variables. This is called a\n[correlation matrix]. Figure \\@ref(fig:apamatrix) is a correlation\nmatrix based on a study by David McCabe and colleagues\n[@mccabe2010relationship]. They were interested in the relationships\nbetween working memory and several other variables. We can see from the\ntable that the correlation between working memory and executive\nfunction, for example, was an extremely strong .96, that the correlation\nbetween working memory and vocabulary was a medium .27, and that all the\nmeasures except vocabulary tend to decline with age. Notice here that\nonly half the table is filled in because the other half would have\nidentical values. For example, the Pearson's *r* value in the upper\nright corner (working memory and age) would be the same as the one in\nthe lower left corner (age and working memory). The correlation of a\nvariable with itself is always 1.00, so these values are replaced by\ndashes to make the table easier to read.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sample APA-style table (correlation matrix) based on research by McCabe and colleagues.](images/ch12/apamatrix.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\nAs with graphs, precise statistical results that appear in a table do\nnot need to be repeated in the text. Instead, the writer can note major\ntrends and alert the reader to details (e.g., specific correlations)\nthat are of particular interest.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   In an APA-style article, simple results are most efficiently\n    presented in the text, while more complex results are most\n    efficiently presented in graphs or tables.\n-   APA style includes several rules for presenting numerical results in\n    the text. These include using words only for numbers less than 10\n    that do not represent precise statistical results, and rounding\n    results to two decimal places, using words (e.g., \"mean\") in the\n    text and symbols (e.g., \"*M*\") in parentheses.\n-   APA style includes several rules for presenting results in graphs\n    and tables. Graphs and tables should add information rather than\n    repeating information, be as simple as possible, and be\n    interpretable on their own with a descriptive title.\n:::\n\n::: exercises\n##### EXERCISE {.unnumbered}\n\n1.  Practice: In a classic study, men and women rated the importance of\n    physical attractiveness in both a short-term mate and a long-term\n    mate (Buss & Schmitt, 1993).Buss, D. M., & Schmitt, D. P. (1993).\n    Sexual strategies theory: A contextual evolutionary analysis of\n    human mating. Psychological Review, 100, 204--232. The means and\n    standard deviations are as follows. Men / Short Term: *M* = 5.67,\n    *SD* = 2.34; Men / Long Term: *M* = 4.43, *SD* = 2.11; Women / Short\n    Term: *M* = 5.67, *SD* = 2.48; Women / Long Term: *M* = 4.22, *SD* =\n    1.98. Present these results (a) in writing, (b) in a graph, and (c)\n    in a table.\n:::\n\n## Conducting Your Analyses\n\n::: learningobjectives\n##### LEARNING OBJECTIVE {.unnumbered}\n\n1.  Describe the steps involved in preparing and analyzing a typical set\n    of raw data.\n:::\n\nEven when you understand the statistics involved, analyzing data can be\na complicated process. It is likely that for each of several\nparticipants, there are data for several different variables:\ndemographics such as sex and age, one or more independent variables, one\nor more dependent variables, and perhaps a manipulation check.\nFurthermore, the \"raw\" (unanalyzed) data might take several different\nforms---completed paper-and-pencil questionnaires, computer files filled\nwith numbers or text, videos, or written notes---and these may have to\nbe organized, coded, or combined in some way. There might even be\nmissing, incorrect, or just \"suspicious\" responses that must be dealt\nwith. In this section, we consider some practical advice to make this\nprocess as organized and efficient as possible.\n\n### Prepare Your Data for Analysis {.unnumbered}\n\nWhether your raw data are on paper or in a computer file (or both),\nthere are a few things you should do before you begin analyzing them.\nFirst, be sure they do not include any information that might identify\nindividual participants and be sure that you have a secure location\nwhere you can store the data and a separate secure location where you\ncan store any consent forms. Unless the data are highly sensitive, a\nlocked room or password-protected computer is usually good enough. It is\nalso a good idea to make photocopies or backup files of your data and\nstore them in yet another secure location---at least until the project\nis complete. Professional researchers usually keep a copy of their raw\ndata and consent forms for several years in case questions about the\nprocedure, the data, or participant consent arise after the project is\ncompleted.\n\nNext, you should check your [raw data] to make sure that they are\ncomplete and appear to have been accurately recorded (whether it was\nparticipants, yourself, or a computer program that did the recording).\nAt this point, you might find that there are illegible or missing\nresponses, or obvious misunderstandings (e.g., a response of \"12\" on a\n1-to-10 rating scale). You will have to decide whether such problems are\nsevere enough to make a participant's data unusable. If information\nabout the main independent or dependent variable is missing, or if\nseveral responses are missing or suspicious, you may have to exclude\nthat participant's data from the analyses. If you do decide to exclude\nany data, do not throw them away or delete them because you or another\nresearcher might want to see them later. Instead, set them aside and\nkeep notes about why you decided to exclude them because you will need\nto report this information.\n\nNow you are ready to enter your data in a spreadsheet program or, if it\nis already in a computer file, to format it for analysis. You can use a\ngeneral spreadsheet program like Microsoft Excel or a statistical\nanalysis program like SPSS to create your [data file]. (Data files\ncreated in one program can usually be converted to work with other\nprograms.) The most common format is for each row to represent a\nparticipant and for each column to represent a variable (with the\nvariable name at the top of each column). A sample data file is shown in\nTable \\@ref(tab:sampledata). The first column contains participant\nidentification numbers. This is followed by columns containing\ndemographic information (sex and age), independent variables (mood, four\nself-esteem items, and the total of the four self-esteem items), and\nfinally dependent variables (intentions and attitudes). Categorical\nvariables can usually be entered as category labels (e.g., \"M\" and \"F\"\nfor male and female) or as numbers (e.g., \"0\" for negative mood and \"1\"\nfor positive mood). Although category labels are often clearer, some\nanalyses might require numbers. SPSS allows you to enter numbers but\nalso attach a category label to each number.\n\n|  ID | SEX | AGE | MOOD | SE1 | SE2 | SE3 | SE4 | TOTAL | INT | ATT |\n|----:|:----|----:|-----:|----:|----:|----:|----:|------:|----:|----:|\n|   1 | M   |  20 |    1 |   2 |   3 |   2 |   3 |    10 |   6 |   5 |\n|   2 | F   |  22 |    1 |   1 |   0 |   2 |   1 |     4 |   4 |   4 |\n|   3 | F   |  19 |    0 |   2 |   2 |   2 |   2 |     8 |   2 |   3 |\n|   4 | F   |  24 |    0 |   3 |   3 |   2 |   3 |    11 |   5 |   6 |\n\n: (#tab:sampledata) Sample data file.\n\nIf you have multiple-response measures---such the self-esteem measure in\nTable \\@ref(tab:sampledata)---you could combine the items by hand and\nthen enter the total score in your spreadsheet. However, it is much\nbetter to enter each response as a separate variable in the\nspreadsheet---as with the self-esteem measure in Table\n\\@ref(tab:sampledata)---and use the software to combine them (e.g.,\nusing the \"AVERAGE\" function in Excel or the \"Compute\" function in\nSPSS). Not only is this approach more accurate, but it allows you to\ndetect and correct errors, to assess internal consistency, and to\nanalyze individual responses if you decide to do so later.\n\n### Preliminary Analyses {.unnumbered}\n\nBefore turning to your primary research questions, there are often\nseveral preliminary analyses to conduct. For multiple-response measures,\nyou should assess the internal consistency of the measure. Statistical\nprograms like SPSS will allow you to compute Cronbach's α or Cohen's κ.\nIf this is beyond your comfort level, you can still compute and evaluate\na split-half correlation.\n\nNext, you should analyze each important variable separately. (This is\nnot necessary for manipulated independent variables, of course, because\nyou as the researcher determined what the distribution would be.) Make\nhistograms for each one, note their shapes, and compute the common\nmeasures of central tendency and variability. Be sure you understand\nwhat these statistics *mean* in terms of the variables you are\ninterested in. For example, a distribution of self-report happiness\nratings on a 1-to-10-point scale might be unimodal and negatively skewed\nwith a mean of 8.25 and a standard deviation of 1.14. But what this\n*means* is that most participants rated themselves fairly high on the\nhappiness scale, with a small number rating themselves noticeably lower.\n\nNow is the time to identify outliers, examine them more closely, and\ndecide what to do about them. You might discover that what at first\nappears to be an outlier is the result of a response being entered\nincorrectly in the data file, in which case you only need to correct the\ndata file and move on. Alternatively, you might suspect that an outlier\nrepresents some other kind of error, misunderstanding, or lack of effort\nby a participant. For example, in a reaction time distribution in which\nmost participants took only a few seconds to respond, a participant who\ntook 3 minutes to respond would be an outlier. It seems likely that this\nparticipant did not understand the task (or at least was not paying very\nclose attention). Also, including his or her reaction time would have a\nlarge impact on the mean and standard deviation for the sample. In\nsituations like this, it can be justifiable to exclude the outlying\nresponse or participant from the analyses. If you do this, however, you\nshould keep notes on which responses or participants you have excluded\nand why, and apply those same criteria consistently to every response\nand every participant. When you present your results, you should\nindicate how many responses or participants you excluded and the\nspecific criteria that you used. And again, do not literally throw away\nor delete the data that you choose to exclude. Just set them aside\nbecause you or another researcher might want to see them later.\n\nKeep in mind that outliers do not *necessarily* represent an error,\nmisunderstanding, or lack of effort. They might represent truly extreme\nresponses or participants. For example, in one large college student\nsample, the vast majority of participants reported having had fewer than\n15 sexual partners, but there were also a few extreme scores of 60 or 70\n[@brown1999estimating]. Although these scores might represent errors,\nmisunderstandings, or even intentional exaggerations, it is also\nplausible that they represent honest and even accurate estimates. One\nstrategy here would be to use the median and other statistics that are\nnot strongly affected by the outliers. Another would be to analyze the\ndata both including and excluding any outliers. If the results are\nessentially the same, which they often are, then it makes sense to leave\nthe outliers. If the results differ depending on whether the outliers\nare included or excluded them, then both analyses can be reported and\nthe differences between them discussed.\n\n### Answer Your Research Questions {.unnumbered}\n\nFinally, you are ready to answer your primary research questions. If you\nare interested in a difference between group or condition means, you can\ncompute the relevant group or condition means and standard deviations,\nmake a bar graph to display the results, and compute Cohen's *d*. If you\nare interested in a correlation between quantitative variables, you can\nmake a line graph or scatterplot (be sure to check for nonlinearity and\nrestriction of range) and compute Pearson's *r*.\n\nAt this point, you may want to conduct exploratory analyses that might\nprovide the basis for future research (and material for the discussion\nsection of your paper). It is important to be cautious, however, because\ncomplex sets of data are likely to include \"patterns\" that occurred\nentirely by chance. Thus results discovered while \"fishing\" should be\nreplicated in at least one new study before being presented as new\nphenomena in their own right.\n\n### Understand Your Descriptive Statistics {.unnumbered}\n\nIn the next chapter, we will consider inferential statistics---a set of\ntechniques for deciding whether the results for your sample are likely\nto apply to the population. Although inferential statistics are\nimportant for reasons that will be explained shortly, beginning\nresearchers sometimes forget that their descriptive statistics really\ntell \"what happened\" in their study. For example, imagine that a\ntreatment group of 50 participants has a mean score of 34.32 (*SD* =\n10.45), a control group of 50 participants has a mean score of 21.45\n(*SD* = 9.22), and Cohen's *d* is an extremely strong 1.31. Although\nconducting and reporting inferential statistics (like a *t* test) would\ncertainly be a required part of any formal report on this study, it\nshould be clear from the descriptive statistics alone that the treatment\nworked. Or imagine that a scatterplot shows an indistinct \"cloud\" of\npoints and Pearson's *r* is a trivial −.02. Again, although conducting\nand reporting inferential statistics would be a required part of any\nformal report on this study, it should be clear from the descriptive\nstatistics alone that the variables are essentially unrelated. The point\nis that you should always be sure that you thoroughly understand your\nresults at a descriptive level first, and then move on to the\ninferential statistics.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Raw data must be prepared for analysis by examining them for\n    possible errors, organizing them, and entering them into a\n    spreadsheet program.\n-   Preliminary analyses on any data set include checking the\n    reliability of measures, evaluating the effectiveness of any\n    manipulations, examining the distributions of individual variables,\n    and identifying outliers.\n-   Outliers that appear to be the result of an error, a\n    misunderstanding, or a lack of effort can be excluded from the\n    analyses. The criteria for excluded responses or participants should\n    be applied in the same way to all the data and described when you\n    present your results. Excluded data should be set aside rather than\n    destroyed or deleted in case they are needed later.\n-   Descriptive statistics tell the story of what happened in a study.\n    Although inferential statistics are also important, it is essential\n    to understand the descriptive statistics first.\n:::\n\n::: exercises\n##### EXERCISE {.unnumbered}\n\n1.  Discussion: What are at least two reasonable ways to deal with each\n    of the following outliers based on the discussion in this\n    chapter? (a) A participant estimating ordinary people's heights\n    estimates one woman's height to be \"84 inches\" tall. (b) In a study\n    of memory for ordinary objects, one participant scores 0 out\n    of 15. (c) In response to a question about how many \"close friends\"\n    she has, one participant writes \"32.\"\n:::\n\n## Glossary\n\n##### bar graph {#bar-graph .unnumbered}\n\nA graph used to show differences between the mean scores of two or more\ngroups or conditions.\n\n##### central tendency {.unnumbered}\n\nThe middle of a distribution. The mean, median, and mode are measures of\ncentral tendency.\n\n##### Cohen's *d* {#cohens-d .unnumbered}\n\nA measure of relationship strength or \"effect size\" for a difference\nbetween two groups or conditions.\n\n##### correlation matrix {.unnumbered}\n\nA table that shows the correlations among several variables.\n\n##### data file {.unnumbered}\n\nA computer file that contains data formatted for statistical analysis.\n\n##### descriptive statistics {.unnumbered}\n\nA set of techniques for summarizing and displaying data.\n\n##### distribution {.unnumbered}\n\nThe way the scores on a variable are distributed across the levels of\nthat variable.\n\n##### effect size {.unnumbered}\n\nAnother name for measures of relationship strength, including Cohen's\n*d* and Pearson's *r*.\n\n##### error bars {.unnumbered}\n\nIn bar graphs and line graphs, vertical lines that show the amount of\nvariability around the mean in each group or condition. They typically\nextend upward and downward one standard error from the top of each bar\nor point.\n\n##### frequency table {.unnumbered}\n\nA table for displaying the distribution of a variable. The first column\nlists the values of the variable, and the second column lists the\nfrequency of each score.\n\n##### histogram {.unnumbered}\n\nA graph for displaying the distribution of a variable. The *x*-axis\nrepresents the values of the variable, and the *y*-axis represents the\nfrequency of each score.\n\n##### line graph {#line-graph .unnumbered}\n\nA graph used to show the relationship between two quantitative\nvariables. For each level of the *X* variable, there is a point\nrepresenting the mean of the *Y* variable. The points are connected by\nlines.\n\n##### mean {.unnumbered}\n\nThe most common measure of central tendency. The sum of the scores\ndivided by the number of scores.\n\n##### median {.unnumbered}\n\nA measure of central tendency. The value such that half the scores in\nthe distribution are lower than it and half are higher than it.\n\n##### mode {.unnumbered}\n\nA measure of central tendency. The most frequently occurring score in\nthe distribution.\n\n##### nonlinear relationship {#nonlinear-relationship .unnumbered}\n\nA statistical relationship in which as the *X* variable increases, the\n*Y* variable does not increase or decrease at a constant rate. Such\nrelationships are best described by a curved line.\n\n##### outlier {.unnumbered}\n\nAn extreme score that is far removed from the rest of the scores in the\ndistribution.\n\n##### percentile rank {.unnumbered}\n\nA measure of the location of a score within its distribution. The\npercentage of scores below a particular score.\n\n##### range {.unnumbered}\n\nA measure of variability. The difference between the highest and lowest\nscores in the distribution.\n\n##### raw data {.unnumbered}\n\nData in the form in which they were originally collected (e.g.,\ncompleted questionnaires).\n\n##### restriction of range {.unnumbered}\n\nWhen the data used to assess a statistical relationship include a\nlimited range of scores on either the *X* or *Y* variable, relative to\nthe range of scores in the population. This makes the statistical\nrelationships appear weaker than it actually is.\n\n##### scatterplot {#scatterplot .unnumbered}\n\nA graph used to show the correlation between two quantitative variables.\nFor each individual, there is a point representing that individual's\nscore on both the *X* and *Y* variables.\n\n##### skewed {.unnumbered}\n\nRefers to an asymmetrical distribution. A positively skewed distribution\nhas a relatively long positive tail, and a negatively skewed\ndistribution has a relatively long negative tail.\n\n##### standard deviation {.unnumbered}\n\nThe most common measure of variability. The square root of the mean of\nthe squared differences between the scores and the mean. Also the square\nroot of the variance.\n\n##### standard error {.unnumbered}\n\nThe standard deviation divided by the square root of the sample size.\nOften used for error bars in graphs.\n\n##### symmetrical {.unnumbered}\n\nRefers to a distribution in which the left and right sides are near\nmirror images of each other.\n\n##### variability {.unnumbered}\n\nThe extent to which the scores in a distribution vary around their\ncentral tendency.\n\n##### variance {.unnumbered}\n\nA measure of variability. The mean of the squared differences between\nthe scores and the mean. Also the square of the standard deviation.\n\n##### *z* score {.unnumbered}\n\nA measure of the location of a score within its distribution. The score\nminus the mean, divided by the standard deviation.\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "13-descriptives_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}