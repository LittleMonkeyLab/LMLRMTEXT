{
  "hash": "dc530a80a980186dc5d5fc1b5c96a41a",
  "result": {
    "markdown": "# Psychological Measurement\n\nResearchers Tara MacDonald and Alanna Martineau were interested in the\neffect of female college students' moods on their intentions to have\nunprotected sexual intercourse [@macdonald2002self]. In a carefully\ndesigned empirical study, they found that being in a negative mood\nincreased intentions to have unprotected sex---but only for students who\nwere low in self-esteem. Although there are many challenges involved in\nconducting a study like this, one of the primary ones is the measurement\nof the relevant variables. In this study, the researchers needed to know\nwhether each of their participants had high or low self-esteem, which of\ncourse required measuring their self-esteem. They also needed to be sure\nthat their attempt to put people into a negative mood (by having them\nthink negative thoughts) was successful, which required measuring their\nmoods. Finally, they needed to see whether self-esteem and mood were\nrelated to participants' intentions to have unprotected sexual\nintercourse, which required measuring these intentions.\n\nTo students who are just getting started in psychological research, the\nchallenge of measuring such variables might seem insurmountable. Is it\nreally possible to measure things as intangible as self-esteem, mood, or\nan intention to do something? The answer is a resounding yes, and in\nthis chapter we look closely at the nature of the variables that\npsychologists study and how they can be measured. We also look at some\npractical issues in psychological measurement.\n\n::: fyi\n##### Do You Feel You Are a Person of Worth? {.unnumbered}\n\nThe Rosenberg Self-Esteem Scale is one of the most common measures of\nself-esteem and the one that MacDonald and Martineau used in their study\n[@rosenberg1989society]. Participants respond to each of the 10 items\nthat follow with a rating on a 4-point scale: *Strongly Agree, Agree,\nDisagree, Strongly Disagree*. Score Items 1, 2, 4, 6, and 7 by assigning\n3 points for each *Strongly Agree* response, 2 for each *Agree*, 1 for\neach *Disagree*, and 0 for each *Strongly Disagree*. Reverse the scoring\nfor Items 3, 5, 8, 9, and 10 by assigning 0 points for each *Strongly\nAgree*, 1 point for each *Agree*, and so on. The overall score is the\ntotal number of points.\n\n1.  I feel that I'm a person of worth, at least on an equal plane with\n    others.\n2.  I feel that I have a number of good qualities.\n3.  All in all, I am inclined to feel that I am a failure.\n4.  I am able to do things as well as most other people.\n5.  I feel I do not have much to be proud of.\n6.  I take a positive attitude toward myself.\n7.  On the whole, I am satisfied with myself.\n8.  I wish I could have more respect for myself.\n9.  I certainly feel useless at times.\n10. At times I think I am no good at all.\n:::\n\n## Understanding Psychological Measurement\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Define measurement and give several examples of measurement in\n    psychology.\n2.  Explain what a psychological construct is and give several examples.\n3.  Distinguish conceptual from operational definitions, give examples\n    of each, and create simple operational definitions.\n4.  Distinguish the four levels of measurement, give examples of each,\n    and explain why this distinction is important.\n:::\n\n### What Is Measurement? {.unnumbered}\n\n[Measurement] is the assignment of scores to individuals so that the\nscores represent some characteristic of the individuals. This very\ngeneral definition is consistent with the kinds of measurement that\neveryone is familiar with---for example, weighing oneself by stepping\nonto a bathroom scale, or checking the internal temperature of a\nroasting turkey by inserting a meat thermometer. It is also consistent\nwith measurement throughout the sciences. In physics, for example, one\nmight measure the potential energy of an object in Earth's gravitational\nfield by finding its mass and height (which of course requires measuring\nthose variables) and then multiplying them together along with the\ngravitational acceleration of Earth (9.8 m/s2). The result of this\nprocedure is a score that represents the object's potential energy.\n\nOf course this general definition of measurement is consistent with\nmeasurement in psychology too. Psychological measurement is often\nreferred to as [psychometrics]. Imagine, for example, that a cognitive\npsychologist wants to measure a person's working memory capacity---his\nor her ability to hold in mind and think about several pieces of\ninformation all at the same time. To do this, she might use a backward\ndigit span task, where she reads a list of two digits to the person and\nasks him or her to repeat them in reverse order. She then repeats this\nseveral times, increasing the length of the list by one digit each time,\nuntil the person makes an error. The length of the longest list for\nwhich the person responds correctly is the score and represents his or\nher working memory capacity. Or imagine a clinical psychologist who is\ninterested in how depressed a person is. He administers the Beck\nDepression Inventory, which is a 21-item self-report questionnaire in\nwhich the person rates the extent to which he or she has felt sad, lost\nenergy, and experienced other symptoms of depression over the past 2\nweeks. The sum of these 21 ratings is the score and represents his or\nher current level of depression.\n\nThe important point here is that measurement does not require any\nparticular instruments or procedures. It does not require placing\nindividuals or objects on bathroom scales, holding rulers up to them, or\ninserting thermometers into them. What it *does* require is *some*\nsystematic procedure for assigning scores to individuals or objects so\nthat those scores represent the characteristic of interest.\n\n### Psychological Constructs {.unnumbered}\n\nMany variables studied by psychologists are straightforward and simple\nto measure. These include sex, age, height, weight, and birth order. You\ncan almost always tell whether someone is male or female just by\nlooking. You can ask people how old they are and be reasonably sure that\nthey know and will tell you. Although people might not know or want to\ntell you how much they weigh, you can have them step onto a bathroom\nscale. Other variables studied by psychologists---perhaps the\nmajority---are not so straightforward or simple to measure. We cannot\naccurately assess people's level of intelligence by looking at them, and\nwe certainly cannot put their self-esteem on a bathroom scale. These\nkinds of variables are called [constructs](#construct) (pronounced\nCON-structs) and include personality traits (e.g., extroversion),\nemotional states (e.g., fear), attitudes (e.g., toward taxes), and\nabilities (e.g., athleticism).\n\nPsychological constructs cannot be observed directly. One reason is that\nthey often represent tendencies to think, feel, or act in certain ways.\nFor example, to say that a particular college student is highly\nextroverted (see FYI box \"The Big Five\") does not necessarily mean that\nshe is behaving in an extroverted way right now. In fact, she might be\nsitting quietly by herself, reading a book. Instead, it means that she\nhas a general tendency to behave in extroverted ways (talking, laughing,\netc.) across a variety of situations. Another reason psychological\nconstructs cannot be observed directly is that they often involve\ninternal processes. Fear, for example, involves the activation of\ncertain central and peripheral nervous system structures, along with\ncertain kinds of thoughts, feelings, and behaviors---none of which is\nnecessarily obvious to an outside observer. Notice also that neither\nextroversion nor fear \"reduces to\" any particular thought, feeling, act,\nor physiological structure or process. Instead, each is a kind of\nsummary of a complex set of behaviors and internal processes.\n\n::: fyi\n##### The Big Five {.unnumbered}\n\nThe Big Five is a set of five broad dimensions that capture much of the\nvariation in human personality. Each of the Big Five can even be defined\nin terms of six more specific constructs called \"facets\"\n[@costa1992normal].\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The Big Five Personality Dimensions](images/measurement/big5.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\nThe [conceptual definition] of a psychological construct describes the\nbehaviors and internal processes that make up that construct, along with\nhow it relates to other variables. For example, a conceptual definition\nof neuroticism (another one of the Big Five) would be that it is\npeople's tendency to experience negative emotions such as anxiety,\nanger, and sadness across a variety of situations. This definition might\nalso include that it has a strong genetic component, remains fairly\nstable over time, and is positively correlated with the tendency to\nexperience pain and other physical symptoms.\n\nStudents sometimes wonder why, when researchers want to understand a\nconstruct like self-esteem or neuroticism, they do not simply look it up\nin the dictionary. One reason is that many scientific constructs do not\nhave counterparts in everyday language (e.g., working memory capacity).\nMore important, researchers are in the business of developing\ndefinitions that are more detailed and precise---and that more\naccurately describe the way the world is---than the informal definitions\nin the dictionary. As we will see, they do this by proposing conceptual\ndefinitions, testing them empirically, and revising them as necessary.\nSometimes they throw them out altogether. This is why the research\nliterature often includes different conceptual definitions of the same\nconstruct. In some cases, an older conceptual definition has been\nreplaced by a newer one that works better. In others, researchers are\nstill in the process of deciding which of various conceptual definitions\nis the best.\n\n### Operational Definitions {.unnumbered}\n\nAn [operational definition] is a definition of a variable in terms of\nprecisely how it is to be measured. These measures generally fall into\none of three broad categories. [Self-report\nmeasures](#self-report-measure) are those in which participants report\non their own thoughts, feelings, and actions, as with the Rosenberg\nSelf-Esteem Scale. [Behavioral measures](#behavioral-measure) are those\nin which some other aspect of participants' behavior is observed and\nrecorded. This is an extremely broad category that includes the\nobservation of people's behavior both in highly structured laboratory\ntasks and in more natural settings. A good example of the former would\nbe measuring working memory capacity using the backward digit span task.\nA good example of the latter is a famous operational definition of\nphysical aggression from researcher Albert Bandura and his colleagues\n[@bandura1961transmission]. They let each of several children play for\n20 minutes in a room that contained a clown-shaped punching bag called a\nBobo doll. They filmed each child and counted the number of acts of\nphysical aggression he or she committed. These included hitting the doll\nwith a mallet, punching it, and kicking it. Their operational\ndefinition, then, was the number of these specifically defined acts that\nthe child committed in the 20-minute period. Finally, [physiological\nmeasures](#physiological-measure) are those that involve recording any\nof a wide variety of physiological processes, including heart rate and\nblood pressure, galvanic skin response, hormone levels, and electrical\nactivity and blood flow in the brain.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![In addition to self-report and behavioral measures, researchers in psychology use physiological measures. An electroencephalograph (EEG) records electrical activity from the brain. *Image by ulrichw from Pixabay.*](images/measurement/eeg.jpeg){fig-align='center' width=50%}\n:::\n:::\n\n\nFor any given variable or construct, there will be multiple operational\ndefinitions. Stress is a good example. A rough conceptual definition is\nthat stress is an adaptive response to a perceived danger or threat that\ninvolves physiological, cognitive, affective, and behavioral components.\nBut researchers have operationally defined it in several ways. The\nSocial Readjustment Rating Scale is a self-report questionnaire on which\npeople identify stressful events that they have experienced in the past\nyear and assigns points for each one depending on its severity. For\nexample, a man who has been divorced (73 points), changed jobs (36\npoints), and had a change in sleeping habits (16 points) in the past\nyear would have a total score of 125. The Daily Hassles and Uplifts\nScale is similar but focuses on everyday stressors like misplacing\nthings and being concerned about one's weight. The Perceived Stress\nScale is another self-report measure that focuses on people's feelings\nof stress (e.g., \"How often have you felt nervous and stressed?\").\nResearchers have also operationally defined stress in terms of several\nphysiological variables including blood pressure and levels of the\nstress hormone cortisol.\n\nWhen psychologists use multiple operational definitions of the same\nconstruct---either within a study or across studies---they are using\n[converging operations]. The idea is that the various operational\ndefinitions are \"converging\" on the same construct. When scores based on\nseveral different operational definitions are closely related to each\nother and produce similar patterns of results, this constitutes good\nevidence that the construct is being measured effectively and that it is\nuseful. The various measures of stress, for example, are all correlated\nwith each other and have all been shown to be correlated with other\nvariables such as immune system functioning (also measured in a variety\nof ways) [@segerstrom2004psychological]. This is what allows researchers\neventually to draw useful general conclusions, such as \"stress is\nnegatively correlated with immune system functioning,\" as opposed to\nmore specific and less useful ones, such as \"people's scores on the\nPerceived Stress Scale are negatively correlated with their white blood\ncounts.\"\n\n## Levels of Measurement\n\nThe psychologist S. S. Stevens suggested that scores can be assigned to\nindividuals so that they communicate more or less quantitative\ninformation about the variable of interest [@stevens1946theory]. For\nexample, the officials at a 100-m race could simply rank order the\nrunners as they crossed the finish line (first, second, etc.), or they\ncould time each runner to the nearest tenth of a second using a\nstopwatch (11.5 s, 12.1 s, etc.). In either case, they would be\nmeasuring the runners' times by systematically assigning scores to\nrepresent those times. But while the rank ordering procedure\ncommunicates the fact that the second-place runner took longer to finish\nthan the first-place finisher, the stopwatch procedure also communicates\nhow much longer the second-place finisher took. Stevens actually\nsuggested four different [levels of measurement] (which he called\n\"scales of measurement\") that correspond to four different levels of\nquantitative information that can be communicated by a set of scores.\n\nThe [nominal level] of measurement is used for categorical variables and\ninvolves assigning scores that are category labels. Category labels\ncommunicate whether any two individuals are the same or different in\nterms of the variable being measured. For example, if you look at your\nresearch participants as they enter the room, decide whether each one is\nmale or female, and type this information into a spreadsheet, you are\nengaged in nominal-level measurement. Or if you ask your participants to\nindicate which of several ethnicities they identify themselves with, you\nare again engaged in nominal-level measurement.\n\nThe remaining three levels of measurement are used for quantitative\nvariables. The [ordinal level] of measurement involves assigning scores\nso that they represent the rank order of the individuals. Ranks\ncommunicate not only whether any two individuals are the same or\ndifferent in terms of the variable being measured but also whether one\nindividual is higher or lower on that variable. The [interval level] of\nmeasurement involves assigning scores so that they represent the precise\nmagnitude of the difference between individuals, but a score of zero\ndoes not actually represent the complete absence of the characteristic.\nA classic example is the measurement of heat using the Celsius or\nFahrenheit scale. The difference between temperatures of 20°C and 25°C\nis precisely 5°, but a temperature of 0°C does not mean that there is a\ncomplete absence of heat. In psychology, the intelligence quotient (IQ)\nis often considered to be measured at the interval level. Finally, the\n[ratio level] of measurement involves assigning scores in such a way\nthat there is a true zero point that represents the complete absence of\nthe quantity. Height measured in meters and weight measured in kilograms\nare good examples. So are counts of discrete objects or events such as\nthe number of siblings one has or the number of questions a student\nanswers correctly on an exam.\n\nStevens's levels of measurement are important for at least two reasons.\nFirst, they emphasize the generality of the concept of measurement.\nAlthough people do not normally think of categorizing or ranking\nindividuals as measurement, in fact they are as long as they are done so\nthat they represent some characteristic of the individuals. Second, the\nlevels of measurement can serve as a rough guide to the statistical\nprocedures that can be used with the data and the conclusions that can\nbe drawn from them. With nominal-level measurement, for example, the\nonly available measure of central tendency is the mode. Also,\nratio-level measurement is the only level that allows meaningful\nstatements about ratios of scores. One cannot say that someone with an\nIQ of 140 is twice as intelligent as someone with an IQ of 70 because IQ\nis measured at the interval level, but one can say that someone with six\nsiblings has twice as many as someone with three because number of\nsiblings is measured at the ratio level.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Measurement is the assignment of scores to individuals so that the\n    scores represent some characteristic of the individuals.\n    Psychological measurement can be achieved in a wide variety of ways,\n    including self-report, behavioral, and physiological measures.\n-   Psychological constructs such as intelligence, self-esteem, and\n    depression are variables that are not directly observable because\n    they represent behavioral tendencies or complex patterns of behavior\n    and internal processes. An important goal of scientific research is\n    to conceptually define psychological constructs in ways that\n    accurately describe them.\n-   For any conceptual definition of a construct, there will be many\n    different operational definitions or ways of measuring it. The use\n    of multiple operational definitions, or converging operations, is a\n    common strategy in psychological research.\n-   Variables can be measured at four different levels---nominal,\n    ordinal, interval, and ratio---that communicate increasing amounts\n    of quantitative information. The level of measurement affects the\n    kinds of statistics you can use and conclusions you can draw from\n    your data.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: Complete the Rosenberg Self-Esteem Scale and compute your\n    overall score.\n2.  Practice: Think of three operational definitions for sexual\n    jealousy, decisiveness, and social anxiety. Consider the possibility\n    of self-report, behavioral, and physiological measures. Be as\n    precise as you can.\n3.  Practice: For each of the following variables, decide which level of\n    measurement is being used.\n    -   A college instructor measures the time it takes his students to\n        finish an exam by looking through the stack of exams at the end.\n        He assigns the one on the bottom a score of 1, the one on top of\n        that a 2, and so on.\n    -   A researcher accesses her participants' medical records and\n        counts the number of times they have seen a doctor in the past\n        year.\n    -   Participants in a research study are asked whether they are\n        right-handed or left-handed.\n:::\n\n## Reliability and Validity of Measurement\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Define reliability, including the different types and how they are\n    assessed.\n2.  Define validity, including the different types and how they are\n    assessed.\n3.  Describe the kinds of evidence that would be relevant to assessing\n    the reliability and validity of a particular measure.\n:::\n\nAgain, measurement involves assigning scores to individuals so that they\nrepresent some characteristic of the individuals. But how do researchers\nknow that the scores actually represent the characteristic, especially\nwhen it is a construct like intelligence, self-esteem, depression, or\nworking memory capacity? The answer is that they conduct research using\nthe measure to confirm that the scores make sense based on their\nunderstanding of the construct being measured. This is an extremely\nimportant point. Psychologists do not simply *assume* that their\nmeasures work. Instead, they collect data to *demonstrate* that they\nwork. If their research does not demonstrate that a measure works, they\nstop using it.\n\nAs an informal example, imagine that you have been dieting for a month.\nYour clothes seem to be fitting more loosely, and several friends have\nasked if you have lost weight. If at this point your bathroom scale\nindicated that you had lost 10 pounds, this would make sense and you\nwould continue to use the scale. But if it indicated that you had gained\n10 pounds, you would rightly conclude that it was broken and either fix\nit or get rid of it. In evaluating a measurement method, psychologists\nconsider two general dimensions: reliability and validity.\n\n### Reliability {.unnumbered}\n\n[Reliability] refers to the consistency of a measure. Psychologists\nconsider three types of consistency: over time (test-retest\nreliability), across items (internal consistency), and across different\nresearchers (interrater reliability).\n\n#### Test-Retest Reliability {.unnumbered}\n\nWhen researchers measure a construct that they assume to be consistent\nacross time, then the scores they obtain should also be consistent\nacross time. [Test-retest reliability] is the extent to which this is\nactually the case. For example, intelligence is generally thought to be\nconsistent across time. A person who is highly intelligent today will be\nhighly intelligent next week. This means that any good measure of\nintelligence should produce roughly the same scores for this individual\nnext week as it does today. Clearly, a measure that produces highly\ninconsistent scores over time cannot be a very good measure of a\nconstruct that is supposed to be consistent.\n\nAssessing test-retest reliability requires using the measure on a group\nof people at one time, using it again on the same group of people at a\nlater time, and then looking at [test-retest correlation] between the\ntwo sets of scores. This is typically done by graphing the data in a\nscatterplot and computing Pearson's r. Figure \\@ref(fig:retest) shows\nthe correlation between two sets of scores of several college students\non the Rosenberg Self-Esteem Scale, given two times a week apart.\nPearson's r for these data is +.95. In general, a test-retest\ncorrelation of +.80 or greater is considered to indicate good\nreliability.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Test-retest correlation between two sets of scores of several college students on the Rosenberg self-esteem scale, given two times a week apart.](06-measurement_files/figure-html/retest-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nAgain, high test-retest correlations make sense when the construct being\nmeasured is assumed to be consistent over time, which is the case for\nintelligence, self-esteem, and the Big Five personality dimensions. But\nother constructs are not assumed to be stable over time. The very nature\nof mood, for example, is that it changes. So a measure of mood that\nproduced a low test-retest correlation over a period of a month would\nnot be a cause for concern.\n\n#### Internal Consistency {.unnumbered}\n\nA second kind of reliability is [internal consistency], which is the\nconsistency of people's responses across the items on a multiple-item\nmeasure. In general, all the items on such measures are supposed to\nreflect the same underlying construct, so people's scores on those items\nshould be correlated with each other. On the Rosenberg Self-Esteem\nScale, people who agree that they are a person of worth should tend to\nagree that that they have a number of good qualities. If people's\nresponses to the different items are not correlated with each other,\nthen it would no longer make sense to claim that they are all measuring\nthe same underlying construct. This is as true for behavioral and\nphysiological measures as for self-report measures. For example, people\nmight make a series of bets in a simulated game of roulette as a measure\nof their level of risk seeking. This measure would be internally\nconsistent to the extent that individual participants' bets were\nconsistently high or low across trials.\n\nLike test-retest reliability, internal consistency can only be assessed\nby collecting and analyzing data. One approach is to look at a\n[split-half correlation]. This involves splitting the items into two\nsets, such as the first and second halves of the items or the even- and\nodd-numbered items. Then a score is computed for each set of items, and\nthe relationship between the two sets of scores is examined. For\nexample, Figure \\@ref(fig:internal) shows the split-half correlation\nbetween several college students' scores on the even-numbered items and\ntheir scores on the odd-numbered items of the Rosenberg Self-Esteem\nScale. Pearson's r for these data is +.88. A split-half correlation of\n+.80 or greater is generally considered good internal consistency.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Split-half correlation between several college students’ scores on the even-numbered items and their scores on the odd-numbered items of the Rosenberg self-esteem scale.](06-measurement_files/figure-html/internal-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\nPerhaps the most common measure of internal consistency used by\nresearchers in psychology is a statistic called [Cronbach's\nα](#cronbachs-α) (the Greek letter alpha). Conceptually, α is the mean\nof all possible split-half correlations for a set of items. For example,\nthere are 252 ways to split a set of 10 items into two sets of five.\nCronbach's α would be the mean of the 252 split-half correlations. Note\nthat this is not how α is actually computed, but it is a correct way of\ninterpreting the meaning of this statistic. Again, a value of +.80 or\ngreater is generally taken to indicate good internal consistency.\n\n#### Interrater Reliability {.unnumbered}\n\nMany behavioral measures involve significant judgment on the part of an\nobserver or a rater. [Interrater reliability] is the extent to which\ndifferent observers are consistent in their judgments. For example, if\nyou were interested in measuring college students' social skills, you\ncould make video recordings of them as they interacted with another\nstudent whom they are meeting for the first time. Then you could have\ntwo or more observers watch the videos and rate each student's level of\nsocial skills. To the extent that each participant does in fact have\nsome level of social skills that can be detected by an attentive\nobserver, different observers' ratings should be highly correlated with\neach other. If they were not, then those ratings could not be an\naccurate representation of participants' social skills. Interrater\nreliability is often assessed using Cronbach's α when the judgments are\nquantitative or an analogous statistic called [Cohen's κ](#cohens-κ)\n(the Greek letter kappa) when they are categorical.\n\n### Validity {.unnumbered}\n\n[Validity] is the extent to which the scores from a measure represent\nthe variable they are intended to. But how do researchers make this\njudgment? We have already considered one factor that they take into\naccount---reliability. When a measure has good test-retest reliability\nand internal consistency, researchers should be more confident that the\nscores represent what they are supposed to. There has to be more to it,\nhowever, because a measure can be extremely reliable but have no\nvalidity whatsoever. As an absurd example, imagine someone who believes\nthat people's index finger length reflects their self-esteem and\ntherefore tries to measure self-esteem by holding a ruler up to people's\nindex fingers. Although this measure would have extremely good\ntest-retest reliability, it would have absolutely no validity. The fact\nthat one person's index finger is a centimeter longer than another's\nwould indicate nothing about which one had higher self-esteem.\n\nTextbook presentations of validity usually divide it into several\ndistinct \"types.\" But a good way to interpret these types is that they\nare other kinds of evidence---in addition to reliability---that should\nbe taken into account when judging the validity of a measure. Here we\nconsider four basic kinds: face validity, content validity, criterion\nvalidity, and discriminant validity.\n\n#### Face Validity {.unnumbered}\n\n[Face validity] is the extent to which a measurement method appears \"on\nits face\" to measure the construct of interest. Most people would expect\na self-esteem questionnaire to include items about whether they see\nthemselves as a person of worth and whether they think they have good\nqualities. So a questionnaire that included these kinds of items would\nhave good face validity. The finger-length method of measuring\nself-esteem, on the other hand, seems to have nothing to do with\nself-esteem and therefore has poor face validity. Although face validity\ncan be assessed quantitatively---for example, by having a large sample\nof people rate a measure in terms of whether it appears to measure what\nit is intended to---it is usually assessed informally.\n\nFace validity is at best a very weak kind of evidence that a measurement\nmethod is measuring what it is supposed to. One reason is that it is\nbased on people's intuitions about human behavior, which are frequently\nwrong. It is also the case that many established measures in psychology\nwork quite well despite lacking face validity. The Minnesota Multiphasic\nPersonality Inventory (MMPI) measures many personality characteristics\nand disorders by having people decide whether each of over 567 different\nstatements applies to them---where many of the statements do not have\nany obvious relationship to the construct that they measure. Another\nexample is the Implicit Association Test, which measures prejudice in a\nway that is nonintuitive to most people.\n\n::: fyi\n##### How to Measure Implicit Prejudice? {.unnumbered}\n\nThe Implicit Association Test (IAT) is used to measure people's\nattitudes toward various social groups. The IAT is a behavioral measure\ndesigned to reveal negative attitudes that people might not admit to on\na self-report measure. It focuses on how quickly people are able to\ncategorize words and images representing two contrasting groups (e.g.,\ngay and straight) along with other positive and negative stimuli (e.g.,\nthe words \"wonderful\" or \"nasty\"). The IAT has been used in dozens of\npublished research studies, and there is strong evidence for both its\nreliability and its validity [@nosek2007implicit]. You can learn more\nabout the IAT---and take many of them for yourself---at the following\nwebsite: https://implicit.harvard.edu/implicit.\n:::\n\n#### Content Validity {.unnumbered}\n\n[Content validity] is the extent to which a measure \"covers\" the\nconstruct of interest. For example, if a researcher conceptually defines\ntest anxiety as involving both sympathetic nervous system activation\n(leading to nervous feelings) and negative thoughts, then his measure of\ntest anxiety should include items about both nervous feelings and\nnegative thoughts. Or consider that attitudes are usually defined as\ninvolving thoughts, feelings, and actions toward something. By this\nconceptual definition, a person has a positive attitude toward exercise\nto the extent that he or she thinks positive thoughts about exercising,\nfeels good about exercising, and actually exercises. So to have good\ncontent validity, a measure of people's attitudes toward exercise would\nhave to reflect all three of these aspects. Like face validity, content\nvalidity is not usually assessed quantitatively. Instead, it is assessed\nby carefully checking the measurement method against the conceptual\ndefinition of the construct.\n\n#### Criterion Validity {.unnumbered}\n\n[Criterion validity] is the extent to which people's scores on a measure\nare correlated with other variables (known as [criteria]) that one would\nexpect them to be correlated with. For example, people's scores on a new\nmeasure of test anxiety should be negatively correlated with their\nperformance on an important school exam. If it were found that people's\nscores were in fact negatively correlated with their exam performance,\nthen this would be a piece of evidence that these scores really\nrepresent people's test anxiety. But if it were found that people scored\nequally well on the exam regardless of their test anxiety scores, then\nthis would cast doubt on the validity of the measure.\n\nA criterion can be any variable that one has reason to think should be\ncorrelated with the construct being measured, and there will usually be\nmany of them. For example, one would expect test anxiety scores to be\nnegatively correlated with exam performance and course grades and\npositively correlated with general anxiety and with blood pressure\nduring an exam. Or imagine that a researcher develops a new measure of\nphysical risk taking. People's scores on this measure should be\ncorrelated with their participation in \"extreme\" activities such as\nsnowboarding and rock climbing, the number of speeding tickets they have\nreceived, and even the number of broken bones they have had over the\nyears. Criteria can also include other measures of the same construct.\nFor example, one would expect new measures of test anxiety or physical\nrisk taking to be positively correlated with existing measures of the\nsame constructs. So the use of converging operations is one way to\nexamine criterion validity.\n\nAssessing criterion validity requires collecting data using the measure.\nResearchers John Cacioppo and Richard Petty did this when they created\ntheir self-report Need for Cognition Scale to measure how much people\nvalue and engage in thinking [@cacioppo1982need]. In a series of\nstudies, they showed that college faculty scored higher than\nassembly-line workers, that people's scores were positively correlated\nwith their scores on a standardized academic achievement test, and that\ntheir scores were negatively correlated with their scores on a measure\nof dogmatism (which represents a tendency toward obedience). In the\nyears since it was created, the Need for Cognition Scale has been used\nin literally hundreds of studies and has been shown to be correlated\nwith a wide variety of other variables, including the effectiveness of\nan advertisement, interest in politics, and juror decisions\n[@petty2009need].\n\n#### Discriminant Validity {.unnumbered}\n\n[Discriminant validity] is the extent to which scores on a measure are\nnot correlated with measures of variables that are conceptually\ndistinct. For example, self-esteem is a general attitude toward the self\nthat is fairly stable over time. It is not the same as mood, which is\nhow good or bad one happens to be feeling right now. So people's scores\non a new measure of self-esteem should not be very highly correlated\nwith their moods. If the new measure of self-esteem were highly\ncorrelated with a measure of mood, it could be argued that the new\nmeasure is not really measuring self-esteem; it is measuring mood\ninstead.\n\nWhen they created the Need for Cognition Scale, Cacioppo and Petty also\nprovided evidence of discriminant validity by showing that people's\nscores were not correlated with certain other variables. For example,\nthey found only a weak correlation between people's need for cognition\nand a measure of their cognitive style---the extent to which they tend\nto think analytically by breaking ideas into smaller parts or\nholistically in terms of \"the big picture.\" They also found no\ncorrelation between people's need for cognition and measures of their\ntest anxiety and their tendency to respond in socially desirable ways.\nAll these low correlations provide evidence that the measure is\nreflecting a conceptually distinct construct.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Psychological researchers do not simply assume that their measures\n    work. Instead, they conduct research to show that they work. If they\n    cannot show that they work, they stop using them.\n-   There are two distinct criteria by which researchers evaluate their\n    measures: reliability and validity. Reliability is consistency\n    across time (test-retest reliability), across items (internal\n    consistency), and across researchers (interrater reliability).\n    Validity is the extent to which the scores actually represent the\n    variable they are intended to.\n-   Validity is a judgment based on various types of evidence. The\n    relevant evidence includes the measure's reliability, whether it\n    covers the construct of interest, and whether the scores it produces\n    are correlated with other variables they are expected to be\n    correlated with and not correlated with variables that are\n    conceptually distinct.\n-   The reliability and validity of a measure is not established by any\n    single study but by the pattern of results across multiple studies.\n    The assessment of reliability and validity is an ongoing process.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: Ask several friends to complete the Rosenberg Self-Esteem\n    Scale. Then assess its internal consistency by making a scatterplot\n    to show the split-half correlation (even- vs. odd-numbered items).\n    Compute Pearson's r too if you know how.\n2.  Discussion: Think back to the last college exam you took and think\n    of the exam as a psychological measure. What construct do you think\n    it was intended to measure? Comment on its face and content\n    validity. What data could you collect to assess its reliability,\n    criterion validity, and discriminant validity?\n3.  Practice: Take an Implicit Association Test and then list as many\n    ways to assess its criterion validity as you can think of.\n:::\n\n## Practical Strategies for Psychological Measurement\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Specify the four broad steps in the measurement process.\n2.  Explain how you would decide whether to use an existing measure or\n    create your own.\n3.  Describe multiple strategies to identify and locate existing\n    measures of psychological constructs.\n4.  Describe several general principles for creating new measures and\n    for implementing existing and new measures.\n5.  Create a simple plan for assessing the reliability and validity of\n    an existing or new measure.\n:::\n\nSo far in this chapter, we have considered several basic ideas about the\nnature of psychological constructs and their measurement. But now\nimagine that you are in the position of actually having to measure a\npsychological construct for a research project. How should you proceed?\nBroadly speaking, there are four steps in the measurement process: (a)\nconceptually defining the construct, (b) operationally defining the\nconstruct, (c) implementing the measure, and (d) evaluating the measure.\nIn this section, we will look at each of these steps in turn.\n\n### Conceptually Defining the Construct {.unnumbered}\n\nHaving a clear and complete conceptual definition of a construct is a\nprerequisite for good measurement. For one thing, it allows you to make\nsound decisions about exactly how to measure the construct. If you had\nonly a vague idea that you wanted to measure people's \"memory,\" for\nexample, you would have no way to choose whether you should have them\nremember a list of vocabulary words, a set of photographs, a newly\nlearned skill, or an experience from long ago. Because psychologists now\nconceptualize memory as a set of semi-independent systems, you would\nhave to be more precise about what you mean by \"memory.\" If you are\ninterested in long-term declarative memory (memory for facts), then\nhaving participants remember a list of words that they learned last week\nwould make sense, but having them remember and execute a newly learned\nskill would not. In general, there is no substitute for reading the\nresearch literature on a construct and paying close attention to how\nothers have defined it.\n\n### Deciding on an Operational Definition {.unnumbered}\n\n#### Using an Existing Measure {.unnumbered}\n\nIt is usually a good idea to use an existing measure that has been used\nsuccessfully in previous research. Among the advantages are that (a) you\nsave the time and trouble of creating your own, (b) there is already\nsome evidence that the measure is valid (if it has been used\nsuccessfully), and (c) your results can more easily be compared with and\ncombined with previous results. In fact, if there already exists a\nreliable and valid measure of a construct, other researchers might\nexpect you to use it unless you have a good and clearly stated reason\nfor not doing so.\n\nIf you choose to use an existing measure, you may still have to choose\namong several alternatives. You might choose the most common one, the\none with the best evidence of reliability and validity, the one that\nbest measures a particular aspect of a construct that you are interested\nin (e.g., a physiological measure of stress if you are most interested\nin its underlying physiology), or even the one that would be easiest to\nuse. For example, the Ten-Item Personality Inventory (TIPI) is a\nself-report questionnaire that measures all the Big Five personality\ndimensions with just 10 items [@gosling2003very]. It is not as reliable\nor valid as longer and more comprehensive measures, but a researcher\nmight choose to use it when testing time is severely limited.\n\nWhen an existing measure was created primarily for use in scientific\nresearch, it is usually described in detail in a published research\narticle and is free to use in your own research---with a proper\ncitation. You might find that later researchers who use the same measure\ndescribe it only briefly but provide a reference to the original\narticle, in which case you would have to get the details from the\noriginal article. The American Psychological Association also publishes\nthe *Directory of Unpublished Experimental Measures*, which is an\nextensive catalog of measures that have been used in previous research.\nMany existing measures---especially those that have applications in\nclinical psychology---are proprietary. This means that a publisher owns\nthe rights to them and that you would have to purchase them. These\ninclude many standard intelligence tests, the Beck Depression Inventory,\nand the Minnesota Multiphasic Personality Inventory (MMPI). Details\nabout many of these measures and how to obtain them can be found in\nother reference books, including *Tests in Print* and the *Mental\nMeasurements Yearbook*. There is a good chance you can find these\nreference books in your college or university library.\n\n#### Creating Your Own Measure {.unnumbered}\n\nInstead of using an existing measure, you might want to create your own.\nPerhaps there is no existing measure of the construct you are interested\nin or existing ones are too difficult or time-consuming to use. Or\nperhaps you want to use a new measure specifically to see whether it\nworks in the same way as existing measures---that is, to demonstrate\nconverging operations. In this section, we consider some general issues\nin creating new measures that apply equally to self-report, behavioral,\nand physiological measures. More detailed guidelines for creating\nself-report measures are presented in the chapter on survey research.\n\nFirst, be aware that most new measures in psychology are really\nvariations of existing measures, so you should still look to the\nresearch literature for ideas. Perhaps you can modify an existing\nquestionnaire, create a paper-and-pencil version of a measure that is\nnormally computerized (or vice versa), or adapt a measure that has\ntraditionally been used for another purpose. For example, the famous\nStroop task [@stroop1935studies]---in which people quickly name the\ncolors that various color words are printed in---has been adapted for\nthe study of social anxiety. Socially anxious people are slower at color\nnaming when the words have negative social connotations such as \"stupid\"\n[@amir2002enhanced].\n\nWhen you create a new measure, you should strive for simplicity.\nRemember that your participants are not as interested in your research\nas you are and that they will vary widely in their ability to understand\nand carry out whatever task you give them. You should create a set of\nclear instructions using simple language that you can present in writing\nor read aloud (or both). It is also a good idea to include one or more\npractice items so that participants can become familiar with the task,\nand to build in an opportunity for them to ask questions before\ncontinuing. It is also best to keep the measure brief to avoid boring or\nfrustrating your participants to the point that their responses start to\nbecome less reliable and valid.\n\nThe need for brevity, however, needs to be weighed against the fact that\nit is nearly always better for a measure to include multiple items\nrather than a single item. There are two reasons for this. One is a\nmatter of content validity. Multiple items are often required to cover a\nconstruct adequately. The other is a matter of reliability. People's\nresponses to single items can be influenced by all sorts of irrelevant\nfactors---misunderstanding the particular item, a momentary distraction,\nor a simple error such as checking the wrong response option. But when\nseveral responses are summed or averaged, the effects of these\nirrelevant factors tend to cancel each other out to produce more\nreliable scores. Remember, however, that multiple items must be\nstructured in a way that allows them to be combined into a single\noverall score by summing or averaging. To measure \"financial\nresponsibility,\" a student might ask people about their annual income,\nobtain their credit score, and have them rate how \"thrifty\" they\nare---but there is no obvious way to combine these responses into an\noverall score. To create a true multiple-item measure, the student might\ninstead ask people to rate the degree to which 10 statements about\nfinancial responsibility describe them on the same five-point scale.\n\nFinally, the very best way to assure yourself that your measure has\nclear instructions, includes sufficient practice, and is an appropriate\nlength is to test several people. (Family and friends often serve this\npurpose nicely). Observe them as they complete the task, time them, and\nask them afterward to comment on how easy or difficult it was, whether\nthe instructions were clear, and anything else you might be wondering\nabout. Obviously, it is better to discover problems with a measure\nbefore beginning any large-scale data collection.\n\n#### Implementing the Measure {.unnumbered}\n\nYou will want to implement any measure in a way that maximizes its\nreliability and validity. In most cases, it is best to test everyone\nunder similar conditions that, ideally, are quiet and free of\ndistractions. Testing participants in groups is often done because it is\nefficient, but be aware that it can create distractions that reduce the\nreliability and validity of the measure. As always, it is good to use\nprevious research as a guide. If others have successfully tested people\nin groups using a particular measure, then you should consider doing it\ntoo.\n\nBe aware also that people can react in a variety of ways to being\nmeasured that reduce the reliability and validity of the scores.\nAlthough some disagreeable participants might intentionally respond in\nways meant to \"mess up\" a study, participant [reactivity] is more likely\nto take the opposite form. Agreeable participants might respond in ways\nthey believe they are expected to. They might engage in [socially\ndesirable responding]. For example, people with low self-esteem agree\nthat they feel they are a person of worth not because they really feel\nthis way but because they believe this is the socially appropriate\nresponse and do not want to look bad in the eyes of the researcher.\nAdditionally, research studies can have built-in [demand\ncharacteristics]: cues to how the researcher expects participants to\nbehave. For example, a participant whose attitude toward exercise is\nmeasured immediately after she is asked to read a passage about the\ndangers of heart disease might reasonably conclude that the passage was\nmeant to improve her attitude. As a result, she might respond more\nfavorably because she believes she is expected to by the researcher.\nFinally, your own expectations can bias participants' behaviors in\nunintended ways.\n\nThere are several precautions you can take to minimize these kinds of\nreactivity. One is to make the procedure as clear and brief as possible\nso that participants are not tempted to take out their frustrations on\nyour results. Another is to guarantee participants' anonymity and make\nclear to them that you are doing so. If you are testing them in groups,\nbe sure that they are seated far enough apart that they cannot see each\nother's responses. Give them all the same type of writing implement so\nthat they cannot be identified by, for example, the pink glitter pen\nthat they used. You can even allow them to seal completed questionnaires\ninto individual envelopes or put them into a drop box where they\nimmediately become mixed with others' questionnaires. Although informed\nconsent requires telling participants what they will be doing, it does\nnot require revealing your hypothesis or other information that might\nsuggest to participants how you expect them to respond. A questionnaire\ndesigned to measure financial responsibility need not be titled \"Are You\nFinancially Responsible?\" It could be titled \"Money Questionnaire\" or\nhave no title at all. Finally, the effects of your expectations can be\nminimized by arranging to have the measure administered by a helper who\nis unaware of its intent or of any hypothesis being tested. Regardless\nof whether this is possible, you should standardize all interactions\nbetween researchers and participants---for example, by always reading\nthe same set of instructions word for word.\n\n#### Evaluating the Measure {.unnumbered}\n\nOnce you have used your measure on a sample of people and have a set of\nscores, you are in a position to evaluate it more thoroughly in terms of\nreliability and validity. Even if the measure has been used extensively\nby other researchers and has already shown evidence of reliability and\nvalidity, you should not assume that it worked as expected for your\nparticular sample and under your particular testing conditions.\nRegardless, you now have additional evidence bearing on the reliability\nand validity of the measure, and it would make sense to add that\nevidence to the research literature.\n\nIn most research designs, it is not possible to assess test-retest\nreliability because participants are tested at only one time. For a new\nmeasure, you might design a study specifically to assess its test-retest\nreliability by testing the same set of participants at two times. In\nother cases, a study designed to answer a different question still\nallows for the assessment of test-retest reliability. For example, a\npsychology instructor might measure his students' attitude toward\ncritical thinking using the same measure at the beginning and end of the\nsemester to see if there is any change. Even if there is no change, he\ncould still look at the correlation between students' scores at the two\ntimes to assess the measure's test-retest reliability. It is also\ncustomary to assess internal consistency for any multiple-item\nmeasure---usually by looking at a split-half correlation or Cronbach's\nalpha.\n\nCriterion and discriminant validity can be assessed in various ways. For\nexample, if your study included more than one measure of the same\nconstruct or measures of conceptually distinct constructs, then you\nshould look at the correlations among these measures to be sure that\nthey fit your expectations. Note also that a successful experimental\nmanipulation also provides evidence of criterion validity. Recall that\nMacDonald and Martineau manipulated participant's moods by having them\nthink either positive or negative thoughts, and after the manipulation\ntheir mood measure showed a distinct difference between the two groups.\nThis simultaneously provided evidence that their mood manipulation\nworked *and* that their mood measure was valid.\n\nBut what if your newly collected data cast doubt on the reliability or\nvalidity of your measure? The short answer is that you have to ask why.\nIt could be that there is something wrong with your measure or how you\nadministered it. It could be that there is something wrong with your\nconceptual definition. It could be that your experimental manipulation\nfailed. For example, if a mood measure showed no difference between\npeople whom you instructed to think positive versus negative thoughts,\nmaybe it is because the participants did not actually think the thoughts\nthey were supposed to or that the thoughts did not actually affect their\nmoods. In short, it is \"back to the drawing board\" to revise the\nmeasure, revise the conceptual definition, or try a new manipulation.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Good measurement begins with a clear conceptual definition of the\n    construct to be measured. This is accomplished both by clear and\n    detailed thinking and by a review of the research literature.\n-   You often have the option of using an existing measure or creating a\n    new measure. You should make this decision based on the availability\n    of existing measures and their adequacy for your purposes.\n-   Several simple steps can be taken in creating new measures and in\n    implementing both existing and new measures that can help maximize\n    reliability and validity.\n-   Once you have used a measure, you should reevaluate its reliability\n    and validity based on your new data. Remember that the assessment of\n    reliability and validity is an ongoing process.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: Write your own conceptual definition of self-confidence,\n    irritability, and athleticism.\n2.  Practice: Choose a construct (sexual jealousy, self-confidence,\n    etc.) and find two measures of that construct in the research\n    literature. If you were conducting your own study, which one (if\n    either) would you use and why?\n:::\n\n## Glossary\n\n##### behavioral measure {#behavioral-measure .unnumbered}\n\nA measure in which the researcher observes and records some aspect of\nparticipants' behavior. Compare with self-report measure and\nphysiological measure.\n\n##### Cohen's κ {#cohens-κ .unnumbered}\n\nA statistic used to assess interrater reliability when the observer\njudgments are categorical.\n\n##### conceptual definition {.unnumbered}\n\nA description of a variable or construct in terms of the behaviors and\ninternal processes that are involved, along with how that construct\nrelates to other variables.\n\n##### construct {#construct .unnumbered}\n\nA variable that cannot be observed directly because it represents a\ntendency to behave in certain ways or a complex pattern of behavior and\ninternal processes. These include personality traits, emotional states,\nattitudes, and abilities.\n\n##### content validity {.unnumbered}\n\nThe extent to which a measure covers all aspects of the construct it is\nsupposed to measure.\n\n##### converging operations {.unnumbered}\n\nMultiple operational definitions of the same construct. When multiple\noperational definitions are closely related to each other and produce\nthe same pattern of results, this constitutes evidence that the\nconstruct is being measured effectively and is a useful one.\n\n##### criteria {.unnumbered}\n\nA variable or construct expected to be correlated with scores on a\nmeasure that is being evaluated. \"Criteria\" is plural; the singular is\n*criterion.*\n\n##### criterion validity {.unnumbered}\n\nThe extent to which scores on a measure are correlated with other\nvariables and constructs that they are expected to be correlated with,\ngiven the conceptual definition of the construct being measured.\n\n##### Cronbach's α {#cronbachs-α .unnumbered}\n\nA statistic used to assess the internal consistency of a multiple-item\nmeasure. It is conceptually equivalent to the mean of all possible\nsplit-half correlations.\n\n##### demand characteristics {.unnumbered}\n\nFeatures of a study that cue participants as to how the researcher\nexpects them to behave.\n\n##### discriminant validity {.unnumbered}\n\nThe extent to which scores on a measure are not correlated with other\nvariables and constructs that are conceptually distinct.\n\n##### face validity {.unnumbered}\n\nThe extent to which a measure appears \"on its face\" to measure the\nvariable or construct it is supposed to.\n\n##### internal consistency {.unnumbered}\n\nThe extent to which the items on a multiple-item measure are consistent\nwith each other.\n\n##### interrater reliability {.unnumbered}\n\nWhen a measure involves human judgment, the extent to which different\nobservers are consistent in their judgments.\n\n##### interval level {.unnumbered}\n\nThe level of measurement that involves assigning numerical scores so\nthat a given difference between two scores always represents the same\ndifference in the characteristic of interest but a score of zero does\nnot literally represent none of the characteristic. Scores at the\ninterval level indicate how much more or less of the characteristic one\nindividual has than another. Ratios of one score to another are not\nmeaningful at this level.\n\n##### levels of measurement {.unnumbered}\n\nFour different ways of assigning scores to individuals that provide\nincreasing amounts of quantitative information about the characteristic\nbeing measured. The four levels are nominal, ordinal, interval, and\nratio.\n\n##### measurement {.unnumbered}\n\nThe assignment of scores to individuals so that the scores represent\nsome characteristic of the individuals.\n\n##### nominal level {.unnumbered}\n\nThe level of measurement that involves assigning names or category\nlabels to individuals. Scores at the nominal level indicate whether or\nnot one individual is in the same category as another. They do not\ncommunicate any quantitative information.\n\n##### operational definition {.unnumbered}\n\nA definition of a variable or construct in terms of precisely how it\nwill be measured.\n\n##### ordinal level {.unnumbered}\n\nThe level of measurement that involves rank ordering individuals. Scores\nat the ordinal level indicate whether one individual has more or less of\nthe characteristic of interest, but they do not indicate how much more\nor less.\n\n##### physiological measure {#physiological-measure .unnumbered}\n\nA measure that involves recording a physiological variable. Compare with\nself-report measure and behavioral measure.\n\n##### psychometrics {.unnumbered}\n\nThe measurement of psychological variables and constructs.\n\n##### ratio level {.unnumbered}\n\nThe level of measurement that involves assigning numerical scores so\nthat a given difference between two scores always represents the same\ndifference in the characteristic and a score of zero represents none of\nthe characteristic. Ratios of one score to another are meaningful only\nat this level.\n\n##### reactivity {.unnumbered}\n\nParticipants' reactions to the fact that they are being measured.\n\n##### reliability {.unnumbered}\n\nThe extent to which the scores on a measure are consistent across time,\nacross multiple items on the same measure, and across researchers when a\nmeasure has an element of subjective judgment.\n\n##### self-report measure {#self-report-measure .unnumbered}\n\nA measure in which participants report on their own thoughts, feelings,\nand behaviors. Compare with behavioral measure and physiological\nmeasure.\n\n##### socially desirable responding {.unnumbered}\n\nParticipants' responding in ways they believe to be socially appropriate\nrather than in ways that reflect their actual thoughts, feelings, and\nbehavior.\n\n##### split-half correlation {.unnumbered}\n\nThe correlation between scores based on one half of the items on a\nmultiple-item measure and scores based on the other half of the items.\n\n##### test-retest correlation {.unnumbered}\n\nThe correlation between individuals' scores on a measure used at two\ndifferent times.\n\n##### test-retest reliability {.unnumbered}\n\nThe extent to which scores on a measure are consistent across time for\nthe same individuals.\n\n##### validity {.unnumbered}\n\nThe extent to which scores on a measure represent the variable or\nconstruct they are intended to. Validity is a judgment based on the\navailable evidence.\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "06-measurement_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}