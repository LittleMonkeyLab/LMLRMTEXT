{
  "hash": "01a43225e44394fb08f7636ef3f1e4f2",
  "result": {
    "markdown": "# Experimental Research\n\nIn the late 1960s social psychologists John Darley and Bibb Latané\nproposed a counterintuitive hypothesis. The more witnesses there are to\nan accident or a crime, the less likely any of them is to help the\nvictim [@darley1968bystander]. They also suggested the theory that this\nhappens because each witness feels less responsible for helping---a\nprocess referred to as the \"diffusion of responsibility.\" Darley and\nLatané noted that their ideas were consistent with many real-world\ncases. For example, a New York woman named Kitty Genovese was assaulted\nand murdered while several witnesses failed to help. But Darley and\nLatané also understood that such isolated cases did not provide\nconvincing evidence for their hypothesized \"bystander effect.\" There was\nno way to know, for example, whether any of the witnesses to Kitty\nGenovese's murder would have helped had there been fewer of them.\n\nSo to test their hypothesis, Darley and Latané created a simulated\nemergency situation in a laboratory. Each of their college student\nparticipants was isolated in a small room and told that he or she would\nbe having a discussion about college life with other students via an\nintercom system. Early in the discussion, however, one of the students\nbegan having what seemed to be an epileptic seizure. Over the intercom\ncame the following: \"I could really-er-use some help so if somebody\nwould-er-give me a little h-help-uh-er-er-er-er-er c-could\nsomebody-er-er-help-er-uh-uh-uh (choking sounds)...I'm gonna\ndie-er-er-I'm...gonna die-er-help-er-er-seizure-er- \\[chokes, then\nquiet\\]\" [@darley1968bystander p.379].\n\nIn actuality, there were no other students. These comments had been\nprerecorded and were played back to create the appearance of a real\nemergency. The key to the study was that some participants were told\nthat the discussion involved only one other student (the victim), others\nwere told that it involved two other students, and still others were\ntold that it included five other students. Because this was the only\ndifference between these three groups of participants, any difference in\ntheir tendency to help the victim would have to have been caused by it.\nAnd sure enough, the likelihood that the participant left the room to\nseek help for the \"victim\" decreased from 85% to 62% to 31% as the\nnumber of \"witnesses\" increased.\n\n::: fyi\n##### The Parable of the 38 Witnesses {.unnumbered}\n\nThe story of Kitty Genovese has been told and retold in numerous\npsychology textbooks. The standard version is that there were 38\nwitnesses to the crime, that all of them watched (or listened) for an\nextended period of time, and that none of them did anything to help.\nHowever, recent scholarship suggests that the standard story is\ninaccurate in many ways [@manning2007kitty]. For example, only six\neyewitnesses testified at the trial, none of them was aware that he or\nshe was witnessing a lethal assault, and there have been several reports\nof witnesses calling the police or even coming to the aid of Kitty\nGenovese. Although the standard story inspired a long line of research\non the bystander effect and the diffusion of responsibility, it may also\nhave directed researchers' and students' attention away from other\nequally interesting and important issues in the psychology of\nhelping---including the conditions in which people do in fact respond\ncollectively to emergency situations.\n:::\n\nThe study that Darley and Latané conducted was a particular kind of\nstudy called an experiment. Experiments are used to determine not only\nwhether there is a statistical relationship between two variables but\nalso whether the relationship is a causal one. For this reason,\nexperiments are one of the most common and useful tools in the\npsychological researcher's toolbox. In this chapter, we look at\nexperiments in detail. We consider first what sets experiments apart\nfrom other kinds of studies and why they support causal conclusions\nwhile other kinds of studies do not. We then look at two basic ways of\ndesigning an experiment---between-subjects designs and within-subjects\ndesigns---and discuss their pros and cons. Finally, we consider several\nimportant practical issues that arise when conducting experiments.\n\n## Experiment Basics\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Explain what an experiment is and recognize examples of studies that\n    are experiments and studies that are not experiments.\n2.  Explain what internal validity is and why experiments are considered\n    to be high in internal validity.\n3.  Explain what external validity is and evaluate studies in terms of\n    their external validity.\n4.  Distinguish between the manipulation of the independent variable and\n    control of extraneous variables and explain the importance of each.\n5.  Recognize examples of confounding variables and explain how they\n    affect the internal validity of a study.\n:::\n\n### What Is an Experiment? {.unnumbered}\n\nAs we saw earlier in the book, an [experiment] is a type of study\ndesigned specifically to answer the question of whether there is a\ncausal relationship between two variables. Do changes in an independent\nvariable *cause* changes in a dependent variable? Experiments have two\nfundamental features. The first is that the researchers manipulate, or\nsystematically vary, the level of the independent variable. The\ndifferent levels of the independent variable are called conditions. For\nexample, in Darley and Latané's experiment, the independent variable was\nthe number of witnesses that participants believed to be present. The\nresearchers manipulated this independent variable by telling\nparticipants that there were either one, two, or five other students\ninvolved in the discussion, thereby creating three conditions. The\nsecond fundamental feature of an experiment is that the researcher\ncontrols, or minimizes the variability in, variables other than the\nindependent and dependent variable. These other variables are called\nextraneous variables. Darley and Latané tested all their participants in\nthe same room, exposed them to the same emergency situation, and so on.\nThey also randomly assigned their participants to conditions so that the\nthree groups would be similar to each other to begin with. Notice that\nalthough the words *manipulation* and *control* have similar meanings in\neveryday language, researchers make a clear distinction between them.\nThey *manipulate* the independent variable by systematically changing\nits levels and control other variables by holding them constant.\n\n### Internal and External Validity {.unnumbered}\n\n#### Internal Validity {.unnumbered}\n\nRecall that the fact that two variables are statistically related does\nnot necessarily mean that one causes the other. \"Correlation does not\nimply causation.\" For example, if it were the case that people who\nexercise regularly are happier than people who do not exercise\nregularly, this would not necessarily mean that exercising increases\npeople's happiness. It could mean instead that greater happiness causes\npeople to exercise (the directionality problem) or that something like\nbetter physical health causes people to exercise *and* be happier (the\nthird-variable problem).\n\nThe purpose of an experiment, however, is to show that two variables are\nstatistically related and to do so in a way that supports the conclusion\nthat the independent variable caused any observed differences in the\ndependent variable. The basic logic is this: If the researcher creates\ntwo or more highly similar conditions and then manipulates the\nindependent variable to produce just one difference between them, then\nany later difference between the conditions must have been caused by the\nindependent variable. For example, because the only difference between\nDarley and Latané's conditions was the number of students that\nparticipants believed to be involved in the discussion, this must have\nbeen responsible for differences in helping between the conditions.\n\nAn empirical study is said to be high in [internal validity] if the way\nit was conducted supports the conclusion that the independent variable\ncaused any observed differences in the dependent variable. Thus\nexperiments are high in internal validity because the way they are\nconducted---with the manipulation of the independent variable and the\ncontrol of extraneous variables---provides strong support for causal\nconclusions.\n\n#### External Validity {.unnumbered}\n\nAt the same time, the way that experiments are conducted sometimes leads\nto a different kind of criticism. Specifically, the need to manipulate\nthe independent variable and control extraneous variables means that\nexperiments are often conducted under conditions that seem artificial or\nunlike \"real life\" [@stanovich2013think]. In many psychology\nexperiments, the participants are all college undergraduates and come to\na classroom or laboratory to fill out a series of paper-and-pencil\nquestionnaires or to perform a carefully designed computerized task.\nConsider, for example, an experiment in which researcher Barbara\nFredrickson and her colleagues had college students come to a laboratory\non campus and complete a math test while wearing a swimsuit\n[@fredrickson1998swimsuit]. At first, this might seem silly. When will\ncollege students ever have to complete math tests in their swimsuits\noutside of this experiment?\n\nThe issue we are confronting is that of external validity. An empirical\nstudy is high in [external validity] if the way it was conducted\nsupports generalizing the results to people and situations beyond those\nactually studied. As a general rule, studies are higher in external\nvalidity when the participants and the situation studied are similar to\nthose that the researchers want to generalize to. Imagine, for example,\nthat a group of researchers is interested in how shoppers in large\ngrocery stores are affected by whether breakfast cereal is packaged in\nyellow or purple boxes. Their study would be high in external validity\nif they studied the decisions of ordinary people doing their weekly\nshopping in a real grocery store. If the shoppers bought much more\ncereal in purple boxes, the researchers would be fairly confident that\nthis would be true for other shoppers in other stores. Their study would\nbe relatively low in external validity, however, if they studied a\nsample of college students in a laboratory at a selective college who\nmerely judged the appeal of various colors presented on a computer\nscreen. If the students judged purple to be more appealing than yellow,\nthe researchers would not be very confident that this is relevant to\ngrocery shoppers' cereal-buying decisions.\n\nWe should be careful, however, not to draw the blanket conclusion that\nexperiments are low in external validity. One reason is that experiments\nneed not seem artificial. Consider that Darley and Latané's experiment\nprovided a reasonably good simulation of a real emergency situation. Or\nconsider [field experiments] that are conducted entirely outside the\nlaboratory. In one such experiment, Robert Cialdini and his colleagues\nstudied whether hotel guests choose to reuse their towels for a second\nday as opposed to having them washed as a way of conserving water and\nenergy [@cialdini2005don]. These researchers manipulated the message on\na card left in a large sample of hotel rooms. One version of the message\nemphasized showing respect for the environment, another emphasized that\nthe hotel would donate a portion of their savings to an environmental\ncause, and a third emphasized that most hotel guests choose to reuse\ntheir towels. The result was that guests who received the message that\nmost hotel guests choose to reuse their towels reused their own towels\nsubstantially more often than guests receiving either of the other two\nmessages. Given the way they conducted their study, it seems very likely\nthat their result would hold true for other guests in other hotels.\n\nA second reason not to draw the blanket conclusion that experiments are\nlow in external validity is that they are often conducted to learn about\npsychological *processes* that are likely to operate in a variety of\npeople and situations. Let us return to the experiment by Fredrickson\nand colleagues. They found that the women in their study, but not the\nmen, performed worse on the math test when they were wearing swimsuits.\nThey argued that this was due to women's greater tendency to objectify\nthemselves---to think about themselves from the perspective of an\noutside observer---which diverts their attention away from other tasks.\nThey argued, furthermore, that this process of self-objectification and\nits effect on attention is likely to operate in a variety of women and\nsituations---even if none of them ever finds herself taking a math test\nin her swimsuit.\n\n### Manipulation of the Independent Variable {.unnumbered}\n\nAgain, to [manipulate] an independent variable means to change its level\nsystematically so that different groups of participants are exposed to\ndifferent levels of that variable, or the same group of participants is\nexposed to different levels at different times. For example, to see\nwhether expressive writing affects people's health, a researcher might\ninstruct some participants to write about traumatic experiences and\nothers to write about neutral experiences. The different levels of the\nindependent variable are referred to as [conditions](#condition), and\nresearchers often give the conditions short descriptive names to make it\neasy to talk and write about them. In this case, the conditions might be\ncalled the \"traumatic condition\" and the \"neutral condition.\"\n\nNotice that the manipulation of an independent variable must involve the\nactive intervention of the researcher. Comparing groups of people who\ndiffer on the independent variable before the study begins is not the\nsame as manipulating that variable. For example, a researcher who\ncompares the health of people who already keep a journal with the health\nof people who do not keep a journal has not manipulated this variable\nand therefore not conducted an experiment. This is important because\ngroups that already differ in one way at the beginning of a study are\nlikely to differ in other ways too. For example, people who choose to\nkeep journals might also be more conscientious, more introverted, or\nless stressed than people who do not. Therefore, any observed difference\nbetween the two groups in terms of their health might have been caused\nby whether or not they keep a journal, or it might have been caused by\nany of the other differences between people who do and do not keep\njournals. Thus the active manipulation of the independent variable is\ncrucial for eliminating the third-variable problem.\n\nOf course, there are many situations in which the independent variable\ncannot be manipulated for practical or ethical reasons and therefore an\nexperiment is not possible. For example, whether or not people have a\nsignificant early illness experience cannot be manipulated, making it\nimpossible to do an experiment on the effect of early illness\nexperiences on the development of hypochondriasis. This does not mean it\nis impossible to study the relationship between early illness\nexperiences and hypochondriasis---only that it must be done using\nnonexperimental approaches. We will discuss this in detail later in the\nbook.\n\nIn many experiments, the independent variable is a construct that can\nonly be manipulated indirectly. For example, a researcher might try to\nmanipulate participants' stress levels indirectly by telling some of\nthem that they have five minutes to prepare a short speech that they\nwill then have to give to an audience of other participants. In such\nsituations, researchers often include a manipulation check in their\nprocedure. A [manipulation check] is a separate measure of the construct\nthe researcher is trying to manipulate. For example, researchers trying\nto manipulate participants' stress levels might give them a\npaper-and-pencil stress questionnaire or take their blood\npressure---perhaps right after the manipulation or at the end of the\nprocedure---to verify that they successfully manipulated this variable.\n\n### Control of Extraneous Variables {.unnumbered}\n\nAn [extraneous variable] is anything that varies in the context of a\nstudy other than the independent and dependent variables. In an\nexperiment on the effect of expressive writing on health, for example,\nextraneous variables would include participant variables (individual\ndifferences) such as their writing ability, their diet, and their shoe\nsize. They would also include situation or task variables such as the\ntime of day when participants write, whether they write by hand or on a\ncomputer, and the weather. Extraneous variables pose a problem because\nmany of them are likely to have some effect on the dependent variable.\nFor example, participants' health will be affected by many things other\nthan whether or not they engage in expressive writing. This can make it\ndifficult to separate the effect of the independent variable from the\neffects of the extraneous variables, which is why it is important to\n[control] extraneous variables by holding them constant.\n\n#### Extraneous Variables as \"Noise\" {.unnumbered}\n\nExtraneous variables make it difficult to detect the effect of the\nindependent variable in two ways. One is by adding variability or\n\"noise\" to the data. Imagine a simple experiment on the effect of mood\n(happy vs. sad) on the number of happy childhood events people are able\nto recall. Participants are put into a negative or positive mood (by\nshowing them a happy or sad video clip) and then asked to recall as many\nhappy childhood events as they can. The two leftmost columns of Table\n\\@ref(tab:noise) show what the data might look like if there were no\nextraneous variables and the number of happy childhood events\nparticipants recalled was affected only by their moods. Every\nparticipant in the happy mood condition recalled exactly four happy\nchildhood events, and every participant in the sad mood condition\nrecalled exactly three. The effect of mood here is quite obvious. In\nreality, however, the data would probably look more like those in the\ntwo rightmost columns of Table \\@ref(tab:noise). Even in the happy mood\ncondition, some participants would recall fewer happy memories because\nthey have fewer to draw on, use less effective strategies, or are less\nmotivated. And even in the sad mood condition, some participants would\nrecall more happy childhood memories because they have more happy\nmemories to draw on, they use more effective recall strategies, or they\nare more motivated. Although the mean difference between the two groups\nis the same as in the idealized data, this difference is much less\nobvious in the context of the greater variability in the data. Thus one\nreason researchers try to control extraneous variables is so their data\nlook more like the idealized data in Table \\@ref(tab:noise), which makes\nthe effect of the independent variable is easier to detect (although\nreal data never look quite *that* good).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\\begin{table}\n\\caption{\\label{tab:noise}Left: Hypothetical noiseless data; Right: realistic noisy data}\n\n\\centering\n\\begin{tabular}[t]{l|l}\n\\hline\nHappy & Sad\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n4 & 3\\\\\n\\hline\n*M* = 4 & *M* = 3\\\\\n\\hline\n\\end{tabular}\n\\centering\n\\begin{tabular}[t]{l|l}\n\\hline\nHappy & Sad\\\\\n\\hline\n3 & 1\\\\\n\\hline\n6 & 3\\\\\n\\hline\n2 & 4\\\\\n\\hline\n4 & 0\\\\\n\\hline\n5 & 5\\\\\n\\hline\n2 & 7\\\\\n\\hline\n3 & 2\\\\\n\\hline\n1 & 5\\\\\n\\hline\n6 & 1\\\\\n\\hline\n8 & 2\\\\\n\\hline\n*M* = 4 & *M* = 3\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\nOne way to control extraneous variables is to hold them constant. This\ncan mean holding situation or task variables constant by testing all\nparticipants in the same location, giving them identical instructions,\ntreating them in the same way, and so on. It can also mean holding\nparticipant variables constant. For example, many studies of language\nlimit participants to right-handed people, who generally have their\nlanguage areas isolated in their left cerebral hemispheres. Left-handed\npeople are more likely to have their language areas isolated in their\nright cerebral hemispheres or distributed across both hemispheres, which\ncan change the way they process language and thereby add noise to the\ndata.\n\nIn principle, researchers can control extraneous variables by limiting\nparticipants to one very specific category of person, such as\n20-year-old, straight, female, right-handed, sophomore psychology\nmajors. The obvious downside to this approach is that it would lower the\nexternal validity of the study---in particular, the extent to which the\nresults can be generalized beyond the people actually studied. For\nexample, it might be unclear whether results obtained with a sample of\nyounger straight women would apply to older gay men. In many situations,\nthe advantages of a diverse sample outweigh the reduction in noise\nachieved by a homogeneous one.\n\n#### Extraneous Variables as Confounding Variables {.unnumbered}\n\nThe second way that extraneous variables can make it difficult to detect\nthe effect of the independent variable is by becoming [confounding\nvariables](#confounding-variable). A confounding variable is an\nextraneous variable that differs on average across levels of the\nindependent variable. For example, in almost all experiments,\nparticipants' intelligence quotients (IQs) will be an extraneous\nvariable. But as long as there are participants with lower and higher\nIQs at each level of the independent variable so that the average IQ is\nroughly equal, then this variation is probably acceptable (and may even\nbe desirable). What would be bad, however, would be for participants at\none level of the independent variable to have substantially lower IQs on\naverage and participants at another level to have substantially higher\nIQs on average. In this case, IQ would be a confounding variable.\n\nTo confound means to confuse, and this is exactly what confounding\nvariables do. Because they differ across conditions---just like the\nindependent variable---they provide an alternative explanation for any\nobserved difference in the dependent variable. Figure\n\\@ref(fig:confound) shows the results of a hypothetical study, in which\nparticipants in a positive mood condition scored higher on a memory task\nthan participants in a negative mood condition. But if IQ is a\nconfounding variable---with participants in the positive mood condition\nhaving higher IQs on average than participants in the negative mood\ncondition---then it is unclear whether it was the positive moods or the\nhigher IQs that caused participants in the first condition to score\nhigher. One way to avoid confounding variables is by holding extraneous\nvariables constant. For example, one could prevent IQ from becoming a\nconfounding variable by limiting participants only to those with IQs of\nexactly 100. But this approach is not always desirable for reasons we\nhave already discussed. A second and much more general approach---random\nassignment to conditions---will be discussed in detail shortly.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hypothetical results from a study on the effect of mood on memory. Because IQ also differs across conditions, it is a confounding variable.](05-experiments_files/figure-pdf/confound-1.pdf){fig-align='center' width=70%}\n:::\n:::\n\n\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   An experiment is a type of empirical study that features the\n    manipulation of an independent variable, the measurement of a\n    dependent variable, and control of extraneous variables.\n-   Studies are high in internal validity to the extent that the way\n    they are conducted supports the conclusion that the independent\n    variable caused any observed differences in the dependent variable.\n    Experiments are generally high in internal validity because of the\n    manipulation of the independent variable and control of extraneous\n    variables.\n-   Studies are high in external validity to the extent that the result\n    can be generalized to people and situations beyond those actually\n    studied. Although experiments can seem \"artificial\"---and low in\n    external validity---it is important to consider whether the\n    psychological processes under study are likely to operate in other\n    people and situations.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: List five variables that can be manipulated by the\n    researcher in an experiment. List five variables that cannot be\n    manipulated by the researcher in an experiment.\n2.  Practice: For each of the following topics, decide whether that\n    topic could be studied using an experimental research design and\n    explain why or why not.\n    -   Effect of parietal lobe damage on people's ability to do basic\n        arithmetic.\n    -   Effect of being clinically depressed on the number of close\n        friendships people have.\n    -   Effect of group training on the social skills of teenagers with\n        Asperger's syndrome.\n    -   Effect of paying people to take an IQ test on their performance\n        on that test.\n:::\n\n## Experimental Design\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Explain the difference between between-subjects and within-subjects\n    experiments, list some of the pros and cons of each approach, and\n    decide which approach to use to answer a particular research\n    question.\n2.  Define random assignment, distinguish it from random sampling,\n    explain its purpose in experimental research, and use some simple\n    strategies to implement it.\n3.  Define what a control condition is, explain its purpose in research\n    on treatment effectiveness, and describe some alternative types of\n    control conditions.\n4.  Define several types of carryover effect, give examples of each, and\n    explain how counterbalancing helps to deal with them.\n:::\n\nIn this section, we look at some different ways to design an experiment.\nThe primary distinction we will make is between approaches in which each\nparticipant experiences one level of the independent variable and\napproaches in which each participant experiences all levels of the\nindependent variable. The former are called between-subjects experiments\nand the latter are called within-subjects experiments.\n\n### Between-Subjects Experiments {.unnumbered}\n\nIn a [between-subjects experiment], each participant is tested in only\none condition. For example, a researcher with a sample of 100 college\nstudents might assign half of them to write about a traumatic event and\nthe other half write about a neutral event. Or a researcher with a\nsample of 60 people with severe agoraphobia (fear of open spaces) might\nassign 20 of them to receive each of three different treatments for that\ndisorder. It is essential in a between-subjects experiment that the\nresearcher assign participants to conditions so that the different\ngroups are, on average, highly similar to each other. Those in a trauma\ncondition and a neutral condition, for example, should include a similar\nproportion of men and women, and they should have similar average\nintelligence quotients (IQs), similar average levels of motivation,\nsimilar average numbers of health problems, and so on. This is a matter\nof controlling these extraneous participant variables across conditions\nso that they do not become confounding variables.\n\n#### Random Assignment {.unnumbered}\n\nThe primary way that researchers accomplish this kind of control of\nextraneous variables across conditions is called [random assignment],\nwhich means using a random process to decide which participants are\ntested in which conditions. Do not confuse random assignment with random\nsampling. Random sampling is a method for selecting a sample from a\npopulation, and it is rarely used in psychological research. Random\nassignment is a method for assigning participants in a sample to the\ndifferent conditions, and it is an important element of all experimental\nresearch in psychology and other fields too.\n\nIn its strictest sense, random assignment should meet two criteria. One\nis that each participant has an equal chance of being assigned to each\ncondition (e.g., a 50% chance of being assigned to each of two\nconditions). The second is that each participant is assigned to a\ncondition independently of other participants. Thus one way to assign\nparticipants to two conditions would be to flip a coin for each one. If\nthe coin lands heads, the participant is assigned to Condition A, and if\nit lands tails, the participant is assigned to Condition B. For three\nconditions, one could use a computer to generate a random integer from 1\nto 3 for each participant. If the integer is 1, the participant is\nassigned to Condition A; if it is 2, the participant is assigned to\nCondition B; and if it is 3, the participant is assigned to Condition C.\nIn practice, a full sequence of conditions---one for each participant\nexpected to be in the experiment---is usually created ahead of time, and\neach new participant is assigned to the next condition in the sequence\nas he or she is tested. When the procedure is computerized, the computer\nprogram often handles the random assignment.\n\nOne problem with coin flipping and other strict procedures for random\nassignment is that they are likely to result in unequal sample sizes in\nthe different conditions. Unequal sample sizes are generally not a\nserious problem, and you should never throw away data you have already\ncollected to achieve equal sample sizes. However, for a fixed number of\nparticipants, it is statistically most efficient to divide them into\nequal-sized groups. It is standard practice, therefore, to use a kind of\nmodified random assignment that keeps the number of participants in each\ngroup as similar as possible. One approach is [block randomization]. In\nblock randomization, all the conditions occur once in the sequence\nbefore any of them is repeated. Then they all occur again before any of\nthem is repeated again. Within each of these \"blocks,\" the conditions\noccur in a random order. Again, the sequence of conditions is usually\ngenerated before any participants are tested, and each new participant\nis assigned to the next condition in the sequence. Table\n\\@ref(tab:block) shows such a sequence for assigning nine participants\nto three conditions. The Research Randomizer website\n(http://www.randomizer.org) will generate block randomization sequences\nfor any number of participants and conditions. Again, when the procedure\nis computerized, the computer program often handles the block\nrandomization.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Block randomization sequence for assigning nine participants to three conditions.\n\n|Participant |Condition |\n|:-----------|:---------|\n|**1**       |**A**     |\n|**2**       |**C**     |\n|**3**       |**B**     |\n|4           |B         |\n|5           |C         |\n|6           |A         |\n|**7**       |**C**     |\n|**8**       |**B**     |\n|**9**       |**A**     |\n:::\n:::\n\n\n\nRandom assignment is not guaranteed to control all extraneous variables\nacross conditions. It is always possible that just by chance, the\nparticipants in one condition might turn out to be substantially older,\nless tired, more motivated, or less depressed on average than the\nparticipants in another condition. However, there are some reasons that\nthis is not a major concern. One is that random assignment works better\nthan one might expect, especially for large samples. Another is that the\ninferential statistics that researchers use to decide whether a\ndifference between groups reflects a difference in the population takes\nthe \"fallibility\" of random assignment into account. Yet another reason\nis that even if random assignment does result in a confounding variable\nand therefore produces misleading results, this is likely to be detected\nwhen the experiment is replicated. The upshot is that random assignment\nto conditions---although not infallible in terms of controlling\nextraneous variables---is always considered a strength of a research\ndesign.\n\n#### Treatment and Control Conditions {.unnumbered}\n\nBetween-subjects experiments are often used to determine whether a\ntreatment works. In psychological research, a [treatment] is any\nintervention meant to change people's behavior for the better. This\nincludes psychotherapies and medical treatments for psychological\ndisorders but also interventions designed to improve learning, promote\nconservation, reduce prejudice, and so on. To determine whether a\ntreatment works, participants are randomly assigned to either a\n[treatment condition], in which they receive the treatment, or a\n[control condition], in which they do not receive the treatment. If\nparticipants in the treatment condition end up better off than\nparticipants in the control condition---for example, they are less\ndepressed, learn faster, conserve more, express less prejudice---then\nthe researcher can conclude that the treatment works. In research on the\neffectiveness of psychotherapies and medical treatments, this type of\nexperiment is often called a [randomized clinical trial].\n\nThere are different types of control conditions. In a [no-treatment\ncontrol condition], participants receive no treatment whatsoever. One\nproblem with this approach, however, is the existence of placebo\neffects. A [placebo] is a simulated treatment that lacks any active\ningredient or element that should make it effective, and a [placebo\neffect] is a positive effect of such a treatment. Many folk remedies\nthat seem to work---such as eating chicken soup for a cold or placing\nsoap under the bedsheets to stop nighttime leg cramps---are probably\nnothing more than placebos. Although placebo effects are not well\nunderstood, they are probably driven primarily by people's expectations\nthat they will improve. Having the expectation to improve can result in\nreduced stress, anxiety, and depression, which can alter perceptions and\neven improve immune system functioning [@price2008placebo].\n\nPlacebo effects are interesting in their own right (see box \"The\nPowerful Placebo\"), but they also pose a serious problem for researchers\nwho want to determine whether a treatment works. Figure\n\\@ref(fig:placebo) shows some hypothetical results in which participants\nin a treatment condition improved more on average than participants in a\nno-treatment control condition. If these conditions (the two leftmost\nbars in \\@ref(fig:placebo)) were the only conditions in this experiment,\nhowever, one could not conclude that the treatment worked. It could be\ninstead that participants in the treatment group improved more because\nthey expected to improve, while those in the no-treatment control\ncondition did not.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Hypothetical results from a study including treatment, no-treatment, and placebo conditions.](05-experiments_files/figure-pdf/placebo-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\nFortunately, there are several solutions to this problem. One is to\ninclude a [placebo control condition], in which participants receive a\nplacebo that looks much like the treatment but lacks the active\ningredient or element thought to be responsible for the treatment's\neffectiveness. When participants in a treatment condition take a pill,\nfor example, then those in a placebo control condition would take an\nidentical-looking pill that lacks the active ingredient in the treatment\n(a \"sugar pill\"). In research on psychotherapy effectiveness, the\nplacebo might involve going to a psychotherapist and talking in an\nunstructured way about one's problems. The idea is that if participants\nin both the treatment and the placebo control groups expect to improve,\nthen any improvement in the treatment group over and above that in the\nplacebo control group must have been caused by the treatment and not by\nparticipants' expectations. This is what is shown by a comparison of the\ntwo outer bars in Figure \\@ref(fig:placebo).\n\nOf course, the principle of informed consent requires that participants\nbe told that they will be assigned to either a treatment or a placebo\ncontrol condition---even though they cannot be told which until the\nexperiment ends. In many cases the participants who had been in the\ncontrol condition are then offered an opportunity to have the real\ntreatment. An alternative approach is to use a [waitlist control\ncondition], in which participants are told that they will receive the\ntreatment but must wait until the participants in the treatment\ncondition have already received it. This allows researchers to compare\nparticipants who have received the treatment with participants who are\nnot currently receiving it but who still expect to improve (eventually).\nA final solution to the problem of placebo effects is to leave out the\ncontrol condition completely and compare any new treatment with the best\navailable alternative treatment. For example, a new treatment for simple\nphobia could be compared with standard exposure therapy. Because\nparticipants in both conditions receive a treatment, their expectations\nabout improvement should be similar. This approach also makes sense\nbecause once there is an effective treatment, the interesting question\nabout a new treatment is not simply \"Does it work?\" but \"Does it work\nbetter than what is already available?\"\n\n::: fyi\n##### The Powerful Placebo {.unnumbered}\n\nMany people are not surprised that placebos can have a positive effect\non disorders that seem fundamentally psychological, including\ndepression, anxiety, and insomnia. However, placebos can also have a\npositive effect on disorders that most people think of as fundamentally\nphysiological. These include asthma, ulcers, and warts\n[@shapiro2000powerful]. There is even evidence that placebo\nsurgery---also called \"sham surgery\"---can be as effective as actual\nsurgery.\n\nMedical researcher J. Bruce Moseley and his colleagues conducted a study\non the effectiveness of two arthroscopic surgery procedures for\nosteoarthritis of the knee [@moseley2002controlled]. The control\nparticipants in this study were prepped for surgery, received a\ntranquilizer, and even received three small incisions in their knees.\nBut they did not receive the actual arthroscopic surgical procedure. The\nsurprising result was that all participants improved in terms of both\nknee pain and function, and the sham surgery group improved just as much\nas the treatment groups. According to the researchers, \"This study\nprovides strong evidence that arthroscopic lavage with or without\ndébridement \\[the surgical procedures used\\] is not better than and\nappears to be equivalent to a placebo procedure in improving knee pain\nand self-reported function\" (p. 85).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Research has shown that patients with osteoarthritis of the knee who receive a “sham surgery” experience reductions in pain and improvement in knee function similar to those of patients who receive a real surgery. *Photo by Piron Guillaume on Unsplash.*](images/experiments/surgery.jpeg){fig-align='center' width=50%}\n:::\n:::\n\n\n:::\n\n## Within-Subjects Experiments\n\nIn a [within-subjects experiment], each participant is tested under all\nconditions. Consider an experiment on the effect of a defendant's\nphysical attractiveness on judgments of his guilt. Again, in a\nbetween-subjects experiment, one group of participants would be shown an\nattractive defendant and asked to judge his guilt, and another group of\nparticipants would be shown an unattractive defendant and asked to judge\nhis guilt. In a within-subjects experiment, however, the same group of\nparticipants would judge the guilt of both an attractive and an\nunattractive defendant.\n\nThe primary advantage of this approach is that it provides maximum\ncontrol of extraneous participant variables. Participants in all\nconditions have the same mean IQ, same socioeconomic status, same number\nof siblings, and so on---because they are the very same people.\nWithin-subjects experiments also make it possible to use statistical\nprocedures that remove the effect of these extraneous participant\nvariables on the dependent variable and therefore make the data less\n\"noisy\" and the effect of the independent variable easier to detect. We\nwill look more closely at this idea later in the book.\n\n### Carryover Effects and Counterbalancing {.unnumbered}\n\nThe primary disadvantage of within-subjects designs is that they can\nresult in carryover effects. A [carryover effect] is an effect of being\ntested in one condition on participants' behavior in later conditions.\nOne type of carryover effect is a [practice effect], where participants\nperform a task better in later conditions because they have had a chance\nto practice it. Another type is a [fatigue effect], where participants\nperform a task worse in later conditions because they become tired or\nbored. Being tested in one condition can also change how participants\nperceive stimuli or interpret their task in later conditions. This is\ncalled a [context effect]. For example, an average-looking defendant\nmight be judged more harshly when participants have just judged an\nattractive defendant than when they have just judged an unattractive\ndefendant. Within-subjects experiments also make it easier for\nparticipants to guess the hypothesis. For example, a participant who is\nasked to judge the guilt of an attractive defendant and then is asked to\njudge the guilt of an unattractive defendant is likely to guess that the\nhypothesis is that defendant attractiveness affects judgments of guilt.\nThis could lead the participant to judge the unattractive defendant more\nharshly because he thinks this is what he is expected to do. Or it could\nmake participants judge the two defendants similarly in an effort to be\n\"fair.\"\n\nCarryover effects can be interesting in their own right. (Does the\nattractiveness of one person depend on the attractiveness of other\npeople that we have seen recently?) But when they are not the focus of\nthe research, carryover effects can be problematic. Imagine, for\nexample, that participants judge the guilt of an attractive defendant\nand then judge the guilt of an unattractive defendant. If they judge the\nunattractive defendant more harshly, this might be because of his\nunattractiveness. But it could be instead that they judge him more\nharshly because they are becoming bored or tired. In other words, the\norder of the conditions is a confounding variable. The attractive\ncondition is always the first condition and the unattractive condition\nthe second. Thus any difference between the conditions in terms of the\ndependent variable could be caused by the order of the conditions and\nnot the independent variable itself.\n\nThere is a solution to the problem of order effects, however, that can\nbe used in many situations. It is [counterbalancing], which means\ntesting different participants in different orders. For example, some\nparticipants would be tested in the attractive defendant condition\nfollowed by the unattractive defendant condition, and others would be\ntested in the unattractive condition followed by the attractive\ncondition. With three conditions, there would be six different orders\n(ABC, ACB, BAC, BCA, CAB, and CBA), so some participants would be tested\nin each of the six orders. With counterbalancing, participants are\nassigned to orders randomly, using the techniques we have already\ndiscussed. Thus random assignment plays an important role in\nwithin-subjects designs just as in between-subjects designs. Here,\ninstead of randomly assigning to conditions, they are randomly assigned\nto different orders of conditions. In fact, it can safely be said that\nif a study does not involve random assignment in one form or another, it\nis not an experiment.\n\nThere are two ways to think about what counterbalancing accomplishes.\nOne is that it controls the order of conditions so that it is no longer\na confounding variable. Instead of the attractive condition always being\nfirst and the unattractive condition always being second, the attractive\ncondition comes first for some participants and second for others.\nLikewise, the unattractive condition comes first for some participants\nand second for others. Thus any overall difference in the dependent\nvariable between the two conditions cannot have been caused by the order\nof conditions. A second way to think about what counterbalancing\naccomplishes is that if there are carryover effects, it makes it\npossible to detect them. One can analyze the data separately for each\norder to see whether it had an effect.\n\n::: fyi\n##### When 9 Is \"Larger\" Than 221 {.unnumbered}\n\nResearcher Michael Birnbaum has argued that the lack of context provided\nby between-subjects designs is often a bigger problem than the context\neffects created by within-subjects designs. To demonstrate this, he\nasked one group of participants to rate how large the number 9 was on a\n1-to-10 rating scale and another group to rate how large the number 221\nwas on the same 1-to-10 rating scale [@birnbaum1999show]. Participants\nin this between-subjects design gave the number 9 a mean rating of 5.13\nand the number 221 a mean rating of 3.10. In other words, they rated 9\nas larger than 221! According to Birnbaum, this is because participants\nspontaneously compared 9 with other one-digit numbers (in which case it\nis relatively large) and compared 221 with other three-digit numbers (in\nwhich case it is relatively small).\n:::\n\n#### Simultaneous Within-Subjects Designs {.unnumbered}\n\nSo far, we have discussed an approach to within-subjects designs in\nwhich participants are tested in one condition at a time. There is\nanother approach, however, that is often used when participants make\nmultiple responses in each condition. Imagine, for example, that\nparticipants judge the guilt of 10 attractive defendants and 10\nunattractive defendants. Instead of having people make judgments about\nall 10 defendants of one type followed by all 10 defendants of the other\ntype, the researcher could present all 20 defendants in a sequence that\nmixed the two types. The researcher could then compute each\nparticipant's mean rating for each type of defendant. Or imagine an\nexperiment designed to see whether people with social anxiety disorder\nremember negative adjectives (e.g., \"stupid,\" \"incompetent\") better than\npositive ones (e.g., \"happy,\" \"productive\"). The researcher could have\nparticipants study a single list that includes both kinds of words and\nthen have them try to recall as many words as possible. The researcher\ncould then count the number of each type of word that was recalled.\nThere are many ways to determine the order in which the stimuli are\npresented, but one common way is to generate a different random order\nfor each participant.\n\n### Between-Subjects or Within-Subjects? {.unnumbered}\n\nAlmost every experiment can be conducted using either a between-subjects\ndesign or a within-subjects design. This means that researchers must\nchoose between the two approaches based on their relative merits for the\nparticular situation.\n\nBetween-subjects experiments have the advantage of being conceptually\nsimpler and requiring less testing time per participant. They also avoid\ncarryover effects without the need for counterbalancing. Within-subjects\nexperiments have the advantage of controlling extraneous participant\nvariables, which generally reduces noise in the data and makes it easier\nto detect a relationship between the independent and dependent\nvariables.\n\nA good rule of thumb, then, is that if it is possible to conduct a\nwithin-subjects experiment (with proper counterbalancing) in the time\nthat is available per participant---and you have no serious concerns\nabout carryover effects---this is probably the best option. If a\nwithin-subjects design would be difficult or impossible to carry out,\nthen you should consider a between-subjects design instead. For example,\nif you were testing participants in a doctor's waiting room or shoppers\nin line at a grocery store, you might not have enough time to test each\nparticipant in all conditions and therefore would opt for a\nbetween-subjects design. Or imagine you were trying to reduce people's\nlevel of prejudice by having them interact with someone of another race.\nA within-subjects design with counterbalancing would require testing\nsome participants in the treatment condition first and then in a control\ncondition. But if the treatment works and reduces people's level of\nprejudice, then they would no longer be suitable for testing in the\ncontrol condition. This is true for many designs that involve a\ntreatment meant to produce long-term change in participants' behavior\n(e.g., studies testing the effectiveness of psychotherapy). Clearly, a\nbetween-subjects design would be necessary here.\n\nRemember also that using one type of design does not preclude using the\nother type in a different study. There is no reason that a researcher\ncould not use both a between-subjects design and a within-subjects\ndesign to answer the same research question. In fact, professional\nresearchers often do exactly this.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   Experiments can be conducted using either between-subjects or\n    within-subjects designs. Deciding which to use in a particular\n    situation requires careful consideration of the pros and cons of\n    each approach.\n-   Random assignment to conditions in between-subjects experiments or\n    to orders of conditions in within-subjects experiments is a\n    fundamental element of experimental research. Its purpose is to\n    control extraneous variables so that they do not become confounding\n    variables.\n-   Experimental research on the effectiveness of a treatment requires\n    both a treatment condition and a control condition, which can be a\n    no-treatment control condition, a placebo control condition, or a\n    waitlist control condition. Experimental treatments can also be\n    compared with the best available alternative.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Discussion: For each of the following topics, list the pros and cons\n    of a between-subjects and within-subjects design and decide which\n    would be better.\n    -   You want to test the relative effectiveness of two training\n        programs for running a marathon.\n    -   Using photographs of people as stimuli, you want to see if\n        smiling people are perceived as more intelligent than people who\n        are not smiling.\n    -   In a field experiment, you want to see if the way a panhandler\n        is dressed (neatly vs. sloppily) affects whether or not\n        passersby give him any money.\n    -   You want to see if concrete nouns (e.g., *dog*) are recalled\n        better than abstract nouns (e.g., *truth*).\n2.  Discussion: Imagine that an experiment shows that participants who\n    receive psychodynamic therapy for a dog phobia improve more than\n    participants in a no-treatment control group. Explain a fundamental\n    problem with this research design and at least two ways that it\n    might be corrected.\n:::\n\n## Conducting Experiments\n\n::: learningobjectives\n##### LEARNING OBJECTIVES {.unnumbered}\n\n1.  Describe several strategies for recruiting participants for an\n    experiment.\n2.  Explain why it is important to standardize the procedure of an\n    experiment and several ways to do this.\n3.  Explain what pilot testing is and why it is important.\n:::\n\nThe information presented so far in this chapter is enough to design a\nbasic experiment. When it comes time to conduct that experiment,\nhowever, several additional practical issues arise. In this section, we\nconsider some of these issues and how to deal with them. Much of this\ninformation applies to nonexperimental studies as well as experimental\nones.\n\n### Recruiting Participants {.unnumbered}\n\nOf course, you should be thinking about how you will obtain your\nparticipants from the beginning of any research project. Unless you have\naccess to people with schizophrenia or incarcerated juvenile offenders,\nfor example, then there is no point designing a study that focuses on\nthese populations. But even if you plan to use a convenience sample, you\nwill have to recruit participants for your study.\n\nThere are several approaches to recruiting participants. One is to use\nparticipants from a formal [subject pool]---an established group of\npeople who have agreed to be contacted about participating in research\nstudies. For example, at many colleges and universities, there is a\nsubject pool consisting of students enrolled in introductory psychology\ncourses who must participate in a certain number of studies to meet a\ncourse requirement. Researchers post descriptions of their studies and\nstudents sign up to participate, usually via an online system.\nParticipants who are not in subject pools can also be recruited by\nposting or publishing advertisements or making personal appeals to\ngroups that represent the population of interest. For example, a\nresearcher interested in studying older adults could arrange to speak at\na meeting of the residents at a retirement community to explain the\nstudy and ask for volunteers.\n\n::: fyi\n##### The Volunteer Subject {.unnumbered}\n\nEven if the participants in a study receive compensation in the form of\ncourse credit, a small amount of money, or a chance at being treated for\na psychological problem, they are still essentially volunteers. This is\nworth considering because people who volunteer to participate in\npsychological research have been shown to differ in predictable ways\nfrom those who do not volunteer. Specifically, there is good evidence\nthat on average, volunteers have the following characteristics compared\nwith nonvolunteers [@rosenthal1965volunteer].\n\n-   They are more interested in the topic of the research.\n-   They are more educated.\n-   They have a greater need for approval.\n-   They have higher intelligence quotients (IQs).\n-   They are more sociable.\n-   They are higher in social class.\n\nThis can be an issue of external validity if there is reason to believe\nthat participants with these characteristics are likely to behave\ndifferently than the general population. For example, in testing\ndifferent methods of persuading people, a rational argument might work\nbetter on volunteers than it does on the general population because of\ntheir generally higher educational level and IQ.\n:::\n\nIn many field experiments, the task is not recruiting participants but\nselecting them. For example, researchers Nicolas Guéguen and Marie-Agnès\nde Gail conducted a field experiment on the effect of being smiled at on\nhelping, in which the participants were shoppers at a supermarket. A\nconfederate walking down a stairway gazed directly at a shopper walking\nup the stairway and either smiled or did not smile. Shortly afterward,\nthe shopper encountered another confederate, who dropped some computer\ndiskettes on the ground. The dependent variable was whether or not the\nshopper stopped to help pick up the diskettes [@gueguen2003effect].\nNotice that these participants were not \"recruited,\" but the researchers\nstill had to select them from among all the shoppers taking the stairs\nthat day. It is extremely important that this kind of selection be done\naccording to a well-defined set of rules that is established before the\ndata collection begins and can be explained clearly afterward. In this\ncase, with each trip down the stairs, the confederate was instructed to\ngaze at the first person he encountered who appeared to be between the\nages of 20 and 50. Only if the person gazed back did he or she become a\nparticipant in the study. The point of having a well-defined selection\nrule is to avoid bias in the selection of participants. For example, if\nthe confederate was free to choose which shoppers he would gaze at, he\nmight choose friendly-looking shoppers when he was set to smile and\nunfriendly-looking ones when he was not set to smile. As we will see\nshortly, such biases can be entirely unintentional.\n\n### Standardizing the Procedure {.unnumbered}\n\nIt is surprisingly easy to introduce extraneous variables during the\nprocedure. For example, the same experimenter might give clear\ninstructions to one participant but vague instructions to another. Or\none experimenter might greet participants warmly while another barely\nmakes eye contact with them. To the extent that such variables affect\nparticipants' behavior, they add noise to the data and make the effect\nof the independent variable more difficult to detect. If they vary\nacross conditions, they become confounding variables and provide\nalternative explanations for the results. For example, if participants\nin a treatment group are tested by a warm and friendly experimenter and\nparticipants in a control group are tested by a cold and unfriendly one,\nthen what appears to be an effect of the treatment might actually be an\neffect of experimenter demeanor.\n\n::: fyi\n##### Experimenter's Sex as an Extraneous Variable {.unnumbered}\n\nIt is well known that whether research participants are male or female\ncan affect the results of a study. But what about whether the\nexperimenter is male or female? There is plenty of evidence that this\nmatters too. Male and female experimenters have slightly different ways\nof interacting with their participants, and of course participants also\nrespond differently to male and female experimenters\n[@rosenthal1976experimenter] For example, in a study on pain perception,\nparticipants immersed their hands in icy water for as long as they could\n[@kallai2004effects]. Male participants tolerated the pain longer when\nthe experimenter was a woman, and female participants tolerated it\nlonger when the experimenter was a man.\n:::\n\nResearcher Robert Rosenthal has spent much of his career showing that\nthis kind of unintended variation in the procedure does, in fact, affect\nparticipants' behavior. Furthermore, one important source of such\nvariation is the experimenter's expectations about how participants\n\"should\" behave in the experiment. This is referred to as an\nexperimenter expectancy effect [@rosenthal1976experimenter]. For\nexample, if an experimenter expects participants in a treatment group to\nperform better on a task than participants in a control group, then he\nor she might unintentionally give the treatment group participants\nclearer instructions or more encouragement or allow them more time to\ncomplete the task. In a striking example, Rosenthal and Kermit Fode had\nseveral students in a laboratory course in psychology train rats to run\nthrough a maze. Although the rats were genetically similar, some of the\nstudents were told that they were working with \"maze-bright\" rats that\nhad been bred to be good learners, and other students were told that\nthey were working with \"maze-dull\" rats that had been bred to be poor\nlearners. Sure enough, over five days of training, the \"maze-bright\"\nrats made more correct responses, made the correct response more\nquickly, and improved more steadily than the \"maze-dull\" rats\n[@rosenthal1963effect]. Clearly it had to have been the students'\nexpectations about how the rats would perform that made the difference.\nBut how? Some clues come from data gathered at the end of the study,\nwhich showed that students who expected their rats to learn quickly felt\nmore positively about their animals and reported behaving toward them in\na more friendly manner (e.g., handling them more).\n\nThe way to minimize unintended variation in the procedure is to\nstandardize it as much as possible so that it is carried out in the same\nway for all participants regardless of the condition they are in. Here\nare several ways to do this:\n\n-   Create a written protocol that specifies everything that the\n    experimenters are to do and say from the time they greet\n    participants to the time they dismiss them.\n-   Create standard instructions that participants read themselves or\n    that are read to them word for word by the experimenter.\n-   Automate the rest of the procedure as much as possible by using\n    software packages for this purpose or even simple computer slide\n    shows.\n-   Anticipate participants' questions and either raise and answer them\n    in the instructions or develop standard answers for them.\n-   Train multiple experimenters on the protocol together and have them\n    practice on each other.\n-   Be sure that each experimenter tests participants in all conditions.\n\nAnother good practice is to arrange for the experimenters to be \"blind\"\nto the research question or to the condition that each participant is\ntested in. The idea is to minimize experimenter expectancy effects by\nminimizing the experimenters' expectations. For example, in a drug study\nin which each participant receives the drug or a placebo, it is often\nthe case that neither the participants nor the experimenter who\ninteracts with the participants know which condition he or she has been\nassigned to. Because both the participants and the experimenters are\nblind to the condition, this is referred to as a [double-blind] study.\n(A single-blind study is one in which the participant, but not the\nexperimenter, is blind to the condition.) Of course, there are many\ntimes this is not possible. For example, if you are both the\ninvestigator and the only experimenter, it is not possible for you to\nremain blind to the research question. Also, in many studies the\nexperimenter must know the condition because he or she must carry out\nthe procedure in a different way in the different conditions.\n\n### Record Keeping {.unnumbered}\n\nIt is essential to keep good records when you conduct an experiment. As\ndiscussed earlier, it is typical for experimenters to generate a written\nsequence of conditions before the study begins and then to test each new\nparticipant in the next condition in the sequence. As you test them, it\nis a good idea to add to this list basic demographic information; the\ndate, time, and place of testing; and the name of the experimenter who\ndid the testing. It is also a good idea to have a place for the\nexperimenter to write down comments about unusual occurrences (e.g., a\nconfused or uncooperative participant) or questions that come up. This\nkind of information can be useful later if you decide to analyze sex\ndifferences or effects of different experimenters, or if a question\narises about a particular participant or testing session.\n\nIt can also be useful to assign an identification number to each\nparticipant as you test them. Simply numbering them consecutively\nbeginning with 1 is usually sufficient. This number can then also be\nwritten on any response sheets or questionnaires that participants\ngenerate, making it easier to keep them together.\n\n### Pilot Testing {.unnumbered}\n\nIt is always a good idea to conduct a [pilot test] of your experiment. A\npilot test is a small-scale study conducted to make sure that a new\nprocedure works as planned. In a pilot test, you can recruit\nparticipants formally (e.g., from an established participant pool) or\nyou can recruit them informally from among family, friends, classmates,\nand so on. The number of participants can be small, but it should be\nenough to give you confidence that your procedure works as planned.\nThere are several important questions that you can answer by conducting\na pilot test:\n\n-   Do participants understand the instructions?\n-   What kind of misunderstandings do participants have, what kind of\n    mistakes do they make, and what kind of questions do they ask?\n-   Do participants become bored or frustrated?\n-   Is an indirect manipulation effective? (You will need to include a\n    manipulation check.)\n-   Can participants guess the research question or hypothesis?\n-   How long does the procedure take?\n-   Are computer programs or other automated procedures working\n    properly?\n-   Are data being recorded correctly?\n\nOf course, to answer some of these questions you will need to observe\nparticipants carefully during the procedure and talk with them about it\nafterward. Participants are often hesitant to criticize a study in front\nof the researcher, so be sure they understand that this is a pilot test\nand you are genuinely interested in feedback that will help you improve\nthe procedure. If the procedure works as planned, then you can proceed\nwith the actual study. If there are problems to be solved, you can solve\nthem, pilot test the new procedure, and continue with this process until\nyou are ready to proceed.\n\n::: takeaways\n##### KEY TAKEAWAYS {.unnumbered}\n\n-   There are several effective methods you can use to recruit research\n    participants for your experiment, including through formal subject\n    pools, advertisements, and personal appeals. Field experiments\n    require well-defined participant selection procedures.\n-   It is important to standardize experimental procedures to minimize\n    extraneous variables, including experimenter expectancy effects.\n-   It is important to conduct one or more small-scale pilot tests of an\n    experiment to be sure that the procedure works as planned.\n:::\n\n::: exercises\n##### EXERCISES {.unnumbered}\n\n1.  Practice: List two ways that you might recruit participants from\n    each of the following populations: (a) elderly adults, (b)\n    unemployed people, (c) regular exercisers, and (d) math majors.\n2.  Discussion: Imagine a study in which you will visually present\n    participants with a list of 20 words, one at a time, wait for a\n    short time, and then ask them to recall as many of the words as they\n    can. In the stressed condition, they are told that they might also\n    be chosen to give a short speech in front of a small audience. In\n    the unstressed condition, they are not told that they might have to\n    give a speech. What are several specific things that you could do to\n    standardize the procedure?\n:::\n\n## Glossary\n\n##### between-subjects experiment {.unnumbered}\n\nAn experiment in which each participant is tested in one condition.\n\n##### block randomization {.unnumbered}\n\nA method of randomly assigning participants that guarantees that the\ncondition sample sizes are equal or almost equal. A random procedure is\nused to assign the first *k* participants into the *k* conditions, and\nthen to assign the next *k* participants into the *k* conditions, and so\non until all the participants have been assigned.\n\n##### carryover effect {.unnumbered}\n\nAn effect of being tested in one condition on participants' behavior in\nlater conditions.\n\n##### condition {#condition .unnumbered}\n\nOne level of the independent variable in an experiment.\n\n##### confounding variable {#confounding-variable .unnumbered}\n\nAn extraneous variable that differs across the levels of the independent\nvariable.\n\n##### context effect {.unnumbered}\n\nAn unintended effect of the context in which a response is made. In\nwithin-subjects experiments, this can be an effect of being tested in\none condition on how participants perceive stimuli or interpret their\ntask and therefore how they respond in later conditions. In survey\nresearch, this can be an effect of the surrounding items or the response\nscale on responses to a particular item.\n\n##### control {.unnumbered}\n\nHolding extraneous variables constant.\n\n##### control condition {.unnumbered}\n\nA condition in a study in which participants do not receive the\ntreatment of interest.\n\n##### counterbalancing {.unnumbered}\n\nSystematically varying the order of conditions across participants.\n\n##### double-blind {.unnumbered}\n\nAn experimental research design in which both the participants and the\nexperimenters are unaware of which condition the participant has been\nassigned to.\n\n##### experiment {.unnumbered}\n\nA type of empirical study in which an independent variable is\nmanipulated and a dependent variable is measured while extraneous\nvariables are controlled.\n\n##### experimenter expectancy effect {.unnumbered}\n\nThe effect of the researcher's expectations on participants' behavior.\n\n##### external validity {.unnumbered}\n\nThe extent to which the results of a study can be generalized to people\nand situations beyond those actually studied.\n\n##### extraneous variable {.unnumbered}\n\nAny variable in the context of an experiment other than the independent\nand dependent variables.\n\n##### fatigue effect {.unnumbered}\n\nA carryover effect in which participants perform worse on a task in\nlater conditions because they have become tired or bored.\n\n##### field experiments {.unnumbered}\n\nAn experiment that is conducted outside the laboratory.\n\n##### internal validity {.unnumbered}\n\nThe extent to which the design of a study supports the conclusion that\ndifferences in the independent variable caused any observed differences\nin the dependent variable.\n\n##### manipulate {.unnumbered}\n\nSystematically changing the level of the independent variable across\ngroups or situations.\n\n##### manipulation check {.unnumbered}\n\nA measure of a manipulated independent variable---usually done at the\nend of the procedure---to confirm that the independent variable was\nsuccessfully manipulated.\n\n##### no-treatment control condition {.unnumbered}\n\nA control condition in which participants receive no treatment\nwhatsoever---not even a placebo.\n\n##### pilot test {.unnumbered}\n\nA small-scale study conducted primarily to be sure that a procedure\nworks as planned.\n\n##### placebo {.unnumbered}\n\nA treatment that lacks any active ingredient or element that should make\nit effective.\n\n##### placebo control condition {.unnumbered}\n\nA control condition in which participants receive a placebo.\n\n##### placebo effect {.unnumbered}\n\nThe positive effect of a placebo.\n\n##### practice effect {.unnumbered}\n\nA carryover effect in which participants perform better on a task in\nlater conditions because they have had a chance to practice.\n\n##### random assignment {.unnumbered}\n\nThe assignment of participants to different conditions according to a\nrandom procedure, such as flipping a coin, rolling a die, or using a\nrandom number generator.\n\n##### randomized clinical trial {.unnumbered}\n\nAn experiment designed to test the effectiveness of a psychological or\nmedical treatment.\n\n##### subject pool {.unnumbered}\n\nA group of people who have agreed to be contacted about opportunities to\nbe research participants. Many universities have subject pools that\nconsist of introductory psychology students who participate to meet a\ncourse requirement.\n\n##### treatment {.unnumbered}\n\nAn intervention intended to change people's behavior for the better.\n\n##### treatment condition {.unnumbered}\n\nA condition in a study in which participants receive some treatment of\ninterest.\n\n##### waitlist control condition {.unnumbered}\n\nA control condition in which participants are put on a waitlist to\nreceive the treatment after the study is completed.\n\n##### within-subjects experiment {.unnumbered}\n\nAn experiment in which each participant is tested in all conditions.\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "05-experiments_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}